"""
AI Auto Chat Service
Handles automatic AI-powered responses for Telegram conversations

æ•´åˆ TelegramRAGSystem å¯¦ç¾çŸ¥è­˜å¢å¼·çš„ AI å›è¦†
"""
import asyncio
import aiohttp
import random
import json
from typing import Dict, Any, Optional, Callable, List
from datetime import datetime
from database import db
from ai_context_manager import ai_context
from ai_response_strategy import AIResponseStrategyManager
from ai_quality_checker import AIQualityChecker

# å°å…¥ RAG ç³»çµ±
try:
    from telegram_rag_system import telegram_rag, ConversationOutcome
    from chat_history_indexer import chat_indexer
    RAG_AVAILABLE = True
except ImportError:
    RAG_AVAILABLE = False
    print("[AIAutoChat] RAG ç³»çµ±æœªè¼‰å…¥ï¼Œä½¿ç”¨åŸºç¤æ¨¡å¼", file=__import__('sys').stderr)


class AIAutoChatService:
    """Service for AI-powered automatic chat responses"""
    
    def __init__(self):
        self.settings = {}
        self.is_running = False
        self.send_callback: Optional[Callable] = None
        self.log_callback: Optional[Callable] = None
        self.event_callback: Optional[Callable] = None
        
        # AI endpoints (will be set from settings)
        self.local_ai_endpoint = ""
        self.local_ai_model = ""
        
        # ç­–ç•¥ç®¡ç†å™¨å’Œè³ªé‡æª¢æŸ¥å™¨
        self.strategy_manager = AIResponseStrategyManager()
        self.quality_checker = AIQualityChecker()
        
    async def initialize(self):
        """Initialize the service with settings from database"""
        self.settings = await db.get_ai_settings()
        
        # å¾è¨­ç½®ä¸­è¼‰å…¥ AI ç«¯é»é…ç½®
        if self.settings:
            endpoint = self.settings.get('local_ai_endpoint', '')
            model = self.settings.get('local_ai_model', '')
            if endpoint:
                self.set_ai_config(endpoint, model)
                print(f"[AIAutoChat] å·²è¼‰å…¥ AI é…ç½®: endpoint={endpoint}, model={model}", file=__import__('sys').stderr)
        
    def set_ai_config(self, endpoint: str, model: str = ""):
        """Set AI endpoint configuration"""
        self.local_ai_endpoint = endpoint
        self.local_ai_model = model
    
    def set_callbacks(self, send_callback: Callable, log_callback: Callable = None,
                      event_callback: Callable = None):
        """Set callback functions"""
        self.send_callback = send_callback
        self.log_callback = log_callback
        self.event_callback = event_callback
    
    def log(self, message: str, level: str = "info"):
        """Log a message"""
        if self.log_callback:
            self.log_callback(message, level)
        else:
            print(f"[AIAutoChat] [{level}] {message}")
    
    async def update_settings(self, settings: Dict[str, Any]):
        """Update AI auto chat settings"""
        await db.update_ai_settings(settings)
        self.settings = await db.get_ai_settings()
    
    async def process_incoming_message(self, user_id: str, username: str,
                                         message: str, account_phone: str,
                                         source_group: str = None,
                                         first_name: str = None) -> Optional[str]:
        """
        Process an incoming message and generate a response if auto-chat is enabled
        
        Returns the response text if auto-reply should be sent, None otherwise
        """
        # Check if auto-chat is enabled (æ•´æ•¸ 0/1)
        auto_chat_enabled = self.settings.get('auto_chat_enabled', 0) == 1
        if not auto_chat_enabled:
            self.log(f"[AI] AI è‡ªå‹•èŠå¤©æœªå•Ÿç”¨ï¼Œè·³éè™•ç† (è¨­ç½®å€¼: {self.settings.get('auto_chat_enabled', 0)})")
            return None
        
        mode = self.settings.get('auto_chat_mode', 'semi')
        self.log(f"[AI] è™•ç†ä¾†è‡ªç”¨æˆ¶ {user_id} çš„æ¶ˆæ¯ï¼Œæ¨¡å¼: {mode}")
        
        # Save incoming message to history
        await ai_context.add_message(
            user_id=user_id,
            role='user',
            content=message,
            account_phone=account_phone,
            source_group=source_group
        )
        
        # åˆ†ææ¶ˆæ¯ä¸¦æå–é—œéµä¿¡æ¯ï¼ˆè‡ªå‹•æ›´æ–°ç”¨æˆ¶ç•«åƒã€ä¿å­˜é‡è¦è¨˜æ†¶ï¼‰
        insights = await ai_context.analyze_and_extract_insights(user_id, message, role='user')
        
        # æ›´æ–°åŸºæœ¬ç”¨æˆ¶ä¿¡æ¯ï¼ˆç”¨æˆ¶åã€åå­—ï¼‰
        await db.update_user_profile(user_id, {
            'username': username,
            'first_name': first_name or '',
        })
        
        # è¨˜éŒ„åˆ†æçµæœ
        if insights.get('suggested_stage'):
            self.log(f"ç”¨æˆ¶ {user_id} éšæ®µåˆ¤æ–·: {insights['suggested_stage']}, èˆˆè¶£åº¦: {insights.get('interest_level', 0)}")
        if insights.get('auto_tags'):
            self.log(f"ç”¨æˆ¶ {user_id} è‡ªå‹•æ¨™ç±¤: {', '.join(insights['auto_tags'])}")
        
        # Check conversation state
        state = await db.get_conversation_state(user_id)
        if state and not state.get('auto_reply_enabled', True):
            self.log(f"Auto-reply disabled for user {user_id}")
            return None
        
        # ä½¿ç”¨ç­–ç•¥ç®¡ç†å™¨ç”Ÿæˆå›å¾©
        context = {
            'user_id': user_id,
            'username': username,
            'first_name': first_name,
            'conversation_count': await self._get_conversation_count(user_id),
            'funnel_stage': await self._get_funnel_stage(user_id)
        }
        
        # ä½¿ç”¨ç­–ç•¥ç”Ÿæˆå›å¾©
        response = await self.strategy_manager.generate_response(
            message, 
            context, 
            self
        )
        
        if not response:
            return None
        
        # è³ªé‡æª¢æŸ¥
        quality_result = await self.quality_checker.check_quality(
            response,
            context,
            original_message=message
        )
        
        # å¦‚æœè³ªé‡ä¸è¶³ï¼Œé‡æ–°ç”Ÿæˆï¼ˆæœ€å¤šé‡è©¦2æ¬¡ï¼‰
        if quality_result['should_regenerate']:
            self.log(f"å›å¾©è³ªé‡ä¸è¶³ï¼ˆåˆ†æ•¸: {quality_result['quality_score']}ï¼‰ï¼Œå˜—è©¦é‡æ–°ç”Ÿæˆ...", "warning")
            for attempt in range(2):
                retry_response = await self.strategy_manager.generate_response(
                    message,
                    context,
                    self
                )
                if retry_response:
                    retry_quality = await self.quality_checker.check_quality(
                        retry_response,
                        context,
                        original_message=message
                    )
                    if not retry_quality['should_regenerate']:
                        response = retry_response
                        self.log(f"é‡æ–°ç”ŸæˆæˆåŠŸï¼ˆè³ªé‡åˆ†æ•¸: {retry_quality['quality_score']}ï¼‰", "success")
                        break
                    elif attempt == 1:
                        # æœ€å¾Œä¸€æ¬¡å˜—è©¦ï¼Œä½¿ç”¨æ›´å¥½çš„å›å¾©
                        if retry_quality['quality_score'] > quality_result['quality_score']:
                            response = retry_response
        
        if not response:
            return None
        
        # Handle based on mode
        if mode == 'full':
            # Full auto: send immediately with delay
            await self._delayed_send(user_id, response, account_phone, source_group, username)
            return response
        elif mode == 'semi':
            # Semi-auto: return response for human approval
            return response
        elif mode == 'assist':
            # Assist: just provide suggestion, don't send
            return response
        elif mode == 'keyword':
            # Keyword mode: only respond if certain conditions met
            # This is handled at a higher level
            return response
        
        return response
    
    async def _generate_response(self, user_id: str, user_message: str) -> Optional[str]:
        """Generate AI response using configured endpoint with RAG support"""
        return await self._generate_response_with_prompt(user_id, user_message, None)
    
    async def _generate_response_with_prompt(
        self, 
        user_id: str, 
        user_message: str, 
        custom_prompt: Optional[str] = None
    ) -> Optional[str]:
        """Generate AI response with custom prompt"""
        if not self.local_ai_endpoint:
            self.log("AI endpoint not configured", "warning")
            return None
        
        try:
            # Build base system prompt
            if custom_prompt:
                system_prompt = custom_prompt
            else:
                system_prompt = self.settings.get('system_prompt', '')
                if not system_prompt:
                    system_prompt = """ä½ æ˜¯æœ‹å‹èˆ¬çš„èŠå¤©åŠ©æ‰‹ã€‚å›è¦†è¦å‰‡ï¼š
1. æ¯æ¬¡å›è¦†å¿…é ˆç°¡çŸ­ï¼ˆ15-50å­—ä»¥å…§ï¼‰
2. åƒå¾®ä¿¡/TelegramèŠå¤©ä¸€æ¨£è‡ªç„¶
3. å¯ä»¥ç”¨emojiä½†ä¸è¦å¤ªå¤š
4. ç›´æ¥å›æ‡‰å•é¡Œï¼Œä¸è¦å›‰å—¦
5. èªæ°£è¼•é¬†å‹å¥½ï¼Œåƒæœ‹å‹èŠå¤©
6. ä¸è¦ä½¿ç”¨"æ‚¨"ï¼Œç”¨"ä½ "
7. é¿å…"è«‹å•é‚„æœ‰ä»€éº¼éœ€è¦å¹«åŠ©"é€™é¡å®¢æœè©±è¡“"""
            
            # === RAG: ç²å–ç›¸é—œçŸ¥è­˜åº«å…§å®¹ ===
            rag_context = ""
            if self.settings.get('rag_enabled', True):
                # æ–¹æ³•1ï¼šä½¿ç”¨æ–°çš„ TelegramRAG ç³»çµ±ï¼ˆå„ªå…ˆï¼‰
                if RAG_AVAILABLE:
                    try:
                        rag_context = await telegram_rag.build_rag_context(
                            user_message=user_message,
                            user_id=user_id,
                            max_items=3,
                            max_tokens=800
                        )
                        if rag_context:
                            self.log(f"[RAG] å¾ TelegramRAG æ‰¾åˆ°ç›¸é—œçŸ¥è­˜", "info")
                    except Exception as e:
                        self.log(f"TelegramRAG error: {e}", "warning")
                
                # æ–¹æ³•2ï¼šå¾ knowledge_learnerï¼ˆå‚™ç”¨ï¼‰
                if not rag_context:
                    try:
                        from knowledge_learner import knowledge_learner
                        learned_context = await knowledge_learner.get_relevant_context(user_message, user_id)
                        if learned_context:
                            rag_context += f"\n\n{learned_context}"
                            self.log(f"[RAG] æ‰¾åˆ°å­¸ç¿’çŸ¥è­˜", "info")
                    except Exception as e:
                        self.log(f"Knowledge learner error: {e}", "warning")
                
                # æ–¹æ³•3ï¼šå¾éœæ…‹çŸ¥è­˜åº«
                if not rag_context:
                    try:
                        from knowledge_base import search_engine
                        rag_result = await search_engine.build_rag_context(user_message, max_chunks=3)
                        if rag_result:
                            rag_context += f"\n\n[çŸ¥è­˜åº«åƒè€ƒ]\n{rag_result}"
                    except Exception as e:
                        self.log(f"RAG error: {e}", "warning")
                
                if rag_context:
                    rag_context += "\nè«‹åƒè€ƒä»¥ä¸Šä¿¡æ¯å›ç­”ï¼Œä½†ä¸è¦ç›´æ¥è¤‡è£½ã€‚"
            
            # æ·»åŠ  RAG ä¸Šä¸‹æ–‡åˆ°ç³»çµ±æç¤º
            full_system_prompt = system_prompt + rag_context
            
            # === ç²å–ç”¨æˆ¶ç•«åƒå’Œæ¼æ–—éšæ®µ ===
            profile = await db.get_user_profile(user_id)
            if profile:
                stage = profile.get('funnel_stage', 'new')
                interest = profile.get('interest_level', 1)
                stage_hint = self._get_stage_prompt(stage, interest)
                if stage_hint:
                    full_system_prompt += f"\n\n[ç”¨æˆ¶éšæ®µæç¤º]\n{stage_hint}"
            
            max_context = self.settings.get('max_context_messages', 20)
            messages = await ai_context.build_context(
                user_id=user_id,
                system_prompt=full_system_prompt,
                max_messages=max_context
            )
            
            # Add current message if not already in context
            if not messages or messages[-1].get('content') != user_message:
                messages.append({
                    "role": "user",
                    "content": user_message
                })
            
            # Call AI endpoint
            response_text = await self._call_ai_api(messages)
            
            if response_text:
                # Save user message to history (æ°¸ä¹…è¨˜æ†¶)
                await ai_context.add_message(
                    user_id=user_id,
                    role='user',
                    content=user_message
                )
                
                # Save AI response to history (æ°¸ä¹…è¨˜æ†¶)
                await ai_context.add_message(
                    user_id=user_id,
                    role='assistant',
                    content=response_text
                )
                
                # åˆ†æå°è©±ä¸¦è‡ªå‹•æ›´æ–°æ¼æ–—éšæ®µ
                await self._analyze_and_update_stage(user_id, user_message, response_text)
                
                # æå–é‡è¦ä¿¡æ¯ä¿å­˜ç‚ºé•·æœŸè¨˜æ†¶
                await self._extract_memories(user_id, user_message)
            
            return response_text
            
        except Exception as e:
            self.log(f"Error generating response: {str(e)}", "error")
            return None
    
    def _get_stage_prompt(self, stage: str, interest: int) -> str:
        """æ ¹æ“šç”¨æˆ¶éšæ®µè¿”å›æç¤º"""
        prompts = {
            'new': 'é€™æ˜¯æ–°ç”¨æˆ¶ï¼Œå‹å¥½å•å€™ä¸¦äº†è§£éœ€æ±‚ã€‚',
            'contacted': 'å·²ç™¼é€éæ¶ˆæ¯ï¼Œç­‰å¾…å›å¾©ä¸­ã€‚',
            'replied': 'ç”¨æˆ¶å·²å›å¾©ï¼Œç¹¼çºŒæ·±å…¥äº¤æµã€‚',
            'interested': f'ç”¨æˆ¶æ„Ÿèˆˆè¶£ï¼ˆèˆˆè¶£åº¦:{interest}/5ï¼‰ï¼Œå¯ä»¥ä»‹ç´¹æ›´å¤šç´°ç¯€ã€‚',
            'negotiating': 'æ­£åœ¨æ´½è«‡åƒ¹æ ¼ï¼Œå¼·èª¿åƒ¹å€¼ä¸¦æä¾›å„ªæƒ ã€‚',
            'follow_up': 'éœ€è¦è·Ÿé€²ï¼Œç™¼é€æº«å’Œæé†’ã€‚',
            'converted': 'å·²æˆäº¤å®¢æˆ¶ï¼Œæä¾›å”®å¾Œæ”¯æŒã€‚',
            'churned': 'ç”¨æˆ¶å¯èƒ½æµå¤±ï¼Œä¿æŒç¦®è²Œä¸¦ç•™ä¸‹å¥½å°è±¡ã€‚',
        }
        return prompts.get(stage, '')
    
    async def _get_conversation_count(self, user_id: str) -> int:
        """ç²å–å°è©±æ¬¡æ•¸"""
        try:
            cursor = await db._connection.execute("""
                SELECT COUNT(*) as count FROM chat_history WHERE user_id = ?
            """, (user_id,))
            row = await cursor.fetchone()
            return row['count'] if row else 0
        except:
            return 0
    
    async def _get_funnel_stage(self, user_id: str) -> str:
        """ç²å–ç”¨æˆ¶æ¼æ–—éšæ®µ"""
        try:
            profile = await db.get_user_profile(user_id)
            return profile.get('funnel_stage', 'new') if profile else 'new'
        except:
            return 'new'
    
    async def _extract_memories(self, user_id: str, message: str):
        """å¾å°è©±ä¸­æå–é‡è¦ä¿¡æ¯ä¿å­˜ç‚ºè¨˜æ†¶"""
        # æª¢æ¸¬å¯èƒ½çš„é‡è¦ä¿¡æ¯
        keywords = {
            'preference': ['å–œæ­¡', 'æƒ³è¦', 'éœ€è¦', 'åå¥½', 'æ„›', 'like', 'want', 'prefer'],
            'fact': ['æˆ‘æ˜¯', 'æˆ‘åœ¨', 'æˆ‘åš', 'æˆ‘æœ‰', 'æˆ‘çš„', "i'm", 'i am', 'my'],
        }
        
        msg_lower = message.lower()
        for mem_type, kws in keywords.items():
            if any(kw in msg_lower for kw in kws):
                # ä¿å­˜ç‚ºè¨˜æ†¶
                await db.add_ai_memory(
                    user_id=user_id,
                    memory_type=mem_type,
                    content=message[:200],
                    importance=0.6
                )
                break
    
    async def _call_ai_api(self, messages: List[Dict[str, str]]) -> Optional[str]:
        """Call the AI API endpoint"""
        try:
            self.log(f"èª¿ç”¨ AI API: endpoint={self.local_ai_endpoint}, model={self.local_ai_model}")
            
            if not self.local_ai_endpoint:
                self.log("AI endpoint æœªé…ç½®ï¼Œä½¿ç”¨å‚™ç”¨å›è¦†", "warning")
                return self._get_fallback_response(messages)
            
            request_data = {
                "model": self.local_ai_model or "default",
                "messages": messages,
                "max_tokens": 500,
                "temperature": 0.7,
                "stream": False
            }
            
            # æ™ºèƒ½æª¢æ¸¬ç«¯é»æ ¼å¼ï¼Œé¿å…è·¯å¾‘é‡è¤‡
            base_endpoint = self.local_ai_endpoint.rstrip('/')
            endpoints_to_try = []
            
            # æª¢æŸ¥ç”¨æˆ¶æ˜¯å¦å·²ç¶“æä¾›å®Œæ•´è·¯å¾‘
            if '/v1/chat/completions' in base_endpoint or '/chat/completions' in base_endpoint:
                # ç”¨æˆ¶å·²æä¾›å®Œæ•´ OpenAI æ ¼å¼è·¯å¾‘ï¼Œå„ªå…ˆä½¿ç”¨
                endpoints_to_try.append(base_endpoint)
                self.log(f"æª¢æ¸¬åˆ°å®Œæ•´ OpenAI ç«¯é»è·¯å¾‘")
            elif '/api/generate' in base_endpoint:
                # ç”¨æˆ¶å·²æä¾›å®Œæ•´ Ollama æ ¼å¼è·¯å¾‘
                endpoints_to_try.append(base_endpoint)
                self.log(f"æª¢æ¸¬åˆ°å®Œæ•´ Ollama ç«¯é»è·¯å¾‘")
            else:
                # ç”¨æˆ¶åªæä¾›åŸºç¤ URLï¼Œå˜—è©¦å¤šç¨®è·¯å¾‘
                endpoints_to_try = [
                    f"{base_endpoint}/v1/chat/completions",
                    f"{base_endpoint}/chat/completions",
                    f"{base_endpoint}/api/generate",
                    base_endpoint
                ]
                self.log(f"åŸºç¤ç«¯é»ï¼Œå°‡å˜—è©¦ {len(endpoints_to_try)} ç¨®è·¯å¾‘")
            
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=60)) as session:
                for endpoint in endpoints_to_try:
                    try:
                        self.log(f"å˜—è©¦ endpoint: {endpoint}")
                        async with session.post(endpoint, json=request_data) as response:
                            self.log(f"API å›æ‡‰ç‹€æ…‹: {response.status}")
                            if response.status == 200:
                                result = await response.json()
                                self.log(f"API å›æ‡‰æ ¼å¼: {list(result.keys()) if isinstance(result, dict) else type(result)}")
                                
                                # OpenAI format
                                if 'choices' in result:
                                    content = result['choices'][0]['message']['content']
                                    self.log(f"âœ“ ç²å¾—å›è¦† (OpenAIæ ¼å¼): {content[:50]}...")
                                    return content
                                # Ollama format
                                if 'response' in result:
                                    content = result['response']
                                    self.log(f"âœ“ ç²å¾—å›è¦† (Ollamaæ ¼å¼): {content[:50]}...")
                                    return content
                                # Direct content
                                if 'content' in result:
                                    content = result['content']
                                    self.log(f"âœ“ ç²å¾—å›è¦† (ç›´æ¥æ ¼å¼): {content[:50]}...")
                                    return content
                                
                                self.log(f"ç„¡æ³•è§£æ API å›æ‡‰: {str(result)[:200]}", "warning")
                            else:
                                text = await response.text()
                                self.log(f"API éŒ¯èª¤ {response.status}: {text[:100]}", "warning")
                                    
                    except asyncio.TimeoutError:
                        self.log(f"Endpoint {endpoint} è¶…æ™‚", "warning")
                        continue
                    except Exception as e:
                        self.log(f"Endpoint {endpoint} éŒ¯èª¤: {e}", "warning")
                        continue
            
            self.log("æ‰€æœ‰ AI endpoints éƒ½å¤±æ•—ï¼Œä½¿ç”¨å‚™ç”¨å›è¦†", "warning")
            return self._get_fallback_response(messages)
            
        except Exception as e:
            self.log(f"AI API èª¿ç”¨éŒ¯èª¤: {str(e)}", "error")
            return self._get_fallback_response(messages)
    
    def _get_fallback_response(self, messages: List[Dict[str, str]]) -> str:
        """ç•¶ AI æœå‹™ä¸å¯ç”¨æ™‚çš„å‚™ç”¨å›è¦†"""
        import random
        
        # ç²å–æœ€å¾Œä¸€æ¢ç”¨æˆ¶æ¶ˆæ¯
        last_user_msg = ""
        for msg in reversed(messages):
            if msg.get('role') == 'user':
                last_user_msg = msg.get('content', '').lower()
                break
        
        # åŸºæ–¼é—œéµè©çš„ç°¡å–®å›è¦†
        if any(kw in last_user_msg for kw in ['ä½ å¥½', 'hi', 'hello', 'å—¨']):
            responses = ['ä½ å¥½å‘€ï½ ğŸ˜Š', 'å—¨å—¨ï¼æœ‰ä»€éº¼å¯ä»¥å¹«ä½ çš„å—ï¼Ÿ', 'ä½ å¥½ï¼å¾ˆé«˜èˆˆèªè­˜ä½ ï½']
        elif any(kw in last_user_msg for kw in ['è¬è¬', 'thanks', 'thank']):
            responses = ['ä¸å®¢æ°£ï¼', 'æ²’äº‹çš„ï½ ğŸ˜„', 'å¾ˆé«˜èˆˆèƒ½å¹«åˆ°ä½ ï¼']
        elif any(kw in last_user_msg for kw in ['ï¼Ÿ', '?', 'å—', 'ä»€éº¼', 'æ€éº¼']):
            responses = ['è®“æˆ‘æƒ³æƒ³...ä½ å¯ä»¥èªªè©³ç´°ä¸€é»å—ï¼Ÿ', 'é€™å€‹å•é¡Œå¾ˆå¥½ï¼Œæˆ‘éœ€è¦äº†è§£æ›´å¤šï½', 'èƒ½å‘Šè¨´æˆ‘æ›´å¤šç´°ç¯€å—ï¼Ÿ']
        elif any(kw in last_user_msg for kw in ['åƒ¹æ ¼', 'å¤šå°‘éŒ¢', 'è²»ç”¨']):
            responses = ['åƒ¹æ ¼æœƒæ ¹æ“šéœ€æ±‚æœ‰æ‰€ä¸åŒï¼Œä½ å…·é«”æƒ³äº†è§£å“ªæ–¹é¢çš„å‘¢ï¼Ÿ', 'é€™å€‹è¦çœ‹å…·é«”éœ€æ±‚ï¼Œæ–¹ä¾¿èªªèªªä½ çš„æƒ…æ³å—ï¼Ÿ']
        else:
            responses = [
                'å¥½çš„ï¼Œæˆ‘æ˜ç™½äº†ï½',
                'å—¯å—¯ï¼Œç¹¼çºŒèªªï¼Ÿ',
                'æ”¶åˆ°ï¼é‚„æœ‰ä»€éº¼æƒ³èŠçš„å—ï¼Ÿ',
                'äº†è§£ï½ ğŸ˜Š',
                'å¥½çš„ï¼Œæœ‰ä»€éº¼éœ€è¦å¹«å¿™çš„å—ï¼Ÿ'
            ]
        
        return random.choice(responses)
    
    async def _delayed_send(self, user_id: str, response: str, 
                             account_phone: str, source_group: str, username: str):
        """Send response with realistic delay"""
        # Calculate delay
        delay_min = self.settings.get('reply_delay_min', 2)
        delay_max = self.settings.get('reply_delay_max', 8)
        delay = random.uniform(delay_min, delay_max)
        
        # Add typing simulation delay based on message length
        typing_speed = self.settings.get('typing_speed', 50)  # chars per minute
        if typing_speed > 0:
            typing_time = len(response) / typing_speed * 60
            delay += min(typing_time, 10)  # Cap typing delay at 10 seconds
        
        self.log(f"Waiting {delay:.1f}s before sending to {username}")
        await asyncio.sleep(delay)
        
        # Send via callback
        if self.send_callback:
            try:
                result = await self.send_callback(
                    account_phone=account_phone,
                    target_user_id=user_id,
                    message=response,
                    source_group=source_group,
                    username=username
                )
                if result:
                    self.log(f"âœ“ Auto-replied to {username}: {response[:50]}...")
                else:
                    self.log(f"âœ— Auto-reply failed for {username}", "warning")
            except Exception as e:
                self.log(f"Error in send callback: {e}", "error")
    
    async def _analyze_and_update_stage(self, user_id: str, user_msg: str, ai_response: str):
        """åˆ†æå°è©±ä¸¦è‡ªå‹•æ›´æ–°æ¼æ–—éšæ®µ"""
        try:
            # ç²å–å®Œæ•´èŠå¤©æ­·å²
            history = await db.get_chat_history(user_id, limit=20)
            
            # ä½¿ç”¨ AI ä¸Šä¸‹æ–‡ç®¡ç†å™¨åˆ†æéšæ®µ
            analysis = await ai_context.analyze_conversation_stage(user_id, history)
            
            new_stage = analysis.get('stage', 'replied')
            interest = analysis.get('interest_level', 2)
            
            # æ›´æ–°æ¼æ–—éšæ®µ
            await db.update_funnel_stage(
                user_id=user_id, 
                stage=new_stage,
                reason=f"è‡ªå‹•åˆ†æ: {analysis.get('suggestions', [''])[0]}"
            )
            
            # æ›´æ–°èˆˆè¶£ç¨‹åº¦
            await db.update_user_interest(user_id, interest)
            
            self.log(f"[æ¼æ–—] ç”¨æˆ¶ {user_id} éšæ®µæ›´æ–°: {new_stage}, èˆˆè¶£åº¦: {interest}/5")
            
            # ç™¼é€æ¼æ–—æ›´æ–°äº‹ä»¶åˆ°å‰ç«¯
            if self.event_callback:
                self.event_callback("funnel-updated", {
                    "userId": user_id,
                    "stage": new_stage,
                    "stageName": analysis.get('stage_name'),
                    "interestLevel": interest,
                    "suggestions": analysis.get('suggestions', [])
                })
                
        except Exception as e:
            self.log(f"Error analyzing stage: {e}", "error")
    
    async def handle_auto_greeting(self, user_id: str, username: str,
                                     account_phone: str, source_group: str = None,
                                     first_name: str = None,
                                     triggered_keyword: str = None) -> Optional[str]:
        """
        Handle automatic greeting for new users
        
        å€‹æ€§åŒ–å•å€™é‚è¼¯ï¼š
        1. æ ¹æ“šè§¸ç™¼é—œéµè©é¸æ“‡ç›¸é—œå•å€™
        2. è­˜åˆ¥è€ç”¨æˆ¶ç™¼é€ä¸åŒå•å€™
        3. ä½¿ç”¨ç”¨æˆ¶åç¨±å€‹æ€§åŒ–
        """
        # æª¢æŸ¥è‡ªå‹•å•å€™è¨­ç½® (æ•´æ•¸ 0/1)
        auto_greeting_enabled = self.settings.get('auto_greeting', 0) == 1
        if not auto_greeting_enabled:
            self.log(f"[å•å€™] è‡ªå‹•å•å€™æœªå•Ÿç”¨ (è¨­ç½®å€¼: {self.settings.get('auto_greeting', 0)})")
            return None
        
        self.log(f"[å•å€™] é–‹å§‹ç‚ºç”¨æˆ¶ {user_id} (@{username}) ç”Ÿæˆå•å€™...")
        import random
        name = first_name or username or ''
        keyword = (triggered_keyword or '').lower()
        
        # Check if we've already greeted this user (è€ç”¨æˆ¶è­˜åˆ¥)
        profile = await db.get_user_profile(user_id)
        is_returning_user = profile and profile.get('total_messages', 0) > 0
        
        # è€ç”¨æˆ¶è­˜åˆ¥
        if is_returning_user:
            previous_stage = profile.get('funnel_stage', 'new')
            last_interaction = profile.get('last_interaction')
            
            # è€ç”¨æˆ¶å€‹æ€§åŒ–å•å€™
            returning_greetings = [
                f"å—¨ {name}ï¼å¥½ä¹…ä¸è¦‹~ ğŸ˜Š",
                f"Hi {name}~ åˆè¦‹é¢å•¦ï¼",
                f"{name}ï¼Œæ­¡è¿å›ä¾†ï¼æœ‰ä»€éº¼æ–°éœ€æ±‚å—ï¼Ÿ",
                f"å“ˆå›‰ {name}ï¼ä¸Šæ¬¡èŠå¾—æ€éº¼æ¨£ï¼Ÿ",
            ]
            
            # æ ¹æ“šä¹‹å‰çš„éšæ®µèª¿æ•´å•å€™
            if previous_stage == 'interested':
                returning_greetings.append(f"{name}ï¼Œä¸Šæ¬¡ä½ å°é€™å€‹æŒºæ„Ÿèˆˆè¶£çš„ï¼Œé‚„æœ‰ä»€éº¼æƒ³äº†è§£çš„ï¼Ÿ")
            elif previous_stage == 'negotiating':
                returning_greetings.append(f"Hi {name}ï¼ä¹‹å‰èŠçš„äº‹æƒ…è€ƒæ…®å¾—æ€éº¼æ¨£äº†ï¼Ÿ")
            
            greeting = random.choice(returning_greetings) if name else "å—¨~ æ­¡è¿å›ä¾†ï¼"
            return greeting
        
        # ========== æ–°ç”¨æˆ¶å•å€™ - æ ¹æ“šé—œéµè©å€‹æ€§åŒ– ==========
        
        # é—œéµè©åˆ†é¡å•å€™æ¨¡æ¿
        keyword_greetings = {
            # æ›åŒ¯ç›¸é—œ
            'æ›åŒ¯': [
                f"å—¨ {name}ï¼çœ‹åˆ°ä½ å°æ›åŒ¯æœ‰éœ€æ±‚ï¼Œè«‹å•è¦æ›ä»€éº¼å¹£ç¨®å‘¢ï¼Ÿ",
                f"Hi {name}~ æ›åŒ¯é€™é‚Šå¯ä»¥å¹«ä½ ï¼Œä½ æƒ³æ›å¤šå°‘ï¼Ÿ",
                f"{name}ï¼Œæœ‰æ›åŒ¯éœ€æ±‚å—ï¼Ÿä»Šå¤©åŒ¯ç‡ä¸éŒ¯å–” ğŸ˜Š",
            ],
            'æ›U': [
                f"å—¨ {name}ï¼è¦æ›Uå—ï¼ŸUSDT/CNYä»Šå¤©åŒ¯ç‡å¾ˆå¥½~",
                f"Hi {name}~ Ué€™é‚Šæœ‰ï¼Œä½ éœ€è¦å¤šå°‘ï¼Ÿ",
            ],
            'usdt': [
                f"å—¨ {name}ï¼éœ€è¦USDTå—ï¼Ÿå¯ä»¥èŠèŠ~",
                f"Hi~ USDTé€™é‚Šå¯ä»¥æ“ä½œï¼Œ{name}ä½ éœ€è¦è²·é‚„æ˜¯è³£ï¼Ÿ",
            ],
            
            # æ”¯ä»˜ç›¸é—œ
            'æ”¯ä»˜': [
                f"å—¨ {name}ï¼çœ‹åˆ°ä½ éœ€è¦æ”¯ä»˜æ–¹é¢çš„å¹«åŠ©ï¼Œæ˜¯ä»€éº¼é¡å‹çš„æ”¯ä»˜å‘¢ï¼Ÿ",
                f"Hi {name}~ æ”¯ä»˜é€™å¡Šæˆ‘å¯ä»¥å¹«ä½ ï¼Œæ˜¯è·¨å¢ƒé‚„æ˜¯æœ¬åœ°çš„ï¼Ÿ",
            ],
            'ä»˜æ¬¾': [
                f"å—¨ {name}ï¼ä»˜æ¬¾é€™é‚Šå¯ä»¥å¹«ä½ è™•ç†~",
                f"Hi~ {name}æœ‰ä»€éº¼ä»˜æ¬¾éœ€æ±‚å—ï¼Ÿ",
            ],
            
            # æŠ•è³‡ç›¸é—œ
            'æŠ•è³‡': [
                f"å—¨ {name}ï¼å°æŠ•è³‡æœ‰èˆˆè¶£å—ï¼Ÿå¯ä»¥èŠèŠ~",
                f"Hi {name}~ æƒ³äº†è§£ä»€éº¼é¡å‹çš„æŠ•è³‡å‘¢ï¼Ÿ",
            ],
            'ç†è²¡': [
                f"å—¨ {name}ï¼ç†è²¡é€™é‚Šæœ‰å¾ˆå¤šé¸æ“‡ï¼Œä½ åå¥½ä»€éº¼é¡å‹ï¼Ÿ",
            ],
            
            # é€šç”¨æŸ¥è©¢
            'äº†è§£': [
                f"å—¨ {name}ï¼æƒ³äº†è§£ä»€éº¼å‘¢ï¼Ÿæˆ‘ä¾†çµ¦ä½ ä»‹ç´¹~",
                f"Hi~ {name}æœ‰ä»€éº¼æƒ³äº†è§£çš„ï¼Œç›¡ç®¡å•ï¼",
            ],
            'è«®è©¢': [
                f"å—¨ {name}ï¼æœ‰ä»€éº¼éœ€è¦è«®è©¢çš„å—ï¼Ÿ",
                f"Hi {name}~ é€™é‚Šå¯ä»¥å¹«ä½ è§£ç­”~",
            ],
        }
        
        # å˜—è©¦åŒ¹é…é—œéµè©æ¨¡æ¿
        greeting = None
        for kw, templates in keyword_greetings.items():
            if kw.lower() in keyword:
                greeting = random.choice(templates)
                break
        
        # å¦‚æœæ²’æœ‰åŒ¹é…åˆ°é—œéµè©ï¼Œä½¿ç”¨é€šç”¨å•å€™
        if not greeting:
            # ä½¿ç”¨ç”¨æˆ¶è¨­ç½®çš„å•å€™èª
            greeting = self.settings.get('greeting_message', '')
            
            if not greeting:
                # é€šç”¨å•å€™
                general_greetings = [
                    f"å—¨ {name}ï¼çœ‹åˆ°ä½ çš„æ¶ˆæ¯äº† ğŸ˜Š æœ‰ä»€éº¼å¯ä»¥å¹«ä½ çš„ï¼Ÿ",
                    f"Hi {name}~ æ­¡è¿æ­¡è¿ï¼éœ€è¦ä»€éº¼æœå‹™å—ï¼Ÿ",
                    f"å“ˆå›‰ {name}ï¼æœ‰ä»€éº¼æƒ³äº†è§£çš„å—ï¼Ÿ",
                    f"å—¨~ éœ€è¦å¹«å¿™å—ï¼Ÿæˆ‘é€™é‚Šå¯ä»¥å”åŠ©ä½  â˜ºï¸",
                ]
                greeting = random.choice(general_greetings) if name else "å—¨~ æœ‰ä»€éº¼å¯ä»¥å¹«ä½ çš„ï¼Ÿ"
        
        # Replace placeholders
        greeting = greeting.replace('{username}', username or '')
        greeting = greeting.replace('{firstName}', first_name or '')
        greeting = greeting.replace('{name}', name)
        greeting = greeting.replace('{keyword}', triggered_keyword or '')
        
        return greeting
    
    async def get_suggested_response(self, user_id: str, user_message: str) -> Optional[str]:
        """Get a suggested response without sending it (for assist mode)"""
        return await self._generate_response(user_id, user_message)
    
    async def regenerate_response(self, user_id: str) -> Optional[str]:
        """Regenerate the last response"""
        # Get the last user message
        history = await db.get_chat_history(user_id, limit=2)
        if not history:
            return None
        
        # Find last user message
        last_user_msg = None
        for msg in reversed(history):
            if msg['role'] == 'user':
                last_user_msg = msg['content']
                break
        
        if not last_user_msg:
            return None
        
        return await self._generate_response(user_id, last_user_msg)
    
    async def trigger_rag_learning(
        self,
        user_id: str,
        account_phone: str = "",
        outcome: str = "unknown"
    ) -> Dict[str, Any]:
        """
        è§¸ç™¼ RAG å­¸ç¿’
        åœ¨å°è©±çµæŸæˆ–é”åˆ°ä¸€å®šæ¶ˆæ¯æ•¸æ™‚èª¿ç”¨
        
        Args:
            user_id: ç”¨æˆ¶ ID
            account_phone: å¸³è™Ÿé›»è©±
            outcome: å°è©±çµæœ
        
        Returns:
            å­¸ç¿’çµæœ
        """
        if not RAG_AVAILABLE:
            return {'error': 'RAG ç³»çµ±ä¸å¯ç”¨'}
        
        try:
            # ä½¿ç”¨ chat_indexer è™•ç†
            await chat_indexer.on_conversation_ended(
                user_id=user_id,
                account_phone=account_phone,
                outcome=outcome
            )
            
            return {'success': True, 'message': f'å·²è§¸ç™¼ç”¨æˆ¶ {user_id} çš„ RAG å­¸ç¿’'}
            
        except Exception as e:
            self.log(f"è§¸ç™¼ RAG å­¸ç¿’å¤±æ•—: {e}", "error")
            return {'error': str(e)}
    
    async def get_rag_statistics(self) -> Dict[str, Any]:
        """ç²å– RAG ç³»çµ±çµ±è¨ˆä¿¡æ¯"""
        if not RAG_AVAILABLE:
            return {'error': 'RAG ç³»çµ±ä¸å¯ç”¨'}
        
        try:
            rag_stats = await telegram_rag.get_statistics()
            indexer_stats = await chat_indexer.get_indexing_statistics()
            
            return {
                'rag': rag_stats,
                'indexer': indexer_stats
            }
        except Exception as e:
            self.log(f"ç²å– RAG çµ±è¨ˆå¤±æ•—: {e}", "error")
            return {'error': str(e)}
    
    async def initialize_rag_system(self) -> bool:
        """åˆå§‹åŒ– RAG ç³»çµ±"""
        if not RAG_AVAILABLE:
            self.log("RAG ç³»çµ±æ¨¡çµ„ä¸å¯ç”¨", "warning")
            return False
        
        try:
            # åˆå§‹åŒ– RAG ç³»çµ±
            await telegram_rag.initialize()
            
            # åˆå§‹åŒ–ç´¢å¼•æœå‹™
            await chat_indexer.initialize()
            
            # å•Ÿå‹•å¾Œå°ç´¢å¼•
            await chat_indexer.start_background_indexing()
            
            self.log("âœ“ RAG ç³»çµ±åˆå§‹åŒ–å®Œæˆ", "success")
            return True
            
        except Exception as e:
            self.log(f"RAG ç³»çµ±åˆå§‹åŒ–å¤±æ•—: {e}", "error")
            return False


# Global instance
ai_auto_chat = AIAutoChatService()


# Import List for type hints
from typing import List
