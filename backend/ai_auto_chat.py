"""
AI Auto Chat Service
Handles automatic AI-powered responses for Telegram conversations

æ•´åˆ TelegramRAGSystem å¯¦ç¾çŸ¥è­˜å¢å¼·çš„ AI å›è¦†
"""
import asyncio
import aiohttp
import random
import json
from typing import Dict, Any, Optional, Callable, List
from datetime import datetime
from database import db
from ai_context_manager import ai_context
from ai_response_strategy import AIResponseStrategyManager
from ai_quality_checker import AIQualityChecker

# ğŸ”§ P0 å„ªåŒ–: å°å…¥å°è©±è¨˜æ†¶ç³»çµ±
try:
    from conversation_memory import get_memory_service
    MEMORY_AVAILABLE = True
except ImportError:
    MEMORY_AVAILABLE = False
    print("[AIAutoChat] å°è©±è¨˜æ†¶ç³»çµ±æœªè¼‰å…¥", file=__import__('sys').stderr)

# å°å…¥ RAG ç³»çµ±
try:
    from telegram_rag_system import telegram_rag, ConversationOutcome
    from chat_history_indexer import chat_indexer
    RAG_AVAILABLE = True
except ImportError:
    RAG_AVAILABLE = False
    print("[AIAutoChat] RAG ç³»çµ±æœªè¼‰å…¥ï¼Œä½¿ç”¨åŸºç¤æ¨¡å¼", file=__import__('sys').stderr)


class AIAutoChatService:
    """Service for AI-powered automatic chat responses"""
    
    def __init__(self):
        self.settings = {}
        self.is_running = False
        self.send_callback: Optional[Callable] = None
        self.log_callback: Optional[Callable] = None
        self.event_callback: Optional[Callable] = None
        
        # AI endpoints (will be set from settings or ai_models table)
        self.local_ai_endpoint = ""
        self.local_ai_model = ""
        self.api_key = ""
        self.provider = "custom"
        self.model_config = {}
        
        # ç­–ç•¥ç®¡ç†å™¨å’Œè³ªé‡æª¢æŸ¥å™¨
        self.strategy_manager = AIResponseStrategyManager()
        self.quality_checker = AIQualityChecker()
        
        # ğŸ†• P1-2: è¨˜éŒ„æœ€å¾Œä½¿ç”¨çš„çŸ¥è­˜ï¼ˆç”¨æ–¼å¯è¦–åŒ–ï¼‰
        self.last_knowledge_used = []
        self.last_knowledge_source = None
        
        # ğŸ†• AI è‡ªä¸»æ±ºç­–å¼•æ“ï¼ˆç„¡åŠ‡æœ¬åŒ–ï¼‰
        self.autonomous_engine = None
        self.autonomous_mode = False  # æ˜¯å¦å•Ÿç”¨è‡ªä¸»æ¨¡å¼
        
    async def initialize(self):
        """Initialize the service with settings from database"""
        import sys
        self.settings = await db.get_ai_settings()
        
        # ğŸ”§ FIX: è¼‰å…¥æ¨¡å‹ç”¨é€”åˆ†é…é…ç½®
        try:
            row = await db.fetch_one(
                "SELECT value FROM ai_settings WHERE key = 'model_usage'"
            )
            if row and row.get('value'):
                self.model_usage = json.loads(row['value'])
                print(f"[AIAutoChat] âœ“ æ¨¡å‹ç”¨é€”åˆ†é…å·²è¼‰å…¥: {self.model_usage}", file=sys.stderr)
            else:
                self.model_usage = {}
        except Exception as e:
            print(f"[AIAutoChat] è¼‰å…¥æ¨¡å‹ç”¨é€”åˆ†é…å¤±æ•—: {e}", file=sys.stderr)
            self.model_usage = {}
        
        # ğŸ”§ å„ªå…ˆå¾ ai_models è¡¨ç²å–å·²é…ç½®çš„ AI æ¨¡å‹
        try:
            # å…ˆå˜—è©¦ç²å–é»˜èªæ¨¡å‹
            model = await db.fetch_one(
                """SELECT id, provider, model_name, display_name, api_key, api_endpoint, is_local
                   FROM ai_models WHERE is_default = 1 AND (api_key != '' OR api_endpoint != '' OR is_local = 1)
                   ORDER BY id DESC LIMIT 1"""
            )
            
            # å¦‚æœæ²’æœ‰é»˜èªæ¨¡å‹ï¼Œç²å–ä»»ä½•å¯ç”¨çš„æ¨¡å‹
            if not model:
                model = await db.fetch_one(
                    """SELECT id, provider, model_name, display_name, api_key, api_endpoint, is_local
                       FROM ai_models WHERE api_key != '' OR api_endpoint != '' OR is_local = 1
                       ORDER BY id DESC LIMIT 1"""
                )
            
            if model:
                # æ ¹æ“šæ¨¡å‹é¡å‹è¨­ç½®ç«¯é»
                model_dict = dict(model) if hasattr(model, 'keys') else {
                    'id': model[0], 'provider': model[1], 'model_name': model[2],
                    'display_name': model[3], 'api_key': model[4], 'api_endpoint': model[5],
                    'is_local': model[6] if len(model) > 6 else 0
                }
                
                provider = model_dict.get('provider', '')
                api_key = model_dict.get('api_key', '')
                api_endpoint = model_dict.get('api_endpoint', '')
                model_name = model_dict.get('model_name', '')
                display_name = model_dict.get('display_name', model_name)
                is_local = model_dict.get('is_local', 0)
                
                # è¨­ç½®ç«¯é»
                if api_endpoint:
                    endpoint = api_endpoint
                elif provider == 'openai':
                    endpoint = 'https://api.openai.com/v1/chat/completions'
                elif provider == 'claude':
                    endpoint = 'https://api.anthropic.com/v1/messages'
                elif provider == 'gemini':
                    endpoint = f'https://generativelanguage.googleapis.com/v1beta/models/{model_name}:generateContent'
                else:
                    endpoint = api_endpoint or ''
                
                if endpoint:
                    self.set_ai_config(endpoint, model_name)
                    self.api_key = api_key
                    self.provider = provider
                    self.model_config = model_dict
                    print(f"[AIAutoChat] âœ“ å·²å¾ ai_models è¼‰å…¥: {display_name} ({provider}), endpoint={endpoint[:50]}...", file=sys.stderr)
                else:
                    print(f"[AIAutoChat] âš  æ¨¡å‹ {display_name} ç„¡æœ‰æ•ˆç«¯é»", file=sys.stderr)
            else:
                print(f"[AIAutoChat] âš  ai_models è¡¨ç„¡å¯ç”¨æ¨¡å‹", file=sys.stderr)
        except Exception as e:
            print(f"[AIAutoChat] å¾ ai_models è¼‰å…¥å¤±æ•—: {e}", file=sys.stderr)
        
        # å¦‚æœ ai_models æ²’æœ‰é…ç½®ï¼Œfallback åˆ°èˆŠçš„ settings è¡¨
        if not self.local_ai_endpoint and self.settings:
            endpoint = self.settings.get('local_ai_endpoint', '')
            model = self.settings.get('local_ai_model', '')
            if endpoint:
                self.set_ai_config(endpoint, model)
                print(f"[AIAutoChat] å·²å¾ settings è¼‰å…¥ AI é…ç½®: endpoint={endpoint}, model={model}", file=sys.stderr)
        
        # ğŸ†• åˆå§‹åŒ– AI è‡ªä¸»æ±ºç­–å¼•æ“
        try:
            from ai_autonomous_engine import get_autonomous_engine
            from intent_scorer import IntentScorer
            
            intent_scorer = IntentScorer()
            self.autonomous_engine = get_autonomous_engine(db, intent_scorer)
            
            # æª¢æŸ¥æ˜¯å¦å•Ÿç”¨è‡ªä¸»æ¨¡å¼
            autonomous_setting = await db.fetch_one(
                "SELECT value FROM ai_settings WHERE key = 'autonomous_mode'"
            )
            self.autonomous_mode = autonomous_setting and autonomous_setting.get('value') == '1'
            
            print(f"[AIAutoChat] âœ“ AI è‡ªä¸»å¼•æ“å·²åˆå§‹åŒ–, è‡ªä¸»æ¨¡å¼={'å•Ÿç”¨' if self.autonomous_mode else 'é—œé–‰'}", file=sys.stderr)
        except Exception as e:
            print(f"[AIAutoChat] âš  AI è‡ªä¸»å¼•æ“åˆå§‹åŒ–å¤±æ•—: {e}", file=sys.stderr)
            self.autonomous_engine = None
        
    def set_ai_config(self, endpoint: str, model: str = ""):
        """Set AI endpoint configuration"""
        self.local_ai_endpoint = endpoint
        self.local_ai_model = model
    
    async def _get_recent_ai_messages(self, user_id: str, limit: int = 5) -> List[str]:
        """
        ğŸ”§ P0: ç²å–æœ€è¿‘çš„ AI å›è¦†ï¼ˆç”¨æ–¼å»é‡ï¼‰
        """
        try:
            history = await db.get_chat_history(user_id, limit=limit * 2)
            ai_messages = [
                msg.get('content', '')
                for msg in history
                if msg.get('role') == 'assistant' and msg.get('content')
            ]
            return ai_messages[:limit]
        except Exception as e:
            self.log(f"[Anti-Repeat] ç²å–æ­·å²éŒ¯èª¤: {e}", "warning")
            return []
    
    # è©±é¡Œé—œéµè©æ˜ å°„ï¼ˆç”¨æ–¼è­˜åˆ¥è©±é¡Œï¼‰
    TOPIC_KEYWORDS = {
        'æ”¯ä»˜æ–¹æ¡ˆ': ['æ”¯ä»˜', 'ä»˜æ¬¾', 'æ”¶æ¬¾', 'ä»£æ”¶', 'ä»£ä»˜'],
        'Uå¹£å…Œæ›': ['æ›U', 'USDT', 'Uå¹£', 'å…Œæ›', 'åŒ¯ç‡'],
        'è·¨å¢ƒæ”¶æ¬¾': ['è·¨å¢ƒ', 'å¢ƒå¤–', 'åœ‹éš›', 'æµ·å¤–'],
        'è²»ç‡å ±åƒ¹': ['è²»ç‡', 'è²»ç”¨', 'åƒ¹æ ¼', 'å¤šå°‘éŒ¢', 'å ±åƒ¹', 'å„ªæƒ '],
        'æ“ä½œæµç¨‹': ['æµç¨‹', 'æ€éº¼', 'å¦‚ä½•', 'æ­¥é©Ÿ', 'æ“ä½œ'],
        'å®‰å…¨ä¿éšœ': ['å®‰å…¨', 'å¯é ', 'ä¿éšœ', 'ä¿è­‰', 'ä¿¡ä»»'],
        'åˆ°è³¬æ™‚é–“': ['å¤šä¹…', 'æ™‚é–“', 'é€Ÿåº¦', 'åˆ°è³¬', 'å¤šå¿«'],
        'æœå‹™ä»‹ç´¹': ['ä»‹ç´¹', 'ä»€éº¼æœå‹™', 'æœ‰ä»€éº¼', 'æä¾›'],
    }
    
    def _extract_covered_topics(self, ai_messages: List[str]) -> List[str]:
        """
        ğŸ”§ P0: å¾ AI å›è¦†ä¸­æå–å·²æ¶µè“‹çš„è©±é¡Œ
        """
        covered = set()
        all_text = ' '.join(ai_messages).lower()
        
        for topic, keywords in self.TOPIC_KEYWORDS.items():
            if any(kw in all_text for kw in keywords):
                covered.add(topic)
        
        return list(covered)
    
    def _identify_topic(self, text: str) -> Optional[str]:
        """è­˜åˆ¥æ–‡æœ¬ä¸­çš„ä¸»è¦è©±é¡Œ"""
        text_lower = text.lower()
        for topic, keywords in self.TOPIC_KEYWORDS.items():
            if any(kw in text_lower for kw in keywords):
                return topic
        return None
    
    async def _update_topic_tracking(self, user_id: str, user_message: str, ai_response: str):
        """
        ğŸ”§ P2: æ›´æ–°è©±é¡Œè¿½è¹¤è¨˜éŒ„
        """
        try:
            topic = self._identify_topic(user_message)
            if not topic:
                topic = self._identify_topic(ai_response)
            
            if topic:
                # ç²å–ç•¶å‰æ·±åº¦
                current_depth = await db.get_topic_depth(user_id, topic)
                new_depth = min(current_depth + 1, 5)  # æœ€å¤§æ·±åº¦ 5
                
                await db.update_topic(
                    user_id=user_id,
                    topic_name=topic,
                    depth_level=new_depth,
                    last_question=user_message[:200],
                    last_response=ai_response[:200]
                )
                self.log(f"[Topic] æ›´æ–°è©±é¡Œ '{topic}' æ·±åº¦: {current_depth} â†’ {new_depth}")
        except Exception as e:
            self.log(f"[Topic] è©±é¡Œè¿½è¹¤éŒ¯èª¤: {e}", "warning")
    
    def _build_anti_repeat_prompt(self, covered_topics: List[str], recent_messages: List[str]) -> str:
        """
        ğŸ”§ P0: æ§‹å»ºé˜²é‡è¤‡ Prompt
        """
        if not covered_topics and not recent_messages:
            return ""
        
        prompt_parts = ["\nã€é˜²é‡è¤‡ç´„æŸ - å¿…é ˆéµå®ˆã€‘"]
        
        if covered_topics:
            prompt_parts.append(f"å·²æ¶µè“‹è©±é¡Œï¼ˆç¦æ­¢ç°¡å–®é‡è¤‡ï¼‰: {', '.join(covered_topics)}")
            prompt_parts.append("â†’ å¦‚ç”¨æˆ¶è¿½å•ç›¸åŒè©±é¡Œï¼Œè«‹æä¾›ï¼šæ›´æ·±å…¥ç´°ç¯€ / æ–°è§’åº¦ / å…·é«”æ¡ˆä¾‹")
        
        if recent_messages:
            # æå–æœ€è¿‘å›è¦†çš„é—œéµçŸ­èªï¼Œé¿å…é‡è¤‡
            recent_phrases = []
            for msg in recent_messages[:3]:
                # æˆªå–å‰30å­—ä½œç‚ºåƒè€ƒ
                phrase = msg[:30].strip()
                if phrase:
                    recent_phrases.append(f'"{phrase}..."')
            if recent_phrases:
                prompt_parts.append(f"æœ€è¿‘å·²èªªéï¼ˆç¦æ­¢é‡è¤‡é–‹é ­ï¼‰: {'; '.join(recent_phrases)}")
        
        prompt_parts.append("â†’ æ¯è¼ªå›è¦†å¿…é ˆæ¨é€²å°è©±ï¼Œä¸è¦åœç•™åœ¨ç›¸åŒä¿¡æ¯å±¤ç´š")
        
        return '\n'.join(prompt_parts)
    
    async def get_model_for_usage(self, usage_type: str = 'dailyChat') -> Optional[Dict[str, Any]]:
        """
        ğŸ”§ æ ¹æ“šç”¨é€”é¡å‹ç²å–å°æ‡‰çš„ AI æ¨¡å‹é…ç½®
        
        Args:
            usage_type: ç”¨é€”é¡å‹ ('intentRecognition', 'dailyChat', 'multiRoleScript')
        
        Returns:
            æ¨¡å‹é…ç½® dict æˆ– None
        """
        import sys
        
        # ç¢ºä¿ model_usage å·²è¼‰å…¥
        if not hasattr(self, 'model_usage') or not self.model_usage:
            try:
                row = await db.fetch_one(
                    "SELECT value FROM ai_settings WHERE key = 'model_usage'"
                )
                if row and row.get('value'):
                    self.model_usage = json.loads(row['value'])
                else:
                    self.model_usage = {}
            except Exception as e:
                print(f"[AIAutoChat] ç²å– model_usage å¤±æ•—: {e}", file=sys.stderr)
                self.model_usage = {}
        
        # ç²å–è©²ç”¨é€”å°æ‡‰çš„æ¨¡å‹ ID
        model_id = self.model_usage.get(usage_type, '')
        
        if not model_id:
            print(f"[AIAutoChat] âš  ç”¨é€” '{usage_type}' æœªé…ç½®æ¨¡å‹ï¼Œä½¿ç”¨é»˜èªæ¨¡å‹", file=sys.stderr)
            return None
        
        # å¾ ai_models è¡¨ç²å–æ¨¡å‹é…ç½®
        try:
            model = await db.fetch_one(
                """SELECT id, provider, model_name, display_name, api_key, api_endpoint, is_local
                   FROM ai_models WHERE id = ?""",
                (model_id,)
            )
            
            if model:
                model_dict = dict(model) if hasattr(model, 'keys') else {
                    'id': model[0], 'provider': model[1], 'model_name': model[2],
                    'display_name': model[3], 'api_key': model[4], 'api_endpoint': model[5],
                    'is_local': model[6] if len(model) > 6 else 0
                }
                
                print(f"[AIAutoChat] âœ“ ç”¨é€” '{usage_type}' ä½¿ç”¨æ¨¡å‹: {model_dict.get('display_name')} (ID={model_id}, provider={model_dict.get('provider')})", file=sys.stderr)
                return model_dict
            else:
                print(f"[AIAutoChat] âš  æ¨¡å‹ ID={model_id} ä¸å­˜åœ¨", file=sys.stderr)
                return None
                
        except Exception as e:
            print(f"[AIAutoChat] ç²å–æ¨¡å‹é…ç½®å¤±æ•—: {e}", file=sys.stderr)
            return None
    
    def set_callbacks(self, send_callback: Callable, log_callback: Callable = None,
                      event_callback: Callable = None):
        """Set callback functions"""
        self.send_callback = send_callback
        self.log_callback = log_callback
        self.event_callback = event_callback
    
    def log(self, message: str, level: str = "info"):
        """Log a message"""
        if self.log_callback:
            self.log_callback(message, level)
        else:
            print(f"[AIAutoChat] [{level}] {message}")
    
    def _emit_event(self, event_name: str, data: Dict[str, Any]):
        """ğŸ†• ç™¼é€äº‹ä»¶åˆ°å‰ç«¯"""
        if self.event_callback:
            self.event_callback(event_name, data)
        else:
            print(f"[AIAutoChat] Event: {event_name} -> {data}", file=sys.stderr)
    
    async def update_settings(self, settings: Dict[str, Any]):
        """Update AI auto chat settings"""
        await db.update_ai_settings(settings)
        self.settings = await db.get_ai_settings()
    
    async def process_incoming_message(self, user_id: str, username: str,
                                         message: str, account_phone: str,
                                         source_group: str = None,
                                         first_name: str = None) -> Optional[str]:
        """
        Process an incoming message and generate a response if auto-chat is enabled
        
        Returns the response text if auto-reply should be sent, None otherwise
        """
        # Check if auto-chat is enabled (æ•´æ•¸ 0/1)
        auto_chat_enabled = self.settings.get('auto_chat_enabled', 0) == 1
        if not auto_chat_enabled:
            self.log(f"[AI] AI è‡ªå‹•èŠå¤©æœªå•Ÿç”¨ï¼Œè·³éè™•ç† (è¨­ç½®å€¼: {self.settings.get('auto_chat_enabled', 0)})")
            return None
        
        mode = self.settings.get('auto_chat_mode', 'semi')
        self.log(f"[AI] è™•ç†ä¾†è‡ªç”¨æˆ¶ {user_id} çš„æ¶ˆæ¯ï¼Œæ¨¡å¼: {mode}")
        
        # Save incoming message to history
        await ai_context.add_message(
            user_id=user_id,
            role='user',
            content=message,
            account_phone=account_phone,
            source_group=source_group
        )
        
        # åˆ†ææ¶ˆæ¯ä¸¦æå–é—œéµä¿¡æ¯ï¼ˆè‡ªå‹•æ›´æ–°ç”¨æˆ¶ç•«åƒã€ä¿å­˜é‡è¦è¨˜æ†¶ï¼‰
        insights = await ai_context.analyze_and_extract_insights(user_id, message, role='user')
        
        # æ›´æ–°åŸºæœ¬ç”¨æˆ¶ä¿¡æ¯ï¼ˆç”¨æˆ¶åã€åå­—ï¼‰
        await db.update_user_profile(user_id, {
            'username': username,
            'first_name': first_name or '',
        })
        
        # è¨˜éŒ„åˆ†æçµæœ
        if insights.get('suggested_stage'):
            self.log(f"ç”¨æˆ¶ {user_id} éšæ®µåˆ¤æ–·: {insights['suggested_stage']}, èˆˆè¶£åº¦: {insights.get('interest_level', 0)}")
        if insights.get('auto_tags'):
            self.log(f"ç”¨æˆ¶ {user_id} è‡ªå‹•æ¨™ç±¤: {', '.join(insights['auto_tags'])}")
        
        # Check conversation state
        state = await db.get_conversation_state(user_id)
        if state and not state.get('auto_reply_enabled', True):
            self.log(f"Auto-reply disabled for user {user_id}")
            return None
        
        # ä½¿ç”¨ç­–ç•¥ç®¡ç†å™¨ç”Ÿæˆå›å¾©
        context = {
            'user_id': user_id,
            'username': username,
            'first_name': first_name,
            'conversation_count': await self._get_conversation_count(user_id),
            'funnel_stage': await self._get_funnel_stage(user_id)
        }
        
        # ä½¿ç”¨ç­–ç•¥ç”Ÿæˆå›å¾©
        response = await self.strategy_manager.generate_response(
            message, 
            context, 
            self
        )
        
        if not response:
            return None
        
        # è³ªé‡æª¢æŸ¥
        quality_result = await self.quality_checker.check_quality(
            response,
            context,
            original_message=message
        )
        
        # å¦‚æœè³ªé‡ä¸è¶³ï¼Œé‡æ–°ç”Ÿæˆï¼ˆæœ€å¤šé‡è©¦2æ¬¡ï¼‰
        if quality_result['should_regenerate']:
            self.log(f"å›å¾©è³ªé‡ä¸è¶³ï¼ˆåˆ†æ•¸: {quality_result['quality_score']}ï¼‰ï¼Œå˜—è©¦é‡æ–°ç”Ÿæˆ...", "warning")
            for attempt in range(2):
                retry_response = await self.strategy_manager.generate_response(
                    message,
                    context,
                    self
                )
                if retry_response:
                    retry_quality = await self.quality_checker.check_quality(
                        retry_response,
                        context,
                        original_message=message
                    )
                    if not retry_quality['should_regenerate']:
                        response = retry_response
                        self.log(f"é‡æ–°ç”ŸæˆæˆåŠŸï¼ˆè³ªé‡åˆ†æ•¸: {retry_quality['quality_score']}ï¼‰", "success")
                        break
                    elif attempt == 1:
                        # æœ€å¾Œä¸€æ¬¡å˜—è©¦ï¼Œä½¿ç”¨æ›´å¥½çš„å›å¾©
                        if retry_quality['quality_score'] > quality_result['quality_score']:
                            response = retry_response
        
        if not response:
            return None
        
        # Handle based on mode
        if mode == 'full':
            # Full auto: send immediately with delay
            await self._delayed_send(user_id, response, account_phone, source_group, username)
            return response
        elif mode == 'semi':
            # Semi-auto: return response for human approval
            return response
        elif mode == 'assist':
            # Assist: just provide suggestion, don't send
            return response
        elif mode == 'keyword':
            # Keyword mode: only respond if certain conditions met
            # This is handled at a higher level
            return response
        
        return response
    
    async def _generate_response(self, user_id: str, user_message: str) -> Optional[str]:
        """Generate AI response using configured endpoint with RAG support"""
        return await self._generate_response_with_prompt(user_id, user_message, None)
    
    async def _generate_response_with_prompt(
        self, 
        user_id: str, 
        user_message: str, 
        custom_prompt: Optional[str] = None,
        usage_type: str = 'dailyChat'  # ğŸ”§ FIX: æ·»åŠ ç”¨é€”é¡å‹åƒæ•¸
    ) -> Optional[str]:
        """Generate AI response with custom prompt"""
        
        # ğŸ†• AI è‡ªä¸»æ±ºç­–å¼•æ“ï¼ˆç„¡åŠ‡æœ¬åŒ–ï¼‰
        autonomous_decision = None
        if self.autonomous_mode and self.autonomous_engine and usage_type == 'dailyChat':
            try:
                autonomous_decision = await self.autonomous_engine.analyze_and_decide(
                    user_id=user_id,
                    message=user_message
                )
                self.log(f"[è‡ªä¸»å¼•æ“] æ±ºç­–: action={autonomous_decision.action.value}, "
                        f"style={autonomous_decision.persona_style}, "
                        f"confidence={autonomous_decision.confidence:.2f}")
                
                # ä½¿ç”¨è‡ªä¸»å¼•æ“çš„ prompt å¢å¼·
                if not custom_prompt:
                    custom_prompt = autonomous_decision.prompt_enhancement
                
                # è™•ç†å”ä½œè§¸ç™¼
                if autonomous_decision.collaboration:
                    self._emit_event('ai-collaboration-trigger', {
                        'user_id': user_id,
                        'role': autonomous_decision.collaboration.value,
                        'reasoning': autonomous_decision.reasoning
                    })
                
                # è™•ç†å‰µå»ºç¾¤çµ„
                if autonomous_decision.create_group:
                    self._emit_event('ai-create-group', {
                        'user_id': user_id,
                        'reasoning': autonomous_decision.reasoning
                    })
                
                # è™•ç†äººå·¥é€šçŸ¥
                if autonomous_decision.notify_human:
                    self._emit_event('ai-notify-human', {
                        'user_id': user_id,
                        'reasoning': autonomous_decision.reasoning
                    })
                    
            except Exception as e:
                self.log(f"[è‡ªä¸»å¼•æ“] æ±ºç­–å¤±æ•—: {e}", "error")
        
        # ğŸ”§ FIX: ç²å–è©²ç”¨é€”å°æ‡‰çš„ AI æ¨¡å‹é…ç½®
        usage_model_config = await self.get_model_for_usage(usage_type)
        
        if usage_model_config:
            self.log(f"[ç”Ÿæˆå›è¦†] âœ“ ä½¿ç”¨ '{usage_type}' æ¨¡å‹: {usage_model_config.get('display_name')} (ID={usage_model_config.get('id')})")
        else:
            self.log(f"[ç”Ÿæˆå›è¦†] âš  '{usage_type}' æœªé…ç½®å°ˆç”¨æ¨¡å‹ï¼Œä½¿ç”¨é»˜èªæ¨¡å‹")
        
        # ğŸ”§ è©³ç´°è¨ºæ–·æ—¥èªŒ
        self.log(f"[ç”Ÿæˆå›è¦†] é–‹å§‹: user_id={user_id}, message={user_message[:30]}...")
        
        # æª¢æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„æ¨¡å‹
        has_model = usage_model_config or self.local_ai_endpoint
        
        if not has_model:
            self.log("[ç”Ÿæˆå›è¦†] âŒ ç„¡å¯ç”¨ AI æ¨¡å‹ï¼Œå˜—è©¦é‡æ–°åˆå§‹åŒ–...", "warning")
            # ğŸ”§ å˜—è©¦é‡æ–°åˆå§‹åŒ–
            await self.initialize()
            usage_model_config = await self.get_model_for_usage(usage_type)
            has_model = usage_model_config or self.local_ai_endpoint
            
            if not has_model:
                self.log("[ç”Ÿæˆå›è¦†] âŒ é‡æ–°åˆå§‹åŒ–å¾Œä»ç„¡å¯ç”¨æ¨¡å‹", "error")
                return None
            self.log(f"[ç”Ÿæˆå›è¦†] âœ“ é‡æ–°åˆå§‹åŒ–æˆåŠŸ")
        
        try:
            # Build base system prompt
            if custom_prompt:
                system_prompt = custom_prompt
            else:
                system_prompt = self.settings.get('system_prompt', '')
                if not system_prompt:
                    # ğŸ”§ P1 å„ªåŒ–: å„ªåŒ–é»˜èª prompt - æ·»åŠ é˜²é‡è¤‡ç´„æŸ + å°è©±æ¨é€²è¦æ±‚
                    system_prompt = """ä½ æ˜¯å°ˆæ¥­ä¸”å‹å¥½çš„æ¥­å‹™é¡§å•ã€‚

ã€æ ¸å¿ƒè¦å‰‡ - å¿…é ˆéµå®ˆã€‘
1. âš ï¸ å¿…é ˆç›´æ¥å›ç­”ç”¨æˆ¶çš„å•é¡Œ - é€™æ˜¯æœ€é‡è¦çš„è¦å‰‡
2. å¦‚æœç”¨æˆ¶å•"ä½ æœ‰ä»€éº¼"æˆ–é¡ä¼¼å•é¡Œï¼Œå¿…é ˆä»‹ç´¹ä½ çš„æ¥­å‹™/æœå‹™
3. æ ¹æ“šå°è©±æ­·å²ç†è§£ä¸Šä¸‹æ–‡ï¼Œä¸è¦ç­”éæ‰€å•
4. å›è¦†ç°¡çŸ­è‡ªç„¶ï¼ˆ20-80å­—ï¼‰ï¼Œåƒæœ‹å‹èŠå¤©
5. èªæ°£è¼•é¬†å°ˆæ¥­ï¼Œç”¨"ä½ "ä¸ç”¨"æ‚¨"
6. å¯ä»¥é©ç•¶ç”¨emojiï¼Œä½†ä¸è¦å¤ªå¤š

ã€æ¥­å‹™çŸ¥è­˜ã€‘
- æˆ‘å€‘æä¾›ï¼šæ”¯ä»˜è§£æ±ºæ–¹æ¡ˆã€è·¨å¢ƒæ”¶æ¬¾ã€Uå…Œæ›ã€ä»£æ”¶ä»£ä»˜
- å„ªå‹¢ï¼šè²»ç‡ä½ã€é€Ÿåº¦å¿«ã€å®‰å…¨å¯é 
- å¦‚æœç”¨æˆ¶è©¢å•å…·é«”åƒ¹æ ¼ï¼Œå¯ä»¥èªª"çœ‹é‡çš„ï¼Œé‡å¤§æ›´å„ªæƒ "

ã€ğŸ”´ å°è©±æ¨é€²è¦å‰‡ - æ¥µå…¶é‡è¦ã€‘
1. âš ï¸ ç¦æ­¢é‡è¤‡å·²èªªéçš„å…§å®¹ - å¦‚æœä½ ä¹‹å‰ä»‹ç´¹éæœå‹™ï¼Œä¸‹æ¬¡è¦æ›´æ·±å…¥
2. âš ï¸ æ¯è¼ªå›è¦†å¿…é ˆæ¨é€²å°è©± - ä¸è¦åœç•™åœ¨ç›¸åŒä¿¡æ¯å±¤ç´š
3. å¦‚æœç”¨æˆ¶è¿½å•åŒä¸€è©±é¡Œï¼Œæä¾›ï¼šæ›´å…·é«”çš„æ•¸å­—/æ¡ˆä¾‹/ç´°ç¯€
4. å¦‚æœç”¨æˆ¶èªª"çµ¦æˆ‘æ–¹æ¡ˆ"ï¼Œçµ¦å‡ºå…·é«”æ–¹æ¡ˆè€Œéæ³›æ³›ä»‹ç´¹
5. æ ¹æ“šç”¨æˆ¶çš„å›æ‡‰èª¿æ•´ç­–ç•¥ï¼Œä¸è¦æ©Ÿæ¢°é‡è¤‡

ã€å°è©±æ¨é€²ç¤ºä¾‹ã€‘
- ç¬¬ä¸€æ¬¡å•æœå‹™ â†’ ç°¡å–®ä»‹ç´¹ä¸‰å¤§é¡
- ç¬¬äºŒæ¬¡è¿½å• â†’ æ ¹æ“šèˆˆè¶£æ·±å…¥æŸä¸€é¡
- ç¬¬ä¸‰æ¬¡ â†’ çµ¦å‡ºå…·é«”è²»ç‡/æµç¨‹/æ¡ˆä¾‹
- ç¬¬å››æ¬¡ â†’ æ¨å‹•æˆäº¤/è¦è¯ç¹«æ–¹å¼

ã€ç¦æ­¢è¡Œç‚ºã€‘
- ä¸è¦èªª"è«‹å•é‚„æœ‰ä»€éº¼éœ€è¦å¹«åŠ©"
- ä¸è¦åå•å¤ªå¤šï¼Œå…ˆçµ¦ä¿¡æ¯
- ä¸è¦å›é¿å•é¡Œæˆ–ç­”éæ‰€å•
- ä¸è¦ç”Ÿæˆéé•·å›è¦†
- âŒ ä¸è¦é‡è¤‡ä¸Šä¸€æ¢å›è¦†çš„é–‹é ­æˆ–æ ¸å¿ƒå…§å®¹"""
            
            # === ğŸ”§ Phase 6: å¼·åŒ–çŸ¥è­˜åº«æŸ¥è©¢ï¼ˆå„ªå…ˆä½¿ç”¨çŸ¥è­˜åº«ï¼‰ ===
            knowledge_context = ""
            # ğŸ†• P1-2: æ¸…ç©ºä¸¦è¨˜éŒ„æœ¬æ¬¡ä½¿ç”¨çš„çŸ¥è­˜
            self.last_knowledge_used = []
            self.last_knowledge_source = None
            
            # ğŸ”§ Phase 6: è©³ç´°æ—¥èªŒ
            self.log(f"[Knowledge] ğŸ” é–‹å§‹æŸ¥è©¢çŸ¥è­˜åº«ï¼Œé—œéµè©: {user_message[:50]}...")
            
            try:
                knowledge_items = await db.search_knowledge(user_message, limit=3)
                self.log(f"[Knowledge] ğŸ“Š ai_knowledge_base æŸ¥è©¢çµæœ: {len(knowledge_items) if knowledge_items else 0} æ¢")
                
                if knowledge_items:
                    knowledge_parts = []
                    for item in knowledge_items:
                        title = item.get('title', '')
                        content = item.get('content', '')
                        knowledge_parts.append(f"- {title}: {content}")
                        self.log(f"[Knowledge] ğŸ“– ä½¿ç”¨çŸ¥è­˜: {title[:30]}...")
                        # å¢åŠ ä½¿ç”¨è¨ˆæ•¸
                        await db.increment_knowledge_use(item.get('id'))
                        # ğŸ†• P1-2: è¨˜éŒ„ä½¿ç”¨çš„çŸ¥è­˜
                        self.last_knowledge_used.append({
                            'id': item.get('id'),
                            'title': title,
                            'content': content[:100],
                            'source': 'KnowledgeBase'
                        })
                    knowledge_context = "\nã€æ¥­å‹™çŸ¥è­˜åƒè€ƒ - å¿…é ˆä½¿ç”¨ä»¥ä¸‹ä¿¡æ¯å›ç­”ã€‘\n" + "\n".join(knowledge_parts)
                    self.last_knowledge_source = 'KnowledgeBase'
                    self.log(f"[Knowledge] âœ“ æ‰¾åˆ° {len(knowledge_items)} æ¢ç›¸é—œçŸ¥è­˜ï¼Œå·²æ·»åŠ åˆ° prompt")
                else:
                    self.log(f"[Knowledge] âš  ai_knowledge_base ç„¡åŒ¹é…çµæœï¼Œå˜—è©¦ RAG ç³»çµ±...")
            except Exception as e:
                self.log(f"[Knowledge] âŒ æŸ¥è©¢çŸ¥è­˜åº«éŒ¯èª¤: {e}", "warning")
            
            # === RAG: ç²å–ç›¸é—œçŸ¥è­˜åº«å…§å®¹ ===
            rag_context = ""
            rag_enabled = self.settings.get('rag_enabled', True)
            self.log(f"[RAG] ğŸ” RAG æŸ¥è©¢: enabled={rag_enabled}, RAG_AVAILABLE={RAG_AVAILABLE}")
            
            if rag_enabled:
                # æ–¹æ³•1ï¼šä½¿ç”¨æ–°çš„ TelegramRAG ç³»çµ±ï¼ˆå„ªå…ˆï¼‰
                if RAG_AVAILABLE:
                    try:
                        self.log(f"[RAG] ğŸ” èª¿ç”¨ telegram_rag.build_rag_context...")
                        rag_context = await telegram_rag.build_rag_context(
                            user_message=user_message,
                            user_id=user_id,
                            max_items=3,
                            max_tokens=800
                        )
                        if rag_context:
                            self.log(f"[RAG] âœ“ TelegramRAG æ‰¾åˆ°çŸ¥è­˜: {rag_context[:80]}...")
                            # ğŸ†• P1-2: è¨˜éŒ„ RAG çŸ¥è­˜
                            if not self.last_knowledge_source:
                                self.last_knowledge_source = 'RAG'
                            self.last_knowledge_used.append({
                                'source': 'RAG',
                                'content': rag_context[:150] + '...' if len(rag_context) > 150 else rag_context
                            })
                        else:
                            self.log(f"[RAG] âš  TelegramRAG è¿”å›ç©ºçµæœ")
                    except Exception as e:
                        self.log(f"[RAG] âŒ TelegramRAG éŒ¯èª¤: {e}", "warning")
                
                # æ–¹æ³•2ï¼šå¾ knowledge_learnerï¼ˆå‚™ç”¨ï¼‰
                if not rag_context:
                    try:
                        from knowledge_learner import knowledge_learner
                        learned_context = await knowledge_learner.get_relevant_context(user_message, user_id)
                        if learned_context:
                            rag_context += f"\n\n{learned_context}"
                            self.log(f"[RAG] æ‰¾åˆ°å­¸ç¿’çŸ¥è­˜", "info")
                    except Exception as e:
                        self.log(f"Knowledge learner error: {e}", "warning")
                
                # æ–¹æ³•3ï¼šå¾éœæ…‹çŸ¥è­˜åº«
                if not rag_context:
                    try:
                        from knowledge_base import search_engine
                        rag_result = await search_engine.build_rag_context(user_message, max_chunks=3)
                        if rag_result:
                            rag_context += f"\n\n[çŸ¥è­˜åº«åƒè€ƒ]\n{rag_result}"
                    except Exception as e:
                        self.log(f"RAG error: {e}", "warning")
                
                if rag_context:
                    rag_context += "\nè«‹åƒè€ƒä»¥ä¸Šä¿¡æ¯å›ç­”ï¼Œä½†ä¸è¦ç›´æ¥è¤‡è£½ã€‚"
            
            # æ·»åŠ çŸ¥è­˜åº«å’Œ RAG ä¸Šä¸‹æ–‡åˆ°ç³»çµ±æç¤º
            full_system_prompt = system_prompt + knowledge_context + rag_context
            
            # === ğŸ”§ P0 å„ªåŒ–: æ¥å…¥å°è©±è¨˜æ†¶ç³»çµ± ===
            memory_context = ""
            if MEMORY_AVAILABLE:
                try:
                    memory_service = get_memory_service()
                    memory_context = await memory_service.generate_memory_prompt(user_id, user_message)
                    if memory_context:
                        full_system_prompt += f"\n\n{memory_context}"
                        self.log(f"[Memory] è¼‰å…¥å°è©±è¨˜æ†¶ä¸Šä¸‹æ–‡", "info")
                except Exception as mem_err:
                    self.log(f"[Memory] è¨˜æ†¶ç³»çµ±éŒ¯èª¤: {mem_err}", "warning")
            
            # === ğŸ†• P0 å„ªåŒ–: è¼‰å…¥æ“ä½œä¸Šä¸‹æ–‡ï¼ˆç¾¤é‚€è«‹ç­‰ï¼‰ ===
            if MEMORY_AVAILABLE:
                try:
                    memory_service = get_memory_service()
                    action_context = await memory_service.generate_action_context_prompt(user_id)
                    if action_context:
                        full_system_prompt += f"\n\n{action_context}"
                        self.log(f"[ActionContext] è¼‰å…¥æ“ä½œä¸Šä¸‹æ–‡", "info")
                except Exception as action_err:
                    self.log(f"[ActionContext] è¼‰å…¥æ“ä½œä¸Šä¸‹æ–‡å¤±æ•—: {action_err}", "warning")
            
            # === ğŸ†• P2-2 å„ªåŒ–: è¼‰å…¥çµ±ä¸€å°è©±ç­–ç•¥ï¼ˆç§èŠ/ç¾¤èŠå”èª¿ï¼‰ ===
            if MEMORY_AVAILABLE:
                try:
                    memory_service = get_memory_service()
                    strategy_context = await memory_service.get_unified_strategy_prompt(user_id, 'private')
                    if strategy_context:
                        full_system_prompt += f"\n\n{strategy_context}"
                        self.log(f"[Strategy] è¼‰å…¥çµ±ä¸€å°è©±ç­–ç•¥", "info")
                except Exception as strat_err:
                    self.log(f"[Strategy] è¼‰å…¥ç­–ç•¥å¤±æ•—: {strat_err}", "warning")
            
            # === ğŸ”§ P0 å„ªåŒ–: æ·»åŠ å›è¦†å»é‡æ©Ÿåˆ¶ ===
            try:
                recent_ai_messages = await self._get_recent_ai_messages(user_id, limit=5)
                if recent_ai_messages:
                    covered_topics = self._extract_covered_topics(recent_ai_messages)
                    anti_repeat_prompt = self._build_anti_repeat_prompt(covered_topics, recent_ai_messages)
                    if anti_repeat_prompt:
                        full_system_prompt += anti_repeat_prompt
                        self.log(f"[Anti-Repeat] å·²æ¶µè“‹è©±é¡Œ: {covered_topics}", "info")
            except Exception as ar_err:
                self.log(f"[Anti-Repeat] éŒ¯èª¤: {ar_err}", "warning")
            
            # === ç²å–ç”¨æˆ¶ç•«åƒå’Œæ¼æ–—éšæ®µ ===
            profile = await db.get_user_profile(user_id)
            if profile:
                stage = profile.get('funnel_stage', 'new')
                interest = profile.get('interest_level', 1)
                stage_hint = self._get_stage_prompt(stage, interest)
                if stage_hint:
                    full_system_prompt += f"\n\n[ç”¨æˆ¶éšæ®µæç¤º]\n{stage_hint}"
            
            max_context = self.settings.get('max_context_messages', 20)
            
            # ğŸ”§ Phase 1 è¨ºæ–·ï¼šè¿½è¹¤ä¸Šä¸‹æ–‡æ§‹å»º
            self.log(f"[ç”Ÿæˆå›è¦†] ğŸ“ æº–å‚™æ§‹å»ºä¸Šä¸‹æ–‡...")
            
            try:
                messages = await ai_context.build_context(
                    user_id=user_id,
                    system_prompt=full_system_prompt,
                    max_messages=max_context
                )
                self.log(f"[ç”Ÿæˆå›è¦†] âœ“ ä¸Šä¸‹æ–‡æ§‹å»ºå®Œæˆï¼Œæ¶ˆæ¯æ•¸: {len(messages) if messages else 0}")
            except Exception as ctx_err:
                self.log(f"[ç”Ÿæˆå›è¦†] âŒ ä¸Šä¸‹æ–‡æ§‹å»ºå¤±æ•—: {ctx_err}", "error")
                # ä½¿ç”¨ç°¡åŒ–çš„æ¶ˆæ¯åˆ—è¡¨
                messages = [
                    {"role": "system", "content": full_system_prompt[:2000]},
                    {"role": "user", "content": user_message}
                ]
                self.log(f"[ç”Ÿæˆå›è¦†] ä½¿ç”¨ç°¡åŒ–æ¶ˆæ¯åˆ—è¡¨")
            
            # Add current message if not already in context
            if not messages or messages[-1].get('content') != user_message:
                messages.append({
                    "role": "user",
                    "content": user_message
                })
            
            # ğŸ”§ è¨ºæ–·ï¼šé¡¯ç¤ºå³å°‡ç™¼é€çš„æ¶ˆæ¯
            self.log(f"[ç”Ÿæˆå›è¦†] ğŸ“¤ èª¿ç”¨ APIï¼Œæ¶ˆæ¯æ•¸: {len(messages)}, prompté•·åº¦: {len(full_system_prompt)}")
            
            # ğŸ”§ FIX: å‚³éç”¨é€”å°æ‡‰çš„æ¨¡å‹é…ç½®
            response_text = await self._call_ai_api(messages, model_config=usage_model_config)
            
            # ğŸ”§ è¨ºæ–·ï¼šé¡¯ç¤º API è¿”å›çµæœ
            if response_text:
                self.log(f"[ç”Ÿæˆå›è¦†] âœ“ API è¿”å›: {response_text[:50]}...")
            else:
                self.log(f"[ç”Ÿæˆå›è¦†] âŒ API è¿”å›ç©ºå€¼", "warning")
            
            if response_text:
                # Save user message to history (æ°¸ä¹…è¨˜æ†¶)
                await ai_context.add_message(
                    user_id=user_id,
                    role='user',
                    content=user_message
                )
                
                # Save AI response to history (æ°¸ä¹…è¨˜æ†¶)
                await ai_context.add_message(
                    user_id=user_id,
                    role='assistant',
                    content=response_text
                )
                
                # åˆ†æå°è©±ä¸¦è‡ªå‹•æ›´æ–°æ¼æ–—éšæ®µ
                await self._analyze_and_update_stage(user_id, user_message, response_text)
                
                # æå–é‡è¦ä¿¡æ¯ä¿å­˜ç‚ºé•·æœŸè¨˜æ†¶
                await self._extract_memories(user_id, user_message, response_text)
                
                # ğŸ”§ P2 å„ªåŒ–: æ›´æ–°è©±é¡Œè¿½è¹¤
                await self._update_topic_tracking(user_id, user_message, response_text)
            
            return response_text
            
        except Exception as e:
            self.log(f"Error generating response: {str(e)}", "error")
            return None
    
    def _get_stage_prompt(self, stage: str, interest: int) -> str:
        """æ ¹æ“šç”¨æˆ¶éšæ®µè¿”å›æç¤º"""
        prompts = {
            'new': 'é€™æ˜¯æ–°ç”¨æˆ¶ï¼Œå‹å¥½å•å€™ä¸¦äº†è§£éœ€æ±‚ã€‚',
            'contacted': 'å·²ç™¼é€éæ¶ˆæ¯ï¼Œç­‰å¾…å›å¾©ä¸­ã€‚',
            'replied': 'ç”¨æˆ¶å·²å›å¾©ï¼Œç¹¼çºŒæ·±å…¥äº¤æµã€‚',
            'interested': f'ç”¨æˆ¶æ„Ÿèˆˆè¶£ï¼ˆèˆˆè¶£åº¦:{interest}/5ï¼‰ï¼Œå¯ä»¥ä»‹ç´¹æ›´å¤šç´°ç¯€ã€‚',
            'negotiating': 'æ­£åœ¨æ´½è«‡åƒ¹æ ¼ï¼Œå¼·èª¿åƒ¹å€¼ä¸¦æä¾›å„ªæƒ ã€‚',
            'follow_up': 'éœ€è¦è·Ÿé€²ï¼Œç™¼é€æº«å’Œæé†’ã€‚',
            'converted': 'å·²æˆäº¤å®¢æˆ¶ï¼Œæä¾›å”®å¾Œæ”¯æŒã€‚',
            'churned': 'ç”¨æˆ¶å¯èƒ½æµå¤±ï¼Œä¿æŒç¦®è²Œä¸¦ç•™ä¸‹å¥½å°è±¡ã€‚',
        }
        return prompts.get(stage, '')
    
    async def _get_conversation_count(self, user_id: str) -> int:
        """ç²å–å°è©±æ¬¡æ•¸"""
        try:
            cursor = await db._connection.execute("""
                SELECT COUNT(*) as count FROM chat_history WHERE user_id = ?
            """, (user_id,))
            row = await cursor.fetchone()
            return row['count'] if row else 0
        except:
            return 0
    
    async def _get_funnel_stage(self, user_id: str) -> str:
        """ç²å–ç”¨æˆ¶æ¼æ–—éšæ®µ"""
        try:
            profile = await db.get_user_profile(user_id)
            return profile.get('funnel_stage', 'new') if profile else 'new'
        except:
            return 'new'
    
    async def _extract_memories(self, user_id: str, message: str, ai_response: str = ""):
        """å¾å°è©±ä¸­æå–é‡è¦ä¿¡æ¯ä¿å­˜ç‚ºè¨˜æ†¶"""
        # ğŸ†• Phase1: ä½¿ç”¨æ–°çš„è¨˜æ†¶æœå‹™
        if self.autonomous_engine and self.autonomous_engine.memory_service:
            try:
                await self.autonomous_engine.memory_service.extract_and_store_memories(
                    user_id, message, ai_response
                )
                return
            except Exception as e:
                print(f"[AIAutoChat] æ–°è¨˜æ†¶æœå‹™å¤±æ•—ï¼Œä½¿ç”¨èˆŠç‰ˆ: {e}", file=sys.stderr)
        
        # èˆŠç‰ˆé‚è¼¯ï¼ˆå…¼å®¹ï¼‰
        keywords = {
            'preference': ['å–œæ­¡', 'æƒ³è¦', 'éœ€è¦', 'åå¥½', 'æ„›', 'like', 'want', 'prefer'],
            'fact': ['æˆ‘æ˜¯', 'æˆ‘åœ¨', 'æˆ‘åš', 'æˆ‘æœ‰', 'æˆ‘çš„', "i'm", 'i am', 'my'],
        }
        
        msg_lower = message.lower()
        for mem_type, kws in keywords.items():
            if any(kw in msg_lower for kw in kws):
                await db.add_ai_memory(
                    user_id=user_id,
                    memory_type=mem_type,
                    content=message[:200],
                    importance=0.6
                )
                break
    
    async def _call_ai_api(self, messages: List[Dict[str, str]], model_config: Optional[Dict[str, Any]] = None) -> Optional[str]:
        """
        Call the AI API endpoint - æ”¯æŒå¤šç¨® AI æä¾›å•†
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            model_config: ğŸ”§ å¯é¸çš„å‹•æ…‹æ¨¡å‹é…ç½®ï¼ˆè¦†è“‹é»˜èªé…ç½®ï¼‰
        """
        import sys
        print(f"[AIAutoChat] âš¡ _call_ai_api è¢«èª¿ç”¨ï¼Œmodel_config={'æœ‰' if model_config else 'ç„¡'}", file=sys.stderr)
        try:
            # ğŸ”§ FIX: å„ªå…ˆä½¿ç”¨å‚³å…¥çš„æ¨¡å‹é…ç½®
            if model_config:
                provider = model_config.get('provider', 'custom')
                api_key = model_config.get('api_key', '')
                endpoint = model_config.get('api_endpoint', '')
                model_name = model_config.get('model_name', '')
                display_name = model_config.get('display_name', model_name)
                
                # æ§‹å»ºç«¯é» URL
                if not endpoint:
                    if provider == 'openai':
                        endpoint = 'https://api.openai.com/v1/chat/completions'
                    elif provider == 'claude':
                        endpoint = 'https://api.anthropic.com/v1/messages'
                    elif provider == 'gemini':
                        endpoint = f'https://generativelanguage.googleapis.com/v1beta/models/{model_name}:generateContent'
                
                self.log(f"ğŸ”§ [å‹•æ…‹é…ç½®] ä½¿ç”¨æ¨¡å‹: {display_name} (provider={provider})")
            else:
                provider = getattr(self, 'provider', 'custom')
                api_key = getattr(self, 'api_key', '')
                endpoint = self.local_ai_endpoint
                model_name = self.local_ai_model
                display_name = model_name
            
            import sys
            print(f"[AIAutoChat] ğŸ“¤ èª¿ç”¨ AI API: provider={provider}, model={model_name}, endpoint={endpoint[:80] if endpoint else 'None'}...", file=sys.stderr)
            
            # ğŸ”§ FIX: ä½¿ç”¨ endpoint è®Šé‡è€Œä¸æ˜¯ self.local_ai_endpoint
            if not endpoint and provider not in ['openai', 'claude', 'gemini']:
                self.log("AI endpoint æœªé…ç½®ï¼Œä½¿ç”¨å‚™ç”¨å›è¦†", "warning")
                return self._get_fallback_response(messages)
            
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=60)) as session:
                
                # ========== OpenAI API ==========
                if provider == 'openai':
                    openai_endpoint = 'https://api.openai.com/v1/chat/completions'
                    headers = {
                        'Authorization': f'Bearer {api_key}',
                        'Content-Type': 'application/json'
                    }
                    
                    # ğŸ”§ Phase 1 è¨ºæ–·ï¼šé™åˆ¶æ¶ˆæ¯æ•¸é‡å’Œé•·åº¦
                    limited_messages = messages[-10:] if len(messages) > 10 else messages
                    total_chars = sum(len(m.get('content', '')) for m in limited_messages)
                    
                    # ğŸ”§ Phase 2ï¼šå¦‚æœæ¶ˆæ¯éé•·ï¼Œæˆªæ–·
                    if total_chars > 8000:
                        self.log(f"âš ï¸ æ¶ˆæ¯éé•· ({total_chars} å­—ç¬¦)ï¼Œé€²è¡Œæˆªæ–·", "warning")
                        for m in limited_messages:
                            if len(m.get('content', '')) > 2000:
                                m['content'] = m['content'][:2000] + '...[å·²æˆªæ–·]'
                    
                    request_data = {
                        "model": model_name or "gpt-4o-mini",
                        "messages": limited_messages,
                        "max_tokens": 500,
                        "temperature": 0.7
                    }
                    
                    # ğŸ”§ Phase 1 è¨ºæ–·ï¼šè©³ç´°æ—¥èªŒ
                    self.log(f"ğŸ“¤ OpenAI è«‹æ±‚: model={model_name}, messages={len(limited_messages)}, total_chars={total_chars}")
                    self.log(f"ğŸ“¤ æœ€å¾Œä¸€æ¢ç”¨æˆ¶æ¶ˆæ¯: {limited_messages[-1].get('content', '')[:100]}...")
                    
                    async with session.post(openai_endpoint, headers=headers, json=request_data) as response:
                        response_text = await response.text()
                        
                        # ğŸ”§ Phase 1 è¨ºæ–·ï¼šè¨˜éŒ„å®Œæ•´éŸ¿æ‡‰
                        self.log(f"ğŸ“¥ OpenAI éŸ¿æ‡‰: status={response.status}, length={len(response_text)}")
                        
                        if response.status == 200:
                            try:
                                result = json.loads(response_text)
                                content = result.get('choices', [{}])[0].get('message', {}).get('content', '')
                                
                                # ğŸ”§ Phase 1 è¨ºæ–·ï¼šæª¢æŸ¥ finish_reason
                                finish_reason = result.get('choices', [{}])[0].get('finish_reason', 'unknown')
                                self.log(f"ğŸ“¥ finish_reason={finish_reason}")
                                
                                if content:
                                    self.log(f"âœ… OpenAI å›è¦†æˆåŠŸ: {content[:80]}...")
                                    return content
                                else:
                                    self.log(f"âŒ OpenAI è¿”å›ç©º contentï¼Œå®Œæ•´éŸ¿æ‡‰: {response_text[:300]}", "warning")
                            except json.JSONDecodeError as je:
                                self.log(f"âŒ OpenAI éŸ¿æ‡‰ JSON è§£æå¤±æ•—: {je}, åŸå§‹: {response_text[:200]}", "error")
                        else:
                            self.log(f"âŒ OpenAI éŒ¯èª¤ {response.status}: {response_text[:200]}", "error")
                            
                            # ğŸ”§ Phase 1ï¼šç‰¹æ®ŠéŒ¯èª¤è™•ç†
                            if response.status == 401:
                                self.log("âŒ API Key ç„¡æ•ˆæˆ–å·²éæœŸ", "error")
                            elif response.status == 429:
                                self.log("âŒ API è«‹æ±‚éæ–¼é »ç¹ï¼Œè«‹ç¨å¾Œå†è©¦", "error")
                            elif response.status == 402:
                                self.log("âŒ API é¤˜é¡ä¸è¶³", "error")
                
                # ========== Claude API ==========
                elif provider == 'claude':
                    claude_endpoint = 'https://api.anthropic.com/v1/messages'
                    headers = {
                        'x-api-key': api_key,
                        'anthropic-version': '2023-06-01',
                        'Content-Type': 'application/json'
                    }
                    # Claude æ ¼å¼è½‰æ›
                    claude_messages = [{"role": m["role"], "content": m["content"]} for m in messages if m["role"] != "system"]
                    system_msg = next((m["content"] for m in messages if m["role"] == "system"), None)
                    
                    request_data = {
                        "model": model_name or "claude-3-5-sonnet-latest",  # ğŸ”§ FIX
                        "max_tokens": 500,
                        "messages": claude_messages
                    }
                    if system_msg:
                        request_data["system"] = system_msg
                    
                    self.log(f"èª¿ç”¨ Claude: {model_name}")
                    async with session.post(claude_endpoint, headers=headers, json=request_data) as response:
                        if response.status == 200:
                            result = await response.json()
                            content = result.get('content', [{}])[0].get('text', '')
                            if content:
                                self.log(f"âœ“ Claude å›è¦†: {content[:50]}...")
                                return content
                        else:
                            text = await response.text()
                            self.log(f"Claude éŒ¯èª¤ {response.status}: {text[:100]}", "warning")
                
                # ========== Gemini API ==========
                elif provider == 'gemini':
                    gemini_model = model_name or 'gemini-1.5-flash-latest'  # ğŸ”§ FIX
                    gemini_endpoint = f'https://generativelanguage.googleapis.com/v1beta/models/{gemini_model}:generateContent?key={api_key}'
                    
                    # Gemini æ ¼å¼è½‰æ›
                    gemini_contents = []
                    for m in messages:
                        if m["role"] == "system":
                            continue  # Gemini ä¸æ”¯æŒ system roleï¼Œæœƒåˆä½µåˆ°ç¬¬ä¸€æ¢ user æ¶ˆæ¯
                        role = "user" if m["role"] == "user" else "model"
                        gemini_contents.append({"role": role, "parts": [{"text": m["content"]}]})
                    
                    request_data = {
                        "contents": gemini_contents,
                        "generationConfig": {"maxOutputTokens": 500, "temperature": 0.7}
                    }
                    
                    self.log(f"èª¿ç”¨ Gemini: {gemini_model}")
                    async with session.post(gemini_endpoint, json=request_data) as response:
                        if response.status == 200:
                            result = await response.json()
                            content = result.get('candidates', [{}])[0].get('content', {}).get('parts', [{}])[0].get('text', '')
                            if content:
                                self.log(f"âœ“ Gemini å›è¦†: {content[:50]}...")
                                return content
                        else:
                            text = await response.text()
                            self.log(f"Gemini éŒ¯èª¤ {response.status}: {text[:100]}", "warning")
                
                # ========== è‡ªå®šç¾©/æœ¬åœ° AI (OpenAI å…¼å®¹æ ¼å¼) ==========
                else:
                    import sys
                    print(f"[AIAutoChat] ğŸ”§ é€²å…¥ Custom AI åˆ†æ”¯: provider={provider}, endpoint={endpoint[:80] if endpoint else 'None'}...", file=sys.stderr)
                    
                    request_data = {
                        "model": model_name or "default",  # ğŸ”§ FIX
                        "messages": messages,
                        "max_tokens": 500,
                        "temperature": 0.7,
                        "stream": False
                    }
                    
                    print(f"[Custom AI] æº–å‚™è«‹æ±‚: model={model_name}, messages_count={len(messages)}", file=sys.stderr)
                    
                    # ğŸ”§ FIX: ä½¿ç”¨ endpoint è®Šé‡
                    base_endpoint = endpoint.rstrip('/') if endpoint else ''
                    
                    if not base_endpoint:
                        self.log("[Custom AI] âŒ ç„¡æœ‰æ•ˆç«¯é»", "error")
                        return self._get_fallback_response(messages)
                    
                    endpoints_to_try = []
                    is_ollama = ':11434' in base_endpoint or '.ts.net' in base_endpoint
                    
                    if '/v1/chat/completions' in base_endpoint or '/chat/completions' in base_endpoint:
                        endpoints_to_try.append(base_endpoint)
                        self.log(f"[Custom AI] æª¢æ¸¬åˆ° chat/completions æ ¼å¼")
                    elif '/api/generate' in base_endpoint or '/api/chat' in base_endpoint:
                        endpoints_to_try.append(base_endpoint)
                        self.log(f"[Custom AI] æª¢æ¸¬åˆ° api/chat æ ¼å¼")
                        # ğŸ”§ FIX: Ollama éœ€è¦ç‰¹æ®Šçš„è«‹æ±‚æ ¼å¼
                        if is_ollama or '/api/chat' in base_endpoint:
                            request_data = {
                                "model": model_name or "llama3",
                                "messages": messages,
                                "stream": False,
                                "options": {"num_predict": 500}
                            }
                            self.log(f"[Custom AI] ä½¿ç”¨ Ollama è«‹æ±‚æ ¼å¼")
                    elif is_ollama:
                        # ğŸ”§ FIX: æª¢æ¸¬åˆ° Ollama ç«¯å£ï¼Œä½¿ç”¨ /api/chat
                        endpoints_to_try = [f"{base_endpoint}/api/chat"]
                        request_data = {
                            "model": model_name or "llama3",
                            "messages": messages,
                            "stream": False,
                            "options": {"num_predict": 500}
                        }
                        self.log(f"[Custom AI] æª¢æ¸¬åˆ° Ollamaï¼Œä½¿ç”¨ /api/chat æ ¼å¼")
                    else:
                        endpoints_to_try = [
                            f"{base_endpoint}/v1/chat/completions",
                            f"{base_endpoint}/chat/completions",
                            base_endpoint
                        ]
                        self.log(f"[Custom AI] å°‡å˜—è©¦ {len(endpoints_to_try)} å€‹ç«¯é»")
                    
                    headers = {'Content-Type': 'application/json'}
                    if api_key:
                        headers['Authorization'] = f'Bearer {api_key}'
                    
                    for endpoint in endpoints_to_try:
                        try:
                            print(f"[Custom AI] ğŸŒ å˜—è©¦ endpoint: {endpoint}", file=sys.stderr)
                            async with session.post(endpoint, headers=headers, json=request_data) as response:
                                print(f"[Custom AI] ğŸ“¥ éŸ¿æ‡‰ç‹€æ…‹: {response.status}", file=sys.stderr)
                                
                                if response.status == 200:
                                    # ğŸ”§ è¨ºæ–·ï¼šå…ˆç²å–åŸå§‹æ–‡æœ¬
                                    raw_text = await response.text()
                                    print(f"[Custom AI] ğŸ“„ åŸå§‹éŸ¿æ‡‰é•·åº¦: {len(raw_text)}", file=sys.stderr)
                                    print(f"[Custom AI] ğŸ“„ åŸå§‹éŸ¿æ‡‰å‰500å­—: {raw_text[:500]}", file=sys.stderr)
                                    
                                    try:
                                        import json
                                        result = json.loads(raw_text)
                                        print(f"[Custom AI] ğŸ“„ éŸ¿æ‡‰æ ¼å¼: {list(result.keys()) if isinstance(result, dict) else type(result)}", file=sys.stderr)
                                    except Exception as json_err:
                                        print(f"[Custom AI] âŒ JSON è§£æå¤±æ•—: {json_err}", file=sys.stderr)
                                        continue
                                    
                                    # OpenAI format
                                    if 'choices' in result:
                                        content = result['choices'][0]['message']['content']
                                        self.log(f"âœ“ å›è¦† (OpenAIæ ¼å¼): {content[:50]}...")
                                        return content
                                    # Ollama format (message object)
                                    if 'message' in result:
                                        msg_obj = result['message']
                                        print(f"[Custom AI] ğŸ“„ message é¡å‹: {type(msg_obj)}, å€¼: {str(msg_obj)[:200]}", file=sys.stderr)
                                        
                                        if isinstance(msg_obj, dict):
                                            content = msg_obj.get('content', '')
                                        elif isinstance(msg_obj, str):
                                            content = msg_obj  # ğŸ”§ ä¿®å¾©ï¼šmessage å¯èƒ½ç›´æ¥æ˜¯å­—ä¸²
                                        else:
                                            content = str(msg_obj) if msg_obj else ''
                                        
                                        print(f"[Custom AI] ğŸ“„ æå–çš„ content: {content[:100] if content else 'EMPTY'}", file=sys.stderr)
                                        
                                        if content:
                                            self.log(f"âœ“ å›è¦† (Ollama messageæ ¼å¼): {content[:50]}...")
                                            return content
                                        else:
                                            print(f"[Custom AI] âŒ message å°è±¡ä¸­ content ç‚ºç©º", file=sys.stderr)
                                    # Ollama format (response string)
                                    if 'response' in result:
                                        content = result['response']
                                        self.log(f"âœ“ å›è¦† (responseæ ¼å¼): {content[:50]}...")
                                        return content
                                    # Direct content
                                    if 'content' in result:
                                        content = result['content']
                                        self.log(f"âœ“ å›è¦† (ç›´æ¥æ ¼å¼): {content[:50]}...")
                                        return content
                                    
                                    # ğŸ”§ å˜—è©¦æ›´å¤šæ ¼å¼
                                    self.log(f"[Custom AI] æœªè­˜åˆ¥çš„éŸ¿æ‡‰æ ¼å¼ï¼Œå˜—è©¦è§£æ: {str(result)[:200]}", "warning")
                                    # å˜—è©¦éæ­¸æŸ¥æ‰¾ content
                                    if isinstance(result, dict):
                                        for key in ['text', 'output', 'result', 'generated_text']:
                                            if key in result:
                                                content = result[key]
                                                if content:
                                                    self.log(f"âœ“ å›è¦† ({key}æ ¼å¼): {content[:50]}...")
                                                    return content
                                else:
                                    text = await response.text()
                                    self.log(f"[Custom AI] éŒ¯èª¤ {response.status}: {text[:200]}", "warning")
                                    
                        except asyncio.TimeoutError:
                            self.log(f"[Custom AI] Endpoint {endpoint} è¶…æ™‚", "warning")
                            continue
                        except Exception as e:
                            self.log(f"[Custom AI] Endpoint {endpoint} éŒ¯èª¤: {e}", "warning")
                            import traceback
                            traceback.print_exc(file=__import__('sys').stderr)
                            continue
            
            self.log("æ‰€æœ‰ AI endpoints éƒ½å¤±æ•—ï¼Œä½¿ç”¨å‚™ç”¨å›è¦†", "warning")
            return self._get_fallback_response(messages)
            
        except Exception as e:
            self.log(f"AI API èª¿ç”¨éŒ¯èª¤: {str(e)}", "error")
            return self._get_fallback_response(messages)
    
    def _get_fallback_response(self, messages: List[Dict[str, str]]) -> str:
        """ç•¶ AI æœå‹™ä¸å¯ç”¨æ™‚çš„å‚™ç”¨å›è¦†"""
        import random
        
        # ç²å–æœ€å¾Œä¸€æ¢ç”¨æˆ¶æ¶ˆæ¯
        last_user_msg = ""
        for msg in reversed(messages):
            if msg.get('role') == 'user':
                last_user_msg = msg.get('content', '').lower()
                break
        
        # åŸºæ–¼é—œéµè©çš„ç°¡å–®å›è¦†
        if any(kw in last_user_msg for kw in ['ä½ å¥½', 'hi', 'hello', 'å—¨']):
            responses = ['ä½ å¥½å‘€ï½ ğŸ˜Š', 'å—¨å—¨ï¼æœ‰ä»€éº¼å¯ä»¥å¹«ä½ çš„å—ï¼Ÿ', 'ä½ å¥½ï¼å¾ˆé«˜èˆˆèªè­˜ä½ ï½']
        elif any(kw in last_user_msg for kw in ['è¬è¬', 'thanks', 'thank']):
            responses = ['ä¸å®¢æ°£ï¼', 'æ²’äº‹çš„ï½ ğŸ˜„', 'å¾ˆé«˜èˆˆèƒ½å¹«åˆ°ä½ ï¼']
        elif any(kw in last_user_msg for kw in ['ï¼Ÿ', '?', 'å—', 'ä»€éº¼', 'æ€éº¼']):
            responses = ['è®“æˆ‘æƒ³æƒ³...ä½ å¯ä»¥èªªè©³ç´°ä¸€é»å—ï¼Ÿ', 'é€™å€‹å•é¡Œå¾ˆå¥½ï¼Œæˆ‘éœ€è¦äº†è§£æ›´å¤šï½', 'èƒ½å‘Šè¨´æˆ‘æ›´å¤šç´°ç¯€å—ï¼Ÿ']
        elif any(kw in last_user_msg for kw in ['åƒ¹æ ¼', 'å¤šå°‘éŒ¢', 'è²»ç”¨']):
            responses = ['åƒ¹æ ¼æœƒæ ¹æ“šéœ€æ±‚æœ‰æ‰€ä¸åŒï¼Œä½ å…·é«”æƒ³äº†è§£å“ªæ–¹é¢çš„å‘¢ï¼Ÿ', 'é€™å€‹è¦çœ‹å…·é«”éœ€æ±‚ï¼Œæ–¹ä¾¿èªªèªªä½ çš„æƒ…æ³å—ï¼Ÿ']
        else:
            responses = [
                'å¥½çš„ï¼Œæˆ‘æ˜ç™½äº†ï½',
                'å—¯å—¯ï¼Œç¹¼çºŒèªªï¼Ÿ',
                'æ”¶åˆ°ï¼é‚„æœ‰ä»€éº¼æƒ³èŠçš„å—ï¼Ÿ',
                'äº†è§£ï½ ğŸ˜Š',
                'å¥½çš„ï¼Œæœ‰ä»€éº¼éœ€è¦å¹«å¿™çš„å—ï¼Ÿ'
            ]
        
        return random.choice(responses)
    
    async def _delayed_send(self, user_id: str, response: str, 
                             account_phone: str, source_group: str, username: str):
        """Send response with realistic delay"""
        # Calculate delay
        delay_min = self.settings.get('reply_delay_min', 2)
        delay_max = self.settings.get('reply_delay_max', 8)
        delay = random.uniform(delay_min, delay_max)
        
        # Add typing simulation delay based on message length
        typing_speed = self.settings.get('typing_speed', 50)  # chars per minute
        if typing_speed > 0:
            typing_time = len(response) / typing_speed * 60
            delay += min(typing_time, 10)  # Cap typing delay at 10 seconds
        
        self.log(f"Waiting {delay:.1f}s before sending to {username}")
        await asyncio.sleep(delay)
        
        # Send via callback
        if self.send_callback:
            try:
                result = await self.send_callback(
                    account_phone=account_phone,
                    target_user_id=user_id,
                    message=response,
                    source_group=source_group,
                    username=username
                )
                if result:
                    self.log(f"âœ“ Auto-replied to {username}: {response[:50]}...")
                else:
                    self.log(f"âœ— Auto-reply failed for {username}", "warning")
            except Exception as e:
                self.log(f"Error in send callback: {e}", "error")
    
    async def _analyze_and_update_stage(self, user_id: str, user_msg: str, ai_response: str):
        """åˆ†æå°è©±ä¸¦è‡ªå‹•æ›´æ–°æ¼æ–—éšæ®µ"""
        try:
            # ç²å–å®Œæ•´èŠå¤©æ­·å²
            history = await db.get_chat_history(user_id, limit=20)
            
            # ä½¿ç”¨ AI ä¸Šä¸‹æ–‡ç®¡ç†å™¨åˆ†æéšæ®µ
            analysis = await ai_context.analyze_conversation_stage(user_id, history)
            
            new_stage = analysis.get('stage', 'replied')
            interest = analysis.get('interest_level', 2)
            
            # æ›´æ–°æ¼æ–—éšæ®µ
            await db.update_funnel_stage(
                user_id=user_id, 
                stage=new_stage,
                reason=f"è‡ªå‹•åˆ†æ: {analysis.get('suggestions', [''])[0]}"
            )
            
            # æ›´æ–°èˆˆè¶£ç¨‹åº¦
            await db.update_user_interest(user_id, interest)
            
            self.log(f"[æ¼æ–—] ç”¨æˆ¶ {user_id} éšæ®µæ›´æ–°: {new_stage}, èˆˆè¶£åº¦: {interest}/5")
            
            # ç™¼é€æ¼æ–—æ›´æ–°äº‹ä»¶åˆ°å‰ç«¯
            if self.event_callback:
                self.event_callback("funnel-updated", {
                    "userId": user_id,
                    "stage": new_stage,
                    "stageName": analysis.get('stage_name'),
                    "interestLevel": interest,
                    "suggestions": analysis.get('suggestions', [])
                })
                
        except Exception as e:
            self.log(f"Error analyzing stage: {e}", "error")
    
    async def handle_auto_greeting(self, user_id: str, username: str,
                                     account_phone: str, source_group: str = None,
                                     first_name: str = None,
                                     triggered_keyword: str = None) -> Optional[str]:
        """
        Handle automatic greeting for new users
        
        å€‹æ€§åŒ–å•å€™é‚è¼¯ï¼š
        1. æ ¹æ“šè§¸ç™¼é—œéµè©é¸æ“‡ç›¸é—œå•å€™
        2. è­˜åˆ¥è€ç”¨æˆ¶ç™¼é€ä¸åŒå•å€™
        3. ä½¿ç”¨ç”¨æˆ¶åç¨±å€‹æ€§åŒ–
        """
        # æª¢æŸ¥è‡ªå‹•å•å€™è¨­ç½® (æ•´æ•¸ 0/1)
        auto_greeting_enabled = self.settings.get('auto_greeting', 0) == 1
        if not auto_greeting_enabled:
            self.log(f"[å•å€™] è‡ªå‹•å•å€™æœªå•Ÿç”¨ (è¨­ç½®å€¼: {self.settings.get('auto_greeting', 0)})")
            return None
        
        self.log(f"[å•å€™] é–‹å§‹ç‚ºç”¨æˆ¶ {user_id} (@{username}) ç”Ÿæˆå•å€™...")
        import random
        name = first_name or username or ''
        keyword = (triggered_keyword or '').lower()
        
        # Check if we've already greeted this user (è€ç”¨æˆ¶è­˜åˆ¥)
        profile = await db.get_user_profile(user_id)
        is_returning_user = profile and profile.get('total_messages', 0) > 0
        
        # è€ç”¨æˆ¶è­˜åˆ¥
        if is_returning_user:
            previous_stage = profile.get('funnel_stage', 'new')
            last_interaction = profile.get('last_interaction')
            
            # è€ç”¨æˆ¶å€‹æ€§åŒ–å•å€™
            returning_greetings = [
                f"å—¨ {name}ï¼å¥½ä¹…ä¸è¦‹~ ğŸ˜Š",
                f"Hi {name}~ åˆè¦‹é¢å•¦ï¼",
                f"{name}ï¼Œæ­¡è¿å›ä¾†ï¼æœ‰ä»€éº¼æ–°éœ€æ±‚å—ï¼Ÿ",
                f"å“ˆå›‰ {name}ï¼ä¸Šæ¬¡èŠå¾—æ€éº¼æ¨£ï¼Ÿ",
            ]
            
            # æ ¹æ“šä¹‹å‰çš„éšæ®µèª¿æ•´å•å€™
            if previous_stage == 'interested':
                returning_greetings.append(f"{name}ï¼Œä¸Šæ¬¡ä½ å°é€™å€‹æŒºæ„Ÿèˆˆè¶£çš„ï¼Œé‚„æœ‰ä»€éº¼æƒ³äº†è§£çš„ï¼Ÿ")
            elif previous_stage == 'negotiating':
                returning_greetings.append(f"Hi {name}ï¼ä¹‹å‰èŠçš„äº‹æƒ…è€ƒæ…®å¾—æ€éº¼æ¨£äº†ï¼Ÿ")
            
            greeting = random.choice(returning_greetings) if name else "å—¨~ æ­¡è¿å›ä¾†ï¼"
            return greeting
        
        # ========== æ–°ç”¨æˆ¶å•å€™ - æ ¹æ“šé—œéµè©å€‹æ€§åŒ– ==========
        
        # é—œéµè©åˆ†é¡å•å€™æ¨¡æ¿
        keyword_greetings = {
            # æ›åŒ¯ç›¸é—œ
            'æ›åŒ¯': [
                f"å—¨ {name}ï¼çœ‹åˆ°ä½ å°æ›åŒ¯æœ‰éœ€æ±‚ï¼Œè«‹å•è¦æ›ä»€éº¼å¹£ç¨®å‘¢ï¼Ÿ",
                f"Hi {name}~ æ›åŒ¯é€™é‚Šå¯ä»¥å¹«ä½ ï¼Œä½ æƒ³æ›å¤šå°‘ï¼Ÿ",
                f"{name}ï¼Œæœ‰æ›åŒ¯éœ€æ±‚å—ï¼Ÿä»Šå¤©åŒ¯ç‡ä¸éŒ¯å–” ğŸ˜Š",
            ],
            'æ›U': [
                f"å—¨ {name}ï¼è¦æ›Uå—ï¼ŸUSDT/CNYä»Šå¤©åŒ¯ç‡å¾ˆå¥½~",
                f"Hi {name}~ Ué€™é‚Šæœ‰ï¼Œä½ éœ€è¦å¤šå°‘ï¼Ÿ",
            ],
            'usdt': [
                f"å—¨ {name}ï¼éœ€è¦USDTå—ï¼Ÿå¯ä»¥èŠèŠ~",
                f"Hi~ USDTé€™é‚Šå¯ä»¥æ“ä½œï¼Œ{name}ä½ éœ€è¦è²·é‚„æ˜¯è³£ï¼Ÿ",
            ],
            
            # æ”¯ä»˜ç›¸é—œ
            'æ”¯ä»˜': [
                f"å—¨ {name}ï¼çœ‹åˆ°ä½ éœ€è¦æ”¯ä»˜æ–¹é¢çš„å¹«åŠ©ï¼Œæ˜¯ä»€éº¼é¡å‹çš„æ”¯ä»˜å‘¢ï¼Ÿ",
                f"Hi {name}~ æ”¯ä»˜é€™å¡Šæˆ‘å¯ä»¥å¹«ä½ ï¼Œæ˜¯è·¨å¢ƒé‚„æ˜¯æœ¬åœ°çš„ï¼Ÿ",
            ],
            'ä»˜æ¬¾': [
                f"å—¨ {name}ï¼ä»˜æ¬¾é€™é‚Šå¯ä»¥å¹«ä½ è™•ç†~",
                f"Hi~ {name}æœ‰ä»€éº¼ä»˜æ¬¾éœ€æ±‚å—ï¼Ÿ",
            ],
            
            # æŠ•è³‡ç›¸é—œ
            'æŠ•è³‡': [
                f"å—¨ {name}ï¼å°æŠ•è³‡æœ‰èˆˆè¶£å—ï¼Ÿå¯ä»¥èŠèŠ~",
                f"Hi {name}~ æƒ³äº†è§£ä»€éº¼é¡å‹çš„æŠ•è³‡å‘¢ï¼Ÿ",
            ],
            'ç†è²¡': [
                f"å—¨ {name}ï¼ç†è²¡é€™é‚Šæœ‰å¾ˆå¤šé¸æ“‡ï¼Œä½ åå¥½ä»€éº¼é¡å‹ï¼Ÿ",
            ],
            
            # é€šç”¨æŸ¥è©¢
            'äº†è§£': [
                f"å—¨ {name}ï¼æƒ³äº†è§£ä»€éº¼å‘¢ï¼Ÿæˆ‘ä¾†çµ¦ä½ ä»‹ç´¹~",
                f"Hi~ {name}æœ‰ä»€éº¼æƒ³äº†è§£çš„ï¼Œç›¡ç®¡å•ï¼",
            ],
            'è«®è©¢': [
                f"å—¨ {name}ï¼æœ‰ä»€éº¼éœ€è¦è«®è©¢çš„å—ï¼Ÿ",
                f"Hi {name}~ é€™é‚Šå¯ä»¥å¹«ä½ è§£ç­”~",
            ],
        }
        
        # å˜—è©¦åŒ¹é…é—œéµè©æ¨¡æ¿
        greeting = None
        for kw, templates in keyword_greetings.items():
            if kw.lower() in keyword:
                greeting = random.choice(templates)
                break
        
        # å¦‚æœæ²’æœ‰åŒ¹é…åˆ°é—œéµè©ï¼Œä½¿ç”¨é€šç”¨å•å€™
        if not greeting:
            # ä½¿ç”¨ç”¨æˆ¶è¨­ç½®çš„å•å€™èª
            greeting = self.settings.get('greeting_message', '')
            
            if not greeting:
                # é€šç”¨å•å€™
                general_greetings = [
                    f"å—¨ {name}ï¼çœ‹åˆ°ä½ çš„æ¶ˆæ¯äº† ğŸ˜Š æœ‰ä»€éº¼å¯ä»¥å¹«ä½ çš„ï¼Ÿ",
                    f"Hi {name}~ æ­¡è¿æ­¡è¿ï¼éœ€è¦ä»€éº¼æœå‹™å—ï¼Ÿ",
                    f"å“ˆå›‰ {name}ï¼æœ‰ä»€éº¼æƒ³äº†è§£çš„å—ï¼Ÿ",
                    f"å—¨~ éœ€è¦å¹«å¿™å—ï¼Ÿæˆ‘é€™é‚Šå¯ä»¥å”åŠ©ä½  â˜ºï¸",
                ]
                greeting = random.choice(general_greetings) if name else "å—¨~ æœ‰ä»€éº¼å¯ä»¥å¹«ä½ çš„ï¼Ÿ"
        
        # Replace placeholders
        greeting = greeting.replace('{username}', username or '')
        greeting = greeting.replace('{firstName}', first_name or '')
        greeting = greeting.replace('{name}', name)
        greeting = greeting.replace('{keyword}', triggered_keyword or '')
        
        return greeting
    
    async def get_suggested_response(self, user_id: str, user_message: str) -> Optional[str]:
        """Get a suggested response without sending it (for assist mode)"""
        # ğŸ”§ é¦–å…ˆå˜—è©¦å®Œæ•´æµç¨‹
        try:
            result = await self._generate_response(user_id, user_message)
            if result:
                return result
        except Exception as e:
            self.log(f"[get_suggested_response] å®Œæ•´æµç¨‹å¤±æ•—: {e}", "warning")
        
        # ğŸ”§ å¦‚æœå¤±æ•—ï¼Œå˜—è©¦ç°¡åŒ–çš„ç´”æ·¨æ¨¡å¼
        self.log("[get_suggested_response] å˜—è©¦ç´”æ·¨æ¨¡å¼...", "info")
        return await self._generate_simple_response(user_message)
    
    async def _generate_simple_response(self, user_message: str) -> Optional[str]:
        """
        ğŸ”§ ç´”æ·¨æ¨¡å¼ï¼šç›´æ¥èª¿ç”¨ AI APIï¼Œä¸ç¶“é RAG/è¨˜æ†¶/è³ªé‡æª¢æŸ¥
        ç”¨æ–¼è¨ºæ–·å’Œå‚™ç”¨
        """
        try:
            # ç¢ºä¿å·²åˆå§‹åŒ–
            if not self.local_ai_endpoint:
                await self.initialize()
            
            # ğŸ”§ FIX: ç²å–æ—¥å¸¸å°è©±æ¨¡å‹é…ç½®
            usage_model_config = await self.get_model_for_usage('dailyChat')
            
            has_model = usage_model_config or self.local_ai_endpoint
            if not has_model:
                self.log("[ç´”æ·¨æ¨¡å¼] ç„¡å¯ç”¨ AI æ¨¡å‹", "error")
                return None
            
            # ğŸ”§ FIX: å„ªåŒ–ç´”æ·¨æ¨¡å¼çš„ prompt - ç¢ºä¿å›ç­”å•é¡Œ
            # å…ˆç²å–æœ€è¿‘çš„å°è©±æ­·å²
            history_context = ""
            try:
                history = await db.get_chat_history(None, limit=5)  # ç²å–æœ€è¿‘5æ¢
                if history:
                    for msg in history[-3:]:  # æœ€è¿‘3æ¢
                        role_name = "ç”¨æˆ¶" if msg.get('role') == 'user' else "åŠ©æ‰‹"
                        history_context += f"{role_name}: {msg.get('content', '')}\n"
            except:
                pass
            
            # æ§‹å»ºå¸¶æ­·å²çš„æ¶ˆæ¯åˆ—è¡¨
            system_content = """ä½ æ˜¯å°ˆæ¥­ä¸”å‹å¥½çš„æ¥­å‹™é¡§å•ã€‚

ã€æ ¸å¿ƒè¦å‰‡ã€‘
1. âš ï¸ å¿…é ˆç›´æ¥å›ç­”ç”¨æˆ¶çš„å•é¡Œ
2. å¦‚æœç”¨æˆ¶å•"ä½ æœ‰ä»€éº¼"ï¼Œå°±ä»‹ç´¹ä½ çš„æ¥­å‹™ï¼šæ”¯ä»˜è§£æ±ºæ–¹æ¡ˆã€Uå…Œæ›ã€ä»£æ”¶ä»£ä»˜
3. å›è¦†ç°¡çŸ­ï¼ˆ20-60å­—ï¼‰ï¼Œèªæ°£è‡ªç„¶åƒæœ‹å‹
4. ä¸è¦èªª"è«‹å•é‚„æœ‰ä»€éº¼éœ€è¦å¹«åŠ©"é€™é¡è©±"""
            
            if history_context:
                system_content += f"\n\nã€æœ€è¿‘å°è©±åƒè€ƒã€‘\n{history_context}"
            
            messages = [
                {"role": "system", "content": system_content},
                {"role": "user", "content": user_message}
            ]
            
            if usage_model_config:
                self.log(f"[ç´”æ·¨æ¨¡å¼] ä½¿ç”¨æ¨¡å‹: {usage_model_config.get('display_name')}")
            else:
                self.log(f"[ç´”æ·¨æ¨¡å¼] ä½¿ç”¨é»˜èªæ¨¡å‹: {getattr(self, 'provider', 'custom')}")
            
            # ğŸ”§ FIX: å‚³éæ¨¡å‹é…ç½®
            result = await self._call_ai_api(messages, model_config=usage_model_config)
            
            if result:
                self.log(f"[ç´”æ·¨æ¨¡å¼] âœ“ æˆåŠŸ: {result[:50]}...")
            else:
                self.log("[ç´”æ·¨æ¨¡å¼] âŒ API è¿”å›ç©º", "warning")
            
            return result
            
        except Exception as e:
            self.log(f"[ç´”æ·¨æ¨¡å¼] éŒ¯èª¤: {e}", "error")
            import traceback
            traceback.print_exc(file=__import__('sys').stderr)
            return None
    
    async def regenerate_response(self, user_id: str) -> Optional[str]:
        """Regenerate the last response"""
        # Get the last user message
        history = await db.get_chat_history(user_id, limit=2)
        if not history:
            return None
        
        # Find last user message
        last_user_msg = None
        for msg in reversed(history):
            if msg['role'] == 'user':
                last_user_msg = msg['content']
                break
        
        if not last_user_msg:
            return None
        
        return await self._generate_response(user_id, last_user_msg)
    
    async def trigger_rag_learning(
        self,
        user_id: str,
        account_phone: str = "",
        outcome: str = "unknown"
    ) -> Dict[str, Any]:
        """
        è§¸ç™¼ RAG å­¸ç¿’
        åœ¨å°è©±çµæŸæˆ–é”åˆ°ä¸€å®šæ¶ˆæ¯æ•¸æ™‚èª¿ç”¨
        
        Args:
            user_id: ç”¨æˆ¶ ID
            account_phone: å¸³è™Ÿé›»è©±
            outcome: å°è©±çµæœ
        
        Returns:
            å­¸ç¿’çµæœ
        """
        if not RAG_AVAILABLE:
            return {'error': 'RAG ç³»çµ±ä¸å¯ç”¨'}
        
        try:
            # ä½¿ç”¨ chat_indexer è™•ç†
            await chat_indexer.on_conversation_ended(
                user_id=user_id,
                account_phone=account_phone,
                outcome=outcome
            )
            
            return {'success': True, 'message': f'å·²è§¸ç™¼ç”¨æˆ¶ {user_id} çš„ RAG å­¸ç¿’'}
            
        except Exception as e:
            self.log(f"è§¸ç™¼ RAG å­¸ç¿’å¤±æ•—: {e}", "error")
            return {'error': str(e)}
    
    async def get_rag_statistics(self) -> Dict[str, Any]:
        """ç²å– RAG ç³»çµ±çµ±è¨ˆä¿¡æ¯"""
        if not RAG_AVAILABLE:
            return {'error': 'RAG ç³»çµ±ä¸å¯ç”¨'}
        
        try:
            rag_stats = await telegram_rag.get_statistics()
            indexer_stats = await chat_indexer.get_indexing_statistics()
            
            return {
                'rag': rag_stats,
                'indexer': indexer_stats
            }
        except Exception as e:
            self.log(f"ç²å– RAG çµ±è¨ˆå¤±æ•—: {e}", "error")
            return {'error': str(e)}
    
    async def initialize_rag_system(self) -> bool:
        """åˆå§‹åŒ– RAG ç³»çµ±"""
        if not RAG_AVAILABLE:
            self.log("RAG ç³»çµ±æ¨¡çµ„ä¸å¯ç”¨", "warning")
            return False
        
        try:
            # åˆå§‹åŒ– RAG ç³»çµ±
            await telegram_rag.initialize()
            
            # åˆå§‹åŒ–ç´¢å¼•æœå‹™
            await chat_indexer.initialize()
            
            # å•Ÿå‹•å¾Œå°ç´¢å¼•
            await chat_indexer.start_background_indexing()
            
            self.log("âœ“ RAG ç³»çµ±åˆå§‹åŒ–å®Œæˆ", "success")
            return True
            
        except Exception as e:
            self.log(f"RAG ç³»çµ±åˆå§‹åŒ–å¤±æ•—: {e}", "error")
            return False
    
    def get_last_knowledge_info(self) -> dict:
        """
        ğŸ†• P1-2: ç²å–æœ€å¾Œä¸€æ¬¡ AI å›è¦†ä½¿ç”¨çš„çŸ¥è­˜ä¿¡æ¯
        ç”¨æ–¼å‰ç«¯å¯è¦–åŒ–é¡¯ç¤º
        """
        return {
            'knowledgeUsed': self.last_knowledge_used,
            'source': self.last_knowledge_source,
            'hasKnowledge': len(self.last_knowledge_used) > 0
        }


# Global instance
ai_auto_chat = AIAutoChatService()


# Import List for type hints
from typing import List
