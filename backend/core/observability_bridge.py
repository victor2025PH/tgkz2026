"""
ğŸ”§ P11-3: å¯è§€æ¸¬æ€§æ©‹æ¥å™¨

æ‰“é€šä¸‰å¤§å­ç³»çµ±ï¼š
  AnomalyDetectionManager â†’ AlertService â†’ NotificationLog

ç•¶ç•°å¸¸æª¢æ¸¬ç™¼ç¾å•é¡Œæ™‚ï¼Œè‡ªå‹•ï¼š
1. å°‡ Anomaly è½‰æ›ç‚º AlertService çš„ send_alert èª¿ç”¨
2. æ ¹æ“šåš´é‡ç¨‹åº¦æ±ºå®šå‘Šè­¦ç´šåˆ¥
3. æ‡‰ç”¨å‘Šè­¦æŠ‘åˆ¶è¦å‰‡ï¼ˆé˜²æ­¢ç›¸åŒå•é¡Œé‡è¤‡å‘Šè­¦ï¼‰
4. è¨˜éŒ„åˆ°å‘Šè­¦æ­·å²

ğŸ”§ P11-4: è³‡æºè¶¨å‹¢åˆ†æ + æ“´ç¸®å®¹å»ºè­°
åŸºæ–¼ PerformanceAnalyzer çš„æ­·å²æŒ‡æ¨™ç”Ÿæˆå»ºè­°

ğŸ”§ P11-5: æ—¥èªŒç•°å¸¸æ¨¡å¼èšé¡
åŸºæ–¼éŒ¯èª¤æ—¥èªŒçš„è‡ªå‹•åˆ†çµ„èˆ‡å»é‡
"""

import asyncio
import logging
import time
import threading
from typing import Dict, Any, List, Optional
from collections import defaultdict
from datetime import datetime, timedelta

logger = logging.getLogger(__name__)


# ==================== P11-3: ç•°å¸¸â†’å‘Šè­¦æ©‹æ¥ ====================

class AnomalyAlertBridge:
    """
    å°‡ AnomalyDetectionManager çš„ç•°å¸¸äº‹ä»¶æ©‹æ¥åˆ° AlertService
    
    åš´é‡ç¨‹åº¦æ˜ å°„ï¼š
      critical â†’ AlertLevel.CRITICAL
      high     â†’ AlertLevel.CRITICAL
      medium   â†’ AlertLevel.WARNING
      low      â†’ AlertLevel.INFO
    """
    
    # æœ€è¿‘å‘Šè­¦æŠ‘åˆ¶ï¼ˆåŒä¸€æŒ‡æ¨™ + åŒä¸€é¡å‹ï¼Œ30 åˆ†é˜å…§ä¸é‡è¤‡å‘Šè­¦ï¼‰
    _suppress_window_seconds = 1800  # 30 åˆ†é˜
    _last_alerts: Dict[str, float] = {}
    _lock = threading.Lock()
    
    @classmethod
    def handle_anomaly(cls, anomaly) -> None:
        """
        ç•°å¸¸è™•ç†å›èª¿ï¼ˆåŒæ­¥ï¼Œç”± AnomalyDetectionManager èª¿ç”¨ï¼‰
        
        å…§éƒ¨å•Ÿå‹•ç•°æ­¥ä»»å‹™ç™¼é€å‘Šè­¦
        """
        try:
            # æŠ‘åˆ¶æª¢æŸ¥
            suppress_key = f"{anomaly.metric_name}:{anomaly.anomaly_type.value}"
            now = time.time()
            
            with cls._lock:
                last_time = cls._last_alerts.get(suppress_key, 0)
                if now - last_time < cls._suppress_window_seconds:
                    return  # æŠ‘åˆ¶é‡è¤‡å‘Šè­¦
                cls._last_alerts[suppress_key] = now
            
            # æ˜ å°„åš´é‡ç¨‹åº¦
            severity = anomaly.severity.value if hasattr(anomaly.severity, 'value') else str(anomaly.severity)
            
            from admin.alert_service import AlertLevel
            level_map = {
                'critical': AlertLevel.CRITICAL,
                'high': AlertLevel.CRITICAL,
                'medium': AlertLevel.WARNING,
                'low': AlertLevel.INFO,
            }
            alert_level = level_map.get(severity, AlertLevel.WARNING)
            
            # æ§‹å»ºå‘Šè­¦å…§å®¹
            alert_type = f"anomaly.{anomaly.metric_name}"
            message = (
                f"[{severity.upper()}] æŒ‡æ¨™ {anomaly.metric_name} ç•°å¸¸\n"
                f"é¡å‹: {anomaly.anomaly_type.value}\n"
                f"ç•¶å‰å€¼: {anomaly.value:.2f}  æœŸæœ›å€¼: {anomaly.expected_value:.2f}\n"
                f"åå·®: {anomaly.deviation:.2f}\n"
                f"æª¢æ¸¬æ–¹æ³•: {anomaly.detection_method.value}"
            )
            
            details = anomaly.to_dict() if hasattr(anomaly, 'to_dict') else {
                'metric': anomaly.metric_name,
                'value': anomaly.value,
                'severity': severity,
            }
            
            suggestion = _generate_suggestion(anomaly)
            
            # ç•°æ­¥ç™¼é€å‘Šè­¦ï¼ˆåœ¨äº‹ä»¶å¾ªç’°ä¸­ï¼‰
            try:
                loop = asyncio.get_running_loop()
                loop.create_task(_async_send_alert(alert_type, message, alert_level, details, suggestion))
            except RuntimeError:
                # æ²’æœ‰é‹è¡Œä¸­çš„äº‹ä»¶å¾ªç’°ï¼Œè·³éç•°æ­¥å‘Šè­¦
                logger.warning(f"[AnomalyAlertBridge] No event loop, alert skipped: {alert_type}")
                
        except Exception as e:
            logger.error(f"[AnomalyAlertBridge] Error handling anomaly: {e}")


async def _async_send_alert(alert_type, message, level, details, suggestion):
    """ç•°æ­¥ç™¼é€å‘Šè­¦"""
    try:
        from admin.alert_service import get_alert_service
        service = get_alert_service()
        result = await service.send_alert(
            alert_type=alert_type,
            message=message,
            level=level,
            details=details,
            suggestion=suggestion,
        )
        if result.get('sent'):
            logger.info(f"[AnomalyAlertBridge] Alert sent: {alert_type}")
    except Exception as e:
        logger.error(f"[AnomalyAlertBridge] Failed to send alert: {e}")


def _generate_suggestion(anomaly) -> str:
    """æ ¹æ“šç•°å¸¸é¡å‹ç”Ÿæˆä¿®å¾©å»ºè­°"""
    metric = anomaly.metric_name
    atype = anomaly.anomaly_type.value if hasattr(anomaly.anomaly_type, 'value') else str(anomaly.anomaly_type)
    
    suggestions = {
        'api_latency': {
            'spike': 'æª¢æŸ¥æ•¸æ“šåº«æŸ¥è©¢æ€§èƒ½ï¼Œç¢ºèªæ˜¯å¦æœ‰æ…¢æŸ¥è©¢ã€‚è€ƒæ…®å¢åŠ ç·©å­˜ã€‚',
            'threshold_breach': 'API éŸ¿æ‡‰è¶…éé–¾å€¼ï¼Œæª¢æŸ¥æœå‹™å™¨è² è¼‰å’Œæ•¸æ“šåº«é€£æ¥æ± ã€‚',
        },
        'api_error_count': {
            'spike': 'éŒ¯èª¤æ¿€å¢ï¼Œæª¢æŸ¥æœ€è¿‘éƒ¨ç½²æ˜¯å¦å¼•å…¥äº† bugã€‚æŸ¥çœ‹éŒ¯èª¤æ—¥èªŒç²å–è©³æƒ…ã€‚',
            'threshold_breach': 'éŒ¯èª¤è¨ˆæ•¸è¶…éé–¾å€¼ï¼Œå¯èƒ½å­˜åœ¨æœå‹™é™ç´šã€‚',
        },
        'capacity_usage': {
            'threshold_breach': 'è³‡æºå®¹é‡å³å°‡ç”¨ç›¡ï¼Œè€ƒæ…®æ“´å®¹æˆ–æ¸…ç†éæœŸæ•¸æ“šã€‚',
        },
    }
    
    metric_suggestions = suggestions.get(metric, {})
    return metric_suggestions.get(atype, f'æŒ‡æ¨™ {metric} å‡ºç¾ {atype} ç•°å¸¸ï¼Œè«‹æª¢æŸ¥ç›¸é—œæœå‹™ã€‚')


def setup_anomaly_alert_bridge():
    """
    è¨­ç½®ç•°å¸¸â†’å‘Šè­¦æ©‹æ¥
    
    åœ¨æ‡‰ç”¨å•Ÿå‹•æ™‚èª¿ç”¨ä¸€æ¬¡
    """
    try:
        from admin.anomaly_detection import get_anomaly_manager
        am = get_anomaly_manager()
        am.register_handler(AnomalyAlertBridge.handle_anomaly)
        logger.info("[ObservabilityBridge] Anomaly â†’ Alert bridge registered")
    except Exception as e:
        logger.warning(f"[ObservabilityBridge] Failed to setup bridge: {e}")


# ==================== P11-4: è³‡æºè¶¨å‹¢åˆ†æ + æ“´ç¸®å®¹å»ºè­° ====================

class ResourceAnalyzer:
    """
    åˆ†æç³»çµ±è³‡æºè¶¨å‹¢ï¼Œç”Ÿæˆæ“´ç¸®å®¹å»ºè­°
    
    æ•¸æ“šä¾†æºï¼š
    - PerformanceAnalyzer çš„æ­·å²å»¶é²æ•¸æ“š
    - HealthService çš„è³‡æºæŒ‡æ¨™ï¼ˆCPU/å…§å­˜/ç£ç›¤ï¼‰
    - MetricsCollector çš„è«‹æ±‚é‡è¶¨å‹¢
    """
    
    @staticmethod
    def analyze_trends() -> Dict[str, Any]:
        """
        åˆ†æç•¶å‰è³‡æºè¶¨å‹¢
        
        Returns:
            {
                'cpu': { 'current': float, 'trend': str, 'risk': str },
                'memory': { 'current': float, 'trend': str, 'risk': str },
                'disk': { 'current': float, 'trend': str, 'risk': str },
                'request_load': { 'current': float, 'trend': str },
                'suggestions': [ str, ... ],
                'overall_risk': str,  # low/medium/high/critical
            }
        """
        result = {
            'cpu': {'current': 0, 'trend': 'stable', 'risk': 'low'},
            'memory': {'current': 0, 'trend': 'stable', 'risk': 'low'},
            'disk': {'current': 0, 'trend': 'stable', 'risk': 'low'},
            'request_load': {'current': 0, 'trend': 'stable'},
            'suggestions': [],
            'overall_risk': 'low',
            'timestamp': datetime.utcnow().isoformat(),
        }
        
        # æ”¶é›†ç³»çµ±æŒ‡æ¨™
        try:
            import psutil
            
            # CPU
            cpu_pct = psutil.cpu_percent(interval=0.1)
            result['cpu']['current'] = cpu_pct
            if cpu_pct > 90:
                result['cpu']['risk'] = 'critical'
                result['cpu']['trend'] = 'overloaded'
                result['suggestions'].append('CPU ä½¿ç”¨ç‡ > 90%ï¼Œå»ºè­°å¢åŠ  CPU æ ¸å¿ƒæˆ–å„ªåŒ–é«˜ CPU æ“ä½œ')
            elif cpu_pct > 70:
                result['cpu']['risk'] = 'high'
                result['suggestions'].append('CPU ä½¿ç”¨ç‡åé«˜ï¼Œè€ƒæ…®å„ªåŒ–è¨ˆç®—å¯†é›†å‹ä»»å‹™')
            
            # å…§å­˜
            mem = psutil.virtual_memory()
            result['memory']['current'] = mem.percent
            if mem.percent > 90:
                result['memory']['risk'] = 'critical'
                result['suggestions'].append('å…§å­˜ä½¿ç”¨ç‡ > 90%ï¼Œå»ºè­°å¢åŠ å…§å­˜æˆ–æ’æŸ¥å…§å­˜æ´©æ¼')
            elif mem.percent > 80:
                result['memory']['risk'] = 'high'
                result['suggestions'].append('å…§å­˜ä½¿ç”¨ç‡åé«˜ï¼Œè€ƒæ…®å¢åŠ å®¹å™¨è¨˜æ†¶é«”é™åˆ¶')
            
            # ç£ç›¤
            disk = psutil.disk_usage('/')
            result['disk']['current'] = disk.percent
            if disk.percent > 90:
                result['disk']['risk'] = 'critical'
                result['suggestions'].append('ç£ç›¤ä½¿ç”¨ç‡ > 90%ï¼Œç·Šæ€¥æ¸…ç†æ—¥èªŒ/å‚™ä»½æˆ–æ“´å®¹')
            elif disk.percent > 80:
                result['disk']['risk'] = 'high'
                result['suggestions'].append('ç£ç›¤ç©ºé–“åä½ï¼Œå»ºè­°æ¸…ç†èˆŠå‚™ä»½å’Œæ—¥èªŒ')
        except ImportError:
            result['suggestions'].append('psutil æœªå®‰è£ï¼Œç„¡æ³•æ”¶é›†ç³»çµ±æŒ‡æ¨™')
        except Exception as e:
            logger.warning(f"Resource analysis error: {e}")
        
        # è«‹æ±‚è² è¼‰
        try:
            from core.metrics_exporter import get_metrics_collector
            mc = get_metrics_collector()
            total_requests = mc._counters.get('tgmatrix_http_requests_total', 0)
            uptime = time.time() - mc._start_time
            if uptime > 0:
                rps = total_requests / uptime
                result['request_load']['current'] = round(rps, 2)
                if rps > 100:
                    result['suggestions'].append(f'å¹³å‡ RPS ç‚º {rps:.1f}ï¼Œè€ƒæ…®å¢åŠ å¾Œç«¯å‰¯æœ¬æˆ–å•Ÿç”¨è² è¼‰å‡è¡¡')
        except Exception:
            pass
        
        # è¨ˆç®—ç¸½é«”é¢¨éšª
        risks = [result['cpu']['risk'], result['memory']['risk'], result['disk']['risk']]
        risk_levels = {'low': 0, 'medium': 1, 'high': 2, 'critical': 3}
        max_risk = max(risk_levels.get(r, 0) for r in risks)
        result['overall_risk'] = {0: 'low', 1: 'medium', 2: 'high', 3: 'critical'}[max_risk]
        
        if not result['suggestions']:
            result['suggestions'].append('æ‰€æœ‰è³‡æºæŒ‡æ¨™æ­£å¸¸ï¼Œç„¡éœ€èª¿æ•´')
        
        return result


# ==================== P11-5: æ—¥èªŒç•°å¸¸æ¨¡å¼èšé¡ ====================

class ErrorPatternCluster:
    """
    æ—¥èªŒç•°å¸¸æ¨¡å¼èšé¡å™¨
    
    è‡ªå‹•å°‡éŒ¯èª¤æ—¥èªŒæŒ‰æ¨¡å¼åˆ†çµ„ï¼Œè­˜åˆ¥ï¼š
    - é«˜é »éŒ¯èª¤æ¨¡å¼
    - æ–°å‡ºç¾çš„éŒ¯èª¤æ¨¡å¼
    - çªå¢çš„éŒ¯èª¤æ¨¡å¼
    """
    
    _instance = None
    _lock = threading.Lock()
    
    def __new__(cls):
        with cls._lock:
            if cls._instance is None:
                cls._instance = super().__new__(cls)
                cls._instance._initialized = False
            return cls._instance
    
    def __init__(self):
        if self._initialized:
            return
        
        # éŒ¯èª¤æ¨¡å¼è¨ˆæ•¸: { normalized_pattern: { count, first_seen, last_seen, samples } }
        self._patterns: Dict[str, Dict[str, Any]] = {}
        self._max_patterns = 200
        self._max_samples_per_pattern = 5
        self._data_lock = threading.Lock()
        self._initialized = True
    
    def record_error(self, error_message: str, context: Dict[str, Any] = None):
        """
        è¨˜éŒ„ä¸€æ¢éŒ¯èª¤æ—¥èªŒ
        
        è‡ªå‹•æ­¸ä¸€åŒ–ä¸¦åˆ†çµ„
        """
        import re
        
        # æ­¸ä¸€åŒ–ï¼šå»æ‰å‹•æ…‹éƒ¨åˆ†ï¼ˆæ³¨æ„é †åºï¼šå…ˆ UUID/IPï¼Œå†æ•¸å­—ï¼‰
        normalized = error_message.strip()
        # 1. æ›¿æ› UUIDï¼ˆå¿…é ˆæœ€å…ˆï¼Œå¦å‰‡æ•¸å­—æ›¿æ›æœƒç ´å£ UUID æ ¼å¼ï¼‰
        normalized = re.sub(r'[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}', '<UUID>', normalized)
        # 2. æ›¿æ› IP åœ°å€
        normalized = re.sub(r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}', '<IP>', normalized)
        # 3. æ›¿æ›æ–‡ä»¶è·¯å¾‘
        normalized = re.sub(r'(?:/[\w.-]+){3,}', '<PATH>', normalized)
        # 4. æ›¿æ›å¤§æ•¸å­—ï¼ˆä¿ç•™ HTTP ç‹€æ…‹ç¢¼ç­‰çŸ­æ•¸å­—ï¼‰
        normalized = re.sub(r'(?<!\w)\d{5,}(?!\w)', '<NUM>', normalized)
        # æˆªæ–·åˆ° 200 å­—ç¬¦
        if len(normalized) > 200:
            normalized = normalized[:200] + '...'
        
        now = datetime.utcnow().isoformat()
        
        with self._data_lock:
            if normalized not in self._patterns:
                if len(self._patterns) >= self._max_patterns:
                    # æ·˜æ±°æœ€è€çš„æ¨¡å¼
                    oldest_key = min(self._patterns, key=lambda k: self._patterns[k]['last_seen'])
                    del self._patterns[oldest_key]
                
                self._patterns[normalized] = {
                    'count': 0,
                    'first_seen': now,
                    'last_seen': now,
                    'samples': [],
                    'context': context or {},
                }
            
            entry = self._patterns[normalized]
            entry['count'] += 1
            entry['last_seen'] = now
            if len(entry['samples']) < self._max_samples_per_pattern:
                entry['samples'].append({
                    'message': error_message[:500],
                    'time': now,
                })
    
    def get_top_patterns(self, limit: int = 20) -> List[Dict[str, Any]]:
        """ç²å–æœ€é«˜é »çš„éŒ¯èª¤æ¨¡å¼"""
        with self._data_lock:
            sorted_patterns = sorted(
                self._patterns.items(),
                key=lambda x: x[1]['count'],
                reverse=True
            )[:limit]
        
        return [
            {
                'pattern': pattern,
                'count': data['count'],
                'first_seen': data['first_seen'],
                'last_seen': data['last_seen'],
                'samples': data['samples'],
            }
            for pattern, data in sorted_patterns
        ]
    
    def get_recent_patterns(self, hours: int = 1, limit: int = 20) -> List[Dict[str, Any]]:
        """ç²å–æœ€è¿‘å‡ºç¾çš„éŒ¯èª¤æ¨¡å¼"""
        cutoff = (datetime.utcnow() - timedelta(hours=hours)).isoformat()
        
        with self._data_lock:
            recent = [
                (pattern, data) for pattern, data in self._patterns.items()
                if data['last_seen'] >= cutoff
            ]
        
        recent.sort(key=lambda x: x[1]['count'], reverse=True)
        return [
            {
                'pattern': pattern,
                'count': data['count'],
                'first_seen': data['first_seen'],
                'last_seen': data['last_seen'],
            }
            for pattern, data in recent[:limit]
        ]
    
    def get_stats(self) -> Dict[str, Any]:
        """ç²å–èšé¡çµ±è¨ˆ"""
        with self._data_lock:
            total_patterns = len(self._patterns)
            total_errors = sum(d['count'] for d in self._patterns.values())
        
        return {
            'total_patterns': total_patterns,
            'total_errors': total_errors,
            'top_patterns': self.get_top_patterns(5),
        }
    
    def clear(self):
        """æ¸…é™¤æ‰€æœ‰æ¨¡å¼"""
        with self._data_lock:
            self._patterns.clear()


def get_error_cluster() -> ErrorPatternCluster:
    """ç²å–éŒ¯èª¤æ¨¡å¼èšé¡å™¨å¯¦ä¾‹"""
    return ErrorPatternCluster()
