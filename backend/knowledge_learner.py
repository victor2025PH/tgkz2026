"""
Knowledge Learner - å¾ Telegram èŠå¤©è¨˜éŒ„è‡ªå‹•å­¸ç¿’çŸ¥è­˜
åŠŸèƒ½ï¼š
- å¾æˆåŠŸå°è©±ä¸­æå– Q&A
- å‘é‡åŒ–å­˜å„²çŸ¥è­˜
- æª¢ç´¢ç›¸é—œçŸ¥è­˜è¼”åŠ© AI å›è¦†
- æ•´åˆ TelegramRAGSystem é€²è¡Œçµ±ä¸€çŸ¥è­˜ç®¡ç†

æ³¨æ„ï¼šæ­¤æ¨¡çµ„ç¾å·²æ•´åˆ TelegramRAGSystemï¼Œæä¾›å‘ä¸‹å…¼å®¹çš„ API
"""
import sys
import json
import asyncio
import hashlib
from typing import Dict, Any, Optional, List, Tuple
from datetime import datetime, timedelta
from database import db

# å°å…¥æ–°çš„ RAG ç³»çµ±
try:
    from telegram_rag_system import telegram_rag, ConversationOutcome, KnowledgeType
    RAG_SYSTEM_AVAILABLE = True
except ImportError:
    RAG_SYSTEM_AVAILABLE = False
    print("[KnowledgeLearner] TelegramRAGSystem æœªè¼‰å…¥ï¼Œä½¿ç”¨ç¨ç«‹æ¨¡å¼", file=sys.stderr)

# å˜—è©¦å°å…¥å‘é‡åŒ–ç›¸é—œåº«
try:
    import chromadb
    from chromadb.config import Settings
    CHROMADB_AVAILABLE = True
except ImportError:
    CHROMADB_AVAILABLE = False
    print("[KnowledgeLearner] ChromaDB æœªå®‰è£ï¼Œå°‡ä½¿ç”¨ç°¡åŒ–ç‰ˆæœ¬", file=sys.stderr)

try:
    from sentence_transformers import SentenceTransformer
    SENTENCE_TRANSFORMERS_AVAILABLE = True
except ImportError:
    SENTENCE_TRANSFORMERS_AVAILABLE = False
    print("[KnowledgeLearner] SentenceTransformers æœªå®‰è£ï¼Œå°‡ä½¿ç”¨é—œéµè©åŒ¹é…", file=sys.stderr)

# ğŸ”§ å…¨å±€ ChromaDB å®¢æˆ¶ç«¯ï¼Œé¿å…é‡è¤‡å‰µå»º
_shared_chroma_client = None


class KnowledgeLearner:
    """çŸ¥è­˜å­¸ç¿’æœå‹™ - å¾å°è©±ä¸­è‡ªå‹•å­¸ç¿’"""
    
    # æˆåŠŸå°è©±çš„æ¨™æº–
    SUCCESS_STAGES = ['converted', 'interested', 'negotiating']
    
    # çŸ¥è­˜é¡å‹
    KNOWLEDGE_TYPES = {
        'qa': 'Q&A å•ç­”',
        'script': 'è©±è¡“ç¯„ä¾‹',
        'product': 'ç”¢å“ä¿¡æ¯',
        'objection': 'ç•°è­°è™•ç†',
        'greeting': 'é–‹å ´ç™½',
        'closing': 'æˆäº¤è©±è¡“'
    }
    
    def __init__(self):
        self.is_initialized = False
        self.chroma_client = None
        self.collection = None
        self.embedding_model = None
        self.event_callback = None
        self._knowledge_cache: Dict[str, Dict] = {}
        
    def log(self, message: str, level: str = "info"):
        """è¨˜éŒ„æ—¥èªŒ"""
        formatted = f"[KnowledgeLearner] {message}"
        print(formatted, file=sys.stderr)
        if self.event_callback:
            self.event_callback("log-entry", {
                "message": formatted,
                "type": level
            })
    
    async def initialize(self, use_neural: bool = False):
        """
        åˆå§‹åŒ–çŸ¥è­˜å­¸ç¿’ç³»çµ±
        
        Args:
            use_neural: æ˜¯å¦ä½¿ç”¨ç¥ç¶“ç¶²çµ¡åµŒå…¥ï¼ˆé»˜èª False ç¯€çœ 300-500MB å…§å­˜ï¼‰
        """
        if self.is_initialized:
            return True
        
        try:
            # åˆå§‹åŒ–å‘é‡æ•¸æ“šåº«
            # ğŸ”§ Phase 1 å„ªåŒ–ï¼šä½¿ç”¨çµ±ä¸€çš„ ChromaDB ç›®éŒ„ï¼Œæ¸›å°‘å…§å­˜ä½”ç”¨
            if CHROMADB_AVAILABLE:
                import os
                persist_dir = os.path.join(os.path.dirname(__file__), "chroma_rag_db")
                os.makedirs(persist_dir, exist_ok=True)
                
                # ğŸ”§ ä¿®å¾©ï¼šä½¿ç”¨å…¨å±€å–®ä¾‹é¿å…é‡è¤‡å‰µå»º
                global _shared_chroma_client
                if '_shared_chroma_client' not in globals() or _shared_chroma_client is None:
                    try:
                        # å˜—è©¦ä½¿ç”¨ PersistentClientï¼ˆæ–°ç‰ˆ ChromaDB 0.4.0+ï¼‰
                        try:
                            _shared_chroma_client = chromadb.PersistentClient(path=persist_dir)
                        except AttributeError:
                            # é™ç´šåˆ°èˆŠç‰ˆ API
                            _shared_chroma_client = chromadb.Client(Settings(
                                persist_directory=persist_dir,
                                anonymized_telemetry=False
                            ))
                        self.log(f"âœ“ ChromaDB æ–°å»ºå®¢æˆ¶ç«¯")
                    except ValueError as ve:
                        # ğŸ”§ ä¿®å¾©ï¼šå¯¦ä¾‹å·²å­˜åœ¨ï¼Œå˜—è©¦ç²å–ç¾æœ‰å¯¦ä¾‹
                        if "already exists" in str(ve):
                            self.log(f"âš  ChromaDB å¯¦ä¾‹å·²å­˜åœ¨ï¼Œå˜—è©¦å¾©ç”¨", "warning")
                            try:
                                # ä½¿ç”¨å…§å­˜æ¨¡å¼ä½œç‚ºå‚™é¸
                                _shared_chroma_client = chromadb.Client()
                                self.log(f"âœ“ ChromaDB ä½¿ç”¨å…§å­˜æ¨¡å¼")
                            except Exception:
                                self.log(f"âŒ ChromaDB åˆå§‹åŒ–å¤±æ•—ï¼Œç¦ç”¨çŸ¥è­˜å­¸ç¿’", "error")
                                self.chroma_client = None
                                self.collection = None
                                return
                        else:
                            raise
                
                self.chroma_client = _shared_chroma_client
                
                # ç²å–æˆ–å‰µå»ºé›†åˆ
                try:
                    self.collection = self.chroma_client.get_or_create_collection(
                        name="telegram_knowledge",
                        metadata={"description": "å¾ Telegram å°è©±å­¸ç¿’çš„çŸ¥è­˜åº«"}
                    )
                    self.log(f"âœ“ ChromaDB å·²åˆå§‹åŒ–ï¼Œç¾æœ‰ {self.collection.count()} æ¢çŸ¥è­˜")
                except Exception as coll_err:
                    self.log(f"âš  ChromaDB é›†åˆç²å–å¤±æ•—: {coll_err}", "warning")
                    self.collection = None
            
            # ğŸ”§ Phase 1 å„ªåŒ–ï¼šé»˜èªç¦ç”¨ç¥ç¶“ç¶²çµ¡åµŒå…¥ï¼Œç¯€çœ 300-500MB å…§å­˜
            if use_neural and SENTENCE_TRANSFORMERS_AVAILABLE:
                # ä½¿ç”¨å¤šèªè¨€æ¨¡å‹ï¼Œæ”¯æŒä¸­æ–‡
                self.embedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
                self.log("âœ“ Embedding æ¨¡å‹å·²è¼‰å…¥")
            else:
                self.log("âœ“ ä½¿ç”¨ç°¡å–®åµŒå…¥ï¼ˆç¯€çœå…§å­˜ï¼‰")
            
            # ç¢ºä¿æ•¸æ“šåº«è¡¨å­˜åœ¨
            await self._ensure_tables()
            
            self.is_initialized = True
            self.log("âœ“ çŸ¥è­˜å­¸ç¿’ç³»çµ±åˆå§‹åŒ–å®Œæˆ", "success")
            return True
            
        except Exception as e:
            self.log(f"åˆå§‹åŒ–å¤±æ•—: {e}", "error")
            import traceback
            traceback.print_exc(file=sys.stderr)
            return False
    
    async def _ensure_tables(self):
        """ç¢ºä¿æ•¸æ“šåº«è¡¨å­˜åœ¨"""
        await db._connection.execute("""
            CREATE TABLE IF NOT EXISTS learned_knowledge (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                type TEXT NOT NULL,
                question TEXT NOT NULL,
                answer TEXT NOT NULL,
                context TEXT,
                source_user_id TEXT,
                source_chat_id TEXT,
                success_score REAL DEFAULT 0.5,
                use_count INTEGER DEFAULT 0,
                feedback_positive INTEGER DEFAULT 0,
                feedback_negative INTEGER DEFAULT 0,
                keywords TEXT,
                embedding_id TEXT,
                is_active INTEGER DEFAULT 1,
                created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                updated_at DATETIME DEFAULT CURRENT_TIMESTAMP
            )
        """)
        
        await db._connection.execute("""
            CREATE TABLE IF NOT EXISTS conversation_ratings (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                user_id TEXT NOT NULL,
                session_id TEXT,
                outcome TEXT,
                message_count INTEGER DEFAULT 0,
                auto_rating REAL DEFAULT 0,
                manual_rating REAL,
                is_extracted INTEGER DEFAULT 0,
                created_at DATETIME DEFAULT CURRENT_TIMESTAMP
            )
        """)
        
        # å‰µå»ºç´¢å¼•
        try:
            await db._connection.execute(
                "CREATE INDEX IF NOT EXISTS idx_knowledge_type ON learned_knowledge(type)"
            )
            await db._connection.execute(
                "CREATE INDEX IF NOT EXISTS idx_knowledge_active ON learned_knowledge(is_active)"
            )
        except:
            pass
        
        await db._connection.commit()
    
    async def learn_from_conversation(self, user_id: str, 
                                       messages: List[Dict[str, Any]],
                                       outcome: str = None,
                                       account_phone: str = "",
                                       chat_id: str = "") -> Dict[str, Any]:
        """
        å¾å°è©±ä¸­å­¸ç¿’çŸ¥è­˜
        
        Args:
            user_id: ç”¨æˆ¶ ID
            messages: å°è©±æ¶ˆæ¯åˆ—è¡¨ [{role, content, timestamp}, ...]
            outcome: å°è©±çµæœ (converted/interested/churned)
            account_phone: å¸³è™Ÿé›»è©±
            chat_id: èŠå¤© ID
        
        Returns:
            å­¸ç¿’çµæœçµ±è¨ˆ
        """
        if not self.is_initialized:
            await self.initialize()
        
        # å„ªå…ˆä½¿ç”¨æ–°çš„ RAG ç³»çµ±
        if RAG_SYSTEM_AVAILABLE:
            try:
                # è½‰æ› outcome
                outcome_enum = self._parse_outcome(outcome)
                
                # ä½¿ç”¨ TelegramRAGSystem
                rag_result = await telegram_rag.learn_from_conversation(
                    user_id=user_id,
                    messages=messages,
                    outcome=outcome_enum,
                    account_phone=account_phone,
                    chat_id=chat_id
                )
                
                return {
                    'qa_extracted': rag_result.get('qa_extracted', 0),
                    'scripts_extracted': rag_result.get('scripts_extracted', 0),
                    'total_knowledge': rag_result.get('total_knowledge', 0),
                    'quality_score': rag_result.get('quality_score', 0),
                    'source': 'rag_system'
                }
            except Exception as e:
                self.log(f"RAG ç³»çµ±å­¸ç¿’å¤±æ•—ï¼Œå›é€€åˆ°ç¨ç«‹æ¨¡å¼: {e}", "warning")
        
        # å›é€€åˆ°ç¨ç«‹æ¨¡å¼
        result = {
            'qa_extracted': 0,
            'scripts_extracted': 0,
            'total_knowledge': 0
        }
        
        if len(messages) < 2:
            return result
        
        try:
            # è©•ä¼°å°è©±è³ªé‡
            quality_score = self._calculate_quality_score(messages, outcome)
            
            # åªå¾é«˜è³ªé‡å°è©±ä¸­å­¸ç¿’
            if quality_score < 0.3:
                self.log(f"å°è©±è³ªé‡è¼ƒä½ ({quality_score:.2f})ï¼Œè·³éå­¸ç¿’")
                return result
            
            # æå– Q&A å°
            qa_pairs = self._extract_qa_pairs(messages)
            for qa in qa_pairs:
                await self._save_knowledge(
                    type='qa',
                    question=qa['question'],
                    answer=qa['answer'],
                    context=qa.get('context', ''),
                    source_user_id=user_id,
                    success_score=quality_score
                )
                result['qa_extracted'] += 1
            
            # æå–æˆåŠŸè©±è¡“
            if outcome in self.SUCCESS_STAGES:
                scripts = self._extract_successful_scripts(messages)
                for script in scripts:
                    await self._save_knowledge(
                        type='script',
                        question=script.get('trigger', ''),
                        answer=script['content'],
                        context=script.get('context', ''),
                        source_user_id=user_id,
                        success_score=quality_score
                    )
                    result['scripts_extracted'] += 1
            
            result['total_knowledge'] = result['qa_extracted'] + result['scripts_extracted']
            
            if result['total_knowledge'] > 0:
                self.log(f"âœ“ å¾å°è©±å­¸ç¿’äº† {result['total_knowledge']} æ¢çŸ¥è­˜ (Q&A: {result['qa_extracted']}, è©±è¡“: {result['scripts_extracted']})")
            
            return result
            
        except Exception as e:
            self.log(f"å­¸ç¿’å¤±æ•—: {e}", "error")
            return result
    
    def _parse_outcome(self, outcome_str: str) -> 'ConversationOutcome':
        """è§£æçµæœå­—ç¬¦ä¸²ç‚ºæšèˆ‰"""
        if not RAG_SYSTEM_AVAILABLE:
            return None
        
        outcome_map = {
            'converted': ConversationOutcome.CONVERTED,
            'interested': ConversationOutcome.INTERESTED,
            'negotiating': ConversationOutcome.NEGOTIATING,
            'replied': ConversationOutcome.REPLIED,
            'contacted': ConversationOutcome.CONTACTED,
            'churned': ConversationOutcome.CHURNED,
        }
        return outcome_map.get(outcome_str, ConversationOutcome.UNKNOWN) if outcome_str else ConversationOutcome.UNKNOWN
    
    def _calculate_quality_score(self, messages: List[Dict], outcome: str = None) -> float:
        """è¨ˆç®—å°è©±è³ªé‡è©•åˆ†"""
        score = 0.3  # åŸºç¤åˆ†
        
        # æ ¹æ“šçµæœåŠ åˆ†
        if outcome == 'converted':
            score += 0.4
        elif outcome == 'interested':
            score += 0.2
        elif outcome == 'negotiating':
            score += 0.15
        
        # æ ¹æ“šå°è©±é•·åº¦åŠ åˆ†ï¼ˆå¤šè¼ªå°è©±æ›´æœ‰åƒ¹å€¼ï¼‰
        msg_count = len(messages)
        if msg_count >= 10:
            score += 0.2
        elif msg_count >= 5:
            score += 0.1
        
        # æ ¹æ“šç”¨æˆ¶åƒèˆ‡åº¦åŠ åˆ†
        user_msgs = [m for m in messages if m.get('role') == 'user']
        if len(user_msgs) >= 5:
            score += 0.1
        
        return min(1.0, score)
    
    def _extract_qa_pairs(self, messages: List[Dict]) -> List[Dict]:
        """å¾å°è©±ä¸­æå– Q&A å°"""
        qa_pairs = []
        
        for i in range(len(messages) - 1):
            msg = messages[i]
            next_msg = messages[i + 1]
            
            # ç”¨æˆ¶å•é¡Œ â†’ AI/äººå·¥å›è¦†
            if msg.get('role') == 'user' and next_msg.get('role') == 'assistant':
                question = msg.get('content', '').strip()
                answer = next_msg.get('content', '').strip()
                
                # éæ¿¾å¤ªçŸ­çš„å…§å®¹
                if len(question) >= 5 and len(answer) >= 10:
                    # æª¢æŸ¥æ˜¯å¦æ˜¯å•é¡Œï¼ˆå«å•è™Ÿæˆ–ç–‘å•è©ï¼‰
                    question_indicators = ['?', 'ï¼Ÿ', 'å—', 'å‘¢', 'æ€éº¼', 'ä»€éº¼', 'å¦‚ä½•', 'å¤šå°‘', 'å“ª', 'ç‚ºä»€éº¼', 'èƒ½ä¸èƒ½', 'å¯ä»¥']
                    is_question = any(ind in question for ind in question_indicators)
                    
                    if is_question or len(question) >= 10:
                        qa_pairs.append({
                            'question': question,
                            'answer': answer,
                            'context': self._get_context(messages, i)
                        })
        
        return qa_pairs
    
    def _extract_successful_scripts(self, messages: List[Dict]) -> List[Dict]:
        """æå–æˆåŠŸçš„è©±è¡“"""
        scripts = []
        
        # æå– AI/äººå·¥çš„å›è¦†ä½œç‚ºè©±è¡“
        for i, msg in enumerate(messages):
            if msg.get('role') == 'assistant':
                content = msg.get('content', '').strip()
                
                # éæ¿¾å¤ªçŸ­çš„å›è¦†
                if len(content) >= 20:
                    # ç²å–è§¸ç™¼é€™å€‹å›è¦†çš„ç”¨æˆ¶æ¶ˆæ¯
                    trigger = ''
                    if i > 0 and messages[i-1].get('role') == 'user':
                        trigger = messages[i-1].get('content', '')
                    
                    scripts.append({
                        'trigger': trigger,
                        'content': content,
                        'context': self._get_context(messages, i)
                    })
        
        return scripts
    
    def _get_context(self, messages: List[Dict], index: int, window: int = 2) -> str:
        """ç²å–æ¶ˆæ¯çš„ä¸Šä¸‹æ–‡"""
        start = max(0, index - window)
        end = min(len(messages), index + window + 1)
        
        context_msgs = []
        for msg in messages[start:end]:
            role = msg.get('role', 'user')
            content = msg.get('content', '')[:100]
            context_msgs.append(f"{role}: {content}")
        
        return '\n'.join(context_msgs)
    
    async def _save_knowledge(self, type: str, question: str, answer: str,
                               context: str = '', source_user_id: str = '',
                               success_score: float = 0.5) -> Optional[int]:
        """ä¿å­˜çŸ¥è­˜åˆ°æ•¸æ“šåº«"""
        try:
            # æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸ä¼¼çŸ¥è­˜
            existing = await self._find_similar_knowledge(question)
            if existing:
                # æ›´æ–°ç¾æœ‰çŸ¥è­˜çš„è©•åˆ†
                await db._connection.execute("""
                    UPDATE learned_knowledge 
                    SET success_score = (success_score + ?) / 2,
                        use_count = use_count + 1,
                        updated_at = CURRENT_TIMESTAMP
                    WHERE id = ?
                """, (success_score, existing['id']))
                await db._connection.commit()
                return existing['id']
            
            # ç”Ÿæˆé—œéµè©
            keywords = self._extract_keywords(question + ' ' + answer)
            
            # ç”Ÿæˆ embedding ID
            embedding_id = None
            if self.collection and self.embedding_model:
                embedding_id = hashlib.md5(question.encode()).hexdigest()
                embedding = self.embedding_model.encode(question).tolist()
                
                self.collection.add(
                    embeddings=[embedding],
                    documents=[answer],
                    metadatas=[{
                        'type': type,
                        'question': question,
                        'keywords': keywords
                    }],
                    ids=[embedding_id]
                )
            
            # ä¿å­˜åˆ°æ•¸æ“šåº«
            cursor = await db._connection.execute("""
                INSERT INTO learned_knowledge 
                (type, question, answer, context, source_user_id, success_score, keywords, embedding_id)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            """, (type, question, answer, context, source_user_id, success_score, keywords, embedding_id))
            
            await db._connection.commit()
            return cursor.lastrowid
            
        except Exception as e:
            self.log(f"ä¿å­˜çŸ¥è­˜å¤±æ•—: {e}", "error")
            return None
    
    async def _find_similar_knowledge(self, question: str) -> Optional[Dict]:
        """æŸ¥æ‰¾ç›¸ä¼¼çš„çŸ¥è­˜"""
        keywords = self._extract_keywords(question)
        if not keywords:
            return None
        
        # ä½¿ç”¨é—œéµè©åŒ¹é…
        keyword_list = keywords.split(',')
        placeholders = ' OR '.join(['keywords LIKE ?' for _ in keyword_list])
        params = [f'%{kw}%' for kw in keyword_list]
        
        cursor = await db._connection.execute(f"""
            SELECT * FROM learned_knowledge 
            WHERE is_active = 1 AND ({placeholders})
            ORDER BY success_score DESC
            LIMIT 1
        """, params)
        
        row = await cursor.fetchone()
        return dict(row) if row else None
    
    def _extract_keywords(self, text: str) -> str:
        """æå–é—œéµè©"""
        # ç°¡å–®çš„é—œéµè©æå–
        import re
        
        # ç§»é™¤æ¨™é»ç¬¦è™Ÿ
        text = re.sub(r'[^\w\s]', ' ', text)
        
        # åˆ†è©ï¼ˆç°¡å–®æŒ‰ç©ºæ ¼å’Œä¸­æ–‡å­—ç¬¦åˆ†å‰²ï¼‰
        words = []
        current_word = ''
        for char in text:
            if '\u4e00' <= char <= '\u9fff':  # ä¸­æ–‡å­—ç¬¦
                if current_word:
                    words.append(current_word)
                    current_word = ''
                words.append(char)
            elif char.isalnum():
                current_word += char
            else:
                if current_word:
                    words.append(current_word)
                    current_word = ''
        if current_word:
            words.append(current_word)
        
        # éæ¿¾åœç”¨è©å’ŒçŸ­è©
        stopwords = {'çš„', 'æ˜¯', 'åœ¨', 'æœ‰', 'å’Œ', 'äº†', 'ä¸', 'é€™', 'é‚£', 'æˆ‘', 'ä½ ', 'ä»–', 'å¥¹', 'å®ƒ', 
                     'å€‘', 'å—', 'å‘¢', 'å§', 'å•Š', 'å“¦', 'å‘€', 'the', 'a', 'an', 'is', 'are', 'was', 'were'}
        keywords = [w for w in words if len(w) >= 2 and w.lower() not in stopwords]
        
        return ','.join(keywords[:10])  # æœ€å¤š10å€‹é—œéµè©
    
    async def search_knowledge(self, query: str, limit: int = 5) -> List[Dict[str, Any]]:
        """
        æœç´¢ç›¸é—œçŸ¥è­˜
        
        Args:
            query: æœç´¢æŸ¥è©¢
            limit: è¿”å›æ•¸é‡é™åˆ¶
        
        Returns:
            ç›¸é—œçŸ¥è­˜åˆ—è¡¨
        """
        if not self.is_initialized:
            await self.initialize()
        
        # å„ªå…ˆä½¿ç”¨æ–°çš„ RAG ç³»çµ±
        if RAG_SYSTEM_AVAILABLE:
            try:
                rag_results = await telegram_rag.search(query, limit=limit)
                
                results = []
                for r in rag_results:
                    results.append({
                        'id': r.item.id,
                        'type': r.item.knowledge_type.value,
                        'question': r.item.question,
                        'answer': r.item.answer,
                        'success_score': r.item.success_score,
                        'similarity': r.similarity,
                        'source': r.source
                    })
                
                return results
            except Exception as e:
                self.log(f"RAG æœç´¢å¤±æ•—ï¼Œå›é€€åˆ°ç¨ç«‹æ¨¡å¼: {e}", "warning")
        
        results = []
        
        try:
            # æ–¹æ³•1ï¼šä½¿ç”¨å‘é‡æœç´¢ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.collection and self.embedding_model:
                query_embedding = self.embedding_model.encode(query).tolist()
                
                chroma_results = self.collection.query(
                    query_embeddings=[query_embedding],
                    n_results=limit
                )
                
                if chroma_results['ids'] and chroma_results['ids'][0]:
                    for i, doc_id in enumerate(chroma_results['ids'][0]):
                        results.append({
                            'id': doc_id,
                            'answer': chroma_results['documents'][0][i] if chroma_results['documents'] else '',
                            'metadata': chroma_results['metadatas'][0][i] if chroma_results['metadatas'] else {},
                            'distance': chroma_results['distances'][0][i] if chroma_results.get('distances') else 0,
                            'source': 'vector'
                        })
            
            # æ–¹æ³•2ï¼šä½¿ç”¨é—œéµè©æœç´¢ï¼ˆå‚™é¸ï¼‰
            if len(results) < limit:
                keywords = self._extract_keywords(query)
                if keywords:
                    keyword_list = keywords.split(',')[:5]
                    conditions = ' OR '.join(['keywords LIKE ?' for _ in keyword_list])
                    params = [f'%{kw}%' for kw in keyword_list]
                    
                    cursor = await db._connection.execute(f"""
                        SELECT * FROM learned_knowledge 
                        WHERE is_active = 1 AND ({conditions})
                        ORDER BY success_score DESC, use_count DESC
                        LIMIT ?
                    """, params + [limit - len(results)])
                    
                    rows = await cursor.fetchall()
                    for row in rows:
                        row_dict = dict(row)
                        if row_dict.get('embedding_id') not in [r.get('id') for r in results]:
                            results.append({
                                'id': row_dict['id'],
                                'type': row_dict['type'],
                                'question': row_dict['question'],
                                'answer': row_dict['answer'],
                                'success_score': row_dict['success_score'],
                                'source': 'keyword'
                            })
            
            return results[:limit]
            
        except Exception as e:
            self.log(f"æœç´¢çŸ¥è­˜å¤±æ•—: {e}", "error")
            return []
    
    async def get_relevant_context(self, user_message: str, user_id: str = None) -> str:
        """
        ç²å–èˆ‡ç”¨æˆ¶æ¶ˆæ¯ç›¸é—œçš„çŸ¥è­˜ä¸Šä¸‹æ–‡
        ç”¨æ–¼å¢å¼· AI å›è¦†
        
        Args:
            user_message: ç”¨æˆ¶æ¶ˆæ¯
            user_id: ç”¨æˆ¶ IDï¼ˆå¯é¸ï¼Œç”¨æ–¼å€‹æ€§åŒ–ï¼‰
        
        Returns:
            ç›¸é—œçŸ¥è­˜çš„æ ¼å¼åŒ–æ–‡æœ¬
        """
        # å„ªå…ˆä½¿ç”¨æ–°çš„ RAG ç³»çµ±
        if RAG_SYSTEM_AVAILABLE:
            try:
                rag_context = await telegram_rag.build_rag_context(
                    user_message=user_message,
                    user_id=user_id,
                    max_items=3,
                    max_tokens=800
                )
                if rag_context:
                    return rag_context
            except Exception as e:
                self.log(f"RAG ä¸Šä¸‹æ–‡æ§‹å»ºå¤±æ•—: {e}", "warning")
        
        # å›é€€åˆ°åŸæœ‰é‚è¼¯
        knowledge_items = await self.search_knowledge(user_message, limit=3)
        
        if not knowledge_items:
            return ""
        
        context_parts = ["ã€åƒè€ƒçŸ¥è­˜åº«ã€‘"]
        
        for i, item in enumerate(knowledge_items, 1):
            if item.get('question'):
                context_parts.append(f"{i}. å•: {item['question']}")
                context_parts.append(f"   ç­”: {item['answer'][:200]}...")
            elif item.get('answer'):
                context_parts.append(f"{i}. åƒè€ƒå›è¦†: {item['answer'][:200]}...")
        
        context_parts.append("ã€è«‹åƒè€ƒä»¥ä¸ŠçŸ¥è­˜å›è¦†ç”¨æˆ¶ã€‘")
        
        return '\n'.join(context_parts)
    
    async def record_feedback(self, knowledge_id: int, is_positive: bool):
        """è¨˜éŒ„çŸ¥è­˜ä½¿ç”¨åé¥‹"""
        try:
            field = 'feedback_positive' if is_positive else 'feedback_negative'
            await db._connection.execute(f"""
                UPDATE learned_knowledge 
                SET {field} = {field} + 1,
                    use_count = use_count + 1,
                    updated_at = CURRENT_TIMESTAMP
                WHERE id = ?
            """, (knowledge_id,))
            await db._connection.commit()
        except Exception as e:
            self.log(f"è¨˜éŒ„åé¥‹å¤±æ•—: {e}", "error")
    
    async def get_statistics(self) -> Dict[str, Any]:
        """ç²å–çŸ¥è­˜åº«çµ±è¨ˆ"""
        try:
            cursor = await db._connection.execute("""
                SELECT 
                    type,
                    COUNT(*) as count,
                    AVG(success_score) as avg_score,
                    SUM(use_count) as total_uses
                FROM learned_knowledge
                WHERE is_active = 1
                GROUP BY type
            """)
            
            rows = await cursor.fetchall()
            
            stats = {
                'by_type': {},
                'total_knowledge': 0,
                'total_uses': 0
            }
            
            for row in rows:
                row_dict = dict(row)
                type_name = self.KNOWLEDGE_TYPES.get(row_dict['type'], row_dict['type'])
                stats['by_type'][type_name] = {
                    'count': row_dict['count'],
                    'avg_score': round(row_dict['avg_score'] or 0, 2),
                    'uses': row_dict['total_uses'] or 0
                }
                stats['total_knowledge'] += row_dict['count']
                stats['total_uses'] += row_dict['total_uses'] or 0
            
            # ChromaDB çµ±è¨ˆ
            if self.collection:
                stats['vector_count'] = self.collection.count()
            
            return stats
            
        except Exception as e:
            self.log(f"ç²å–çµ±è¨ˆå¤±æ•—: {e}", "error")
            return {'error': str(e)}


# å‰µå»ºå…¨å±€å¯¦ä¾‹
knowledge_learner = KnowledgeLearner()
