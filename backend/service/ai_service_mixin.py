"""
Phase 9-3: AI generation, local AI, knowledge, collaboration
Extracted from BackendService in main.py.

ğŸ”§ P3-1: æ¨¡å—çº§å¯¼å…¥æ¸…ç† â€” æ¶ˆé™¤æ–¹æ³•å†…é‡å¤å¯¼å…¥
"""
import re
import sys
import json
import time
import random
import asyncio
import traceback
from typing import Dict, Any, Optional, List
from datetime import datetime, timedelta
from text_utils import safe_json_dumps

# Re-use main.py's db and module accessors
from database import db
from config import config, IS_DEV_MODE

def _get_module(name: str):
    """Safe lazy module accessor."""
    from lazy_imports import lazy_imports
    return lazy_imports.get(name)


# ====================================================================
# ğŸ”§ P3-1: å»¶è¿Ÿå¯¼å…¥è·å–å™¨ â€” é¿å…å¾ªç¯ä¾èµ–
# ====================================================================

def _get_ai_auto_chat():
    """å»¶è¿Ÿè·å– ai_auto_chat å•ä¾‹"""
    try:
        return _get_module('ai_auto_chat').ai_auto_chat
    except Exception:
        return None

def _get_telegram_rag():
    """å»¶è¿Ÿè·å– telegram_rag å•ä¾‹"""
    try:
        return _get_module('telegram_rag_system').telegram_rag
    except Exception:
        return None

def _get_KnowledgeType():
    """å»¶è¿Ÿè·å– KnowledgeType æšä¸¾"""
    try:
        return _get_module('telegram_rag_system').KnowledgeType
    except Exception:
        return None

def _get_group_search_service():
    """å»¶è¿Ÿè·å– group_search_service å•ä¾‹"""
    try:
        return _get_module('group_search_service').group_search_service
    except Exception:
        return None

class AiServiceMixin:
    """Mixin: AI generation, local AI, knowledge, collaboration"""

    async def _get_default_ai_model(self) -> Optional[Dict[str, Any]]:
        """ç²å–é»˜èªçš„ AI æ¨¡å‹é…ç½®"""
        try:
            model = await db.fetch_one(
                """SELECT id, provider, model_name, display_name, api_key, api_endpoint,
                   is_local, is_default, is_connected
                   FROM ai_models WHERE is_default = 1 AND (api_key != '' OR is_local = 1)
                   ORDER BY priority DESC LIMIT 1"""
            )
            if model:
                return {
                    'id': model['id'],
                    'provider': model['provider'],
                    'modelName': model['model_name'],
                    'displayName': model['display_name'] or model['model_name'],
                    'apiKey': model['api_key'],
                    'apiEndpoint': model['api_endpoint'],
                    'isLocal': bool(model['is_local']),
                    'isConnected': bool(model['is_connected'])
                }
            
            # å¦‚æœæ²’æœ‰é»˜èªæ¨¡å‹ï¼Œå˜—è©¦ç²å–ä»»ä½•å¯ç”¨çš„æ¨¡å‹
            model = await db.fetch_one(
                """SELECT id, provider, model_name, display_name, api_key, api_endpoint,
                   is_local, is_default, is_connected
                   FROM ai_models WHERE (api_key != '' OR is_local = 1)
                   ORDER BY priority DESC, created_at DESC LIMIT 1"""
            )
            if model:
                return {
                    'id': model['id'],
                    'provider': model['provider'],
                    'modelName': model['model_name'],
                    'displayName': model['display_name'] or model['model_name'],
                    'apiKey': model['api_key'],
                    'apiEndpoint': model['api_endpoint'],
                    'isLocal': bool(model['is_local']),
                    'isConnected': bool(model['is_connected'])
                }
            return None
        except Exception as e:
            print(f"[AI] ç²å– AI æ¨¡å‹å¤±æ•—: {e}", file=__import__('sys').stderr)
            return None

    async def _call_ai_for_text(self, model: Dict[str, Any], prompt: str, max_tokens: int = 500) -> Optional[str]:
        """
        ğŸ†• é€šç”¨ AI èª¿ç”¨æ–¹æ³•
        ğŸ”§ P0: å¢åŠ è¶…æ™‚æ™‚é–“åˆ° 45 ç§’
        """
        import aiohttp
        
        provider = model.get('provider', '').lower()
        api_key = model.get('apiKey', '')
        api_endpoint = model.get('apiEndpoint', '')
        model_name = model.get('modelName', '')
        is_local = model.get('isLocal', False)
        
        # ğŸ”§ P0: å¢åŠ è¶…æ™‚æ™‚é–“ï¼Œèˆ‡å‰ç«¯ä¸€è‡´ï¼ˆä½¿ç”¨é…ç½®å¸¸é‡ï¼‰
        from config import AIConfig
        timeout = aiohttp.ClientTimeout(total=AIConfig.API_TIMEOUT_SECONDS)
        start_time = time.time()
        print(f"[AI] é–‹å§‹èª¿ç”¨: provider={provider}, model={model_name}, endpoint={api_endpoint[:50] if api_endpoint else 'default'}...", file=sys.stderr)
        
        try:
            async with aiohttp.ClientSession(timeout=timeout) as session:
                if is_local or provider == 'ollama' or provider == 'custom':
                    # Ollama / æœ¬åœ°æ¨¡å‹
                    endpoint = api_endpoint or 'http://localhost:11434'
                    
                    # ğŸ”§ ä¿®å¾©: æª¢æŸ¥ç«¯é»æ˜¯å¦å·²åŒ…å« /api/chatï¼Œé¿å…é‡è¤‡æ·»åŠ 
                    if '/api/chat' in endpoint or '/api/generate' in endpoint:
                        chat_url = endpoint
                    else:
                        chat_url = f"{endpoint.rstrip('/')}/api/chat"
                    
                    print(f"[AI] æœ¬åœ° AI è«‹æ±‚ URL: {chat_url}", file=sys.stderr)
                    
                    async with session.post(chat_url, json={
                        "model": model_name or "llama3",
                        "messages": [{"role": "user", "content": prompt}],
                        "stream": False,
                        "options": {"num_predict": max_tokens}
                    }) as resp:
                        if resp.status == 200:
                            data = await resp.json()
                            return data.get('message', {}).get('content', '')
                
                elif provider == 'gemini' or provider == 'google':
                    # Google Gemini
                    endpoint = api_endpoint or 'https://generativelanguage.googleapis.com/v1beta'
                    url = f"{endpoint}/models/{model_name or 'gemini-pro'}:generateContent?key={api_key}"
                    
                    async with session.post(url, json={
                        "contents": [{"parts": [{"text": prompt}]}],
                        "generationConfig": {"maxOutputTokens": max_tokens}
                    }) as resp:
                        if resp.status == 200:
                            data = await resp.json()
                            candidates = data.get('candidates', [])
                            if candidates:
                                parts = candidates[0].get('content', {}).get('parts', [])
                                if parts:
                                    return parts[0].get('text', '')
                
                elif provider == 'openai' or provider == 'gpt':
                    # OpenAI GPT
                    endpoint = api_endpoint or 'https://api.openai.com/v1'
                    url = f"{endpoint.rstrip('/')}/chat/completions"
                    
                    headers = {
                        "Authorization": f"Bearer {api_key}",
                        "Content-Type": "application/json"
                    }
                    
                    async with session.post(url, headers=headers, json={
                        "model": model_name or "gpt-3.5-turbo",
                        "messages": [{"role": "user", "content": prompt}],
                        "max_tokens": max_tokens
                    }) as resp:
                        elapsed = time.time() - start_time
                        if resp.status == 200:
                            data = await resp.json()
                            choices = data.get('choices', [])
                            if choices:
                                content = choices[0].get('message', {}).get('content', '')
                                print(f"[AI] âœ“ OpenAI èª¿ç”¨æˆåŠŸï¼Œè€—æ™‚ {elapsed:.1f}ç§’ï¼Œè¿”å›é•·åº¦ {len(content)}", file=sys.stderr)
                                return content
                        else:
                            error_text = await resp.text()
                            print(f"[AI] âš ï¸ OpenAI è¿”å›éŒ¯èª¤: status={resp.status}, error={error_text[:200]}", file=sys.stderr)
                
                elif provider == 'deepseek':
                    # DeepSeek
                    endpoint = api_endpoint or 'https://api.deepseek.com/v1'
                    url = f"{endpoint.rstrip('/')}/chat/completions"
                    
                    headers = {
                        "Authorization": f"Bearer {api_key}",
                        "Content-Type": "application/json"
                    }
                    
                    async with session.post(url, headers=headers, json={
                        "model": model_name or "deepseek-chat",
                        "messages": [{"role": "user", "content": prompt}],
                        "max_tokens": max_tokens
                    }) as resp:
                        if resp.status == 200:
                            data = await resp.json()
                            choices = data.get('choices', [])
                            if choices:
                                return choices[0].get('message', {}).get('content', '')
                
                print(f"[AI] ä¸æ”¯æŒçš„ provider: {provider}", file=sys.stderr)
                return None
                
        except asyncio.TimeoutError:
            elapsed = time.time() - start_time
            print(f"[AI] âš ï¸ API èª¿ç”¨è¶…æ™‚: {elapsed:.1f}ç§’ (provider={provider})", file=sys.stderr)
            return None
        except Exception as e:
            elapsed = time.time() - start_time
            print(f"[AI] âŒ API èª¿ç”¨å¤±æ•—: {e} (è€—æ™‚ {elapsed:.1f}ç§’)", file=sys.stderr)
            return None

    async def _generate_messages_with_ai(self, model: Dict[str, Any], topic: str, style: str, count: int) -> List[str]:
        """ä½¿ç”¨é…ç½®çš„ AI ç”Ÿæˆæ¶ˆæ¯"""
        import aiohttp
        
        style_descriptions = {
            'friendly': 'å‹å¥½è¦ªåˆ‡ã€è¼•é¬†è‡ªç„¶',
            'formal': 'æ­£å¼å•†å‹™ã€å°ˆæ¥­ç¦®è²Œ',
            'humorous': 'å¹½é»˜é¢¨è¶£ã€è¼•é¬†èª¿ä¾ƒ',
            'concise': 'ç°¡æ½”æ˜äº†ã€ç›´å¥”ä¸»é¡Œ',
            'enthusiastic': 'ç†±æƒ…æ´‹æº¢ã€å……æ»¿æ´»åŠ›'
        }
        
        style_desc = style_descriptions.get(style, 'å‹å¥½è¦ªåˆ‡')
        
        prompt = f"""è«‹ç”Ÿæˆ {count} æ¢ä¸åŒçš„æ‰“æ‹›å‘¼æ¶ˆæ¯ï¼Œç”¨æ–¼åœ¨ Telegram ä¸Šå‘æ½›åœ¨å®¢æˆ¶ç™¼é€ç¬¬ä¸€æ¢æ¶ˆæ¯ã€‚

ä¸»é¡Œï¼š{topic}
é¢¨æ ¼è¦æ±‚ï¼š{style_desc}

è¦æ±‚ï¼š
1. æ¯æ¢æ¶ˆæ¯éƒ½è¦ä¸åŒï¼Œä½†ä¿æŒç›¸åŒçš„é¢¨æ ¼
2. æ¶ˆæ¯è¦è‡ªç„¶ã€çœŸèª ï¼Œä¸è¦åƒå»£å‘Š
3. ä½¿ç”¨è®Šé‡ {{firstName}} è¡¨ç¤ºå°æ–¹åå­—ï¼Œ{{greeting}} è¡¨ç¤ºå•å€™èªï¼ˆå¦‚"æ—©ä¸Šå¥½"ï¼‰
4. æ¯æ¢æ¶ˆæ¯ 20-50 å­—å·¦å³
5. åªè¼¸å‡ºæ¶ˆæ¯å…§å®¹ï¼Œæ¯æ¢æ¶ˆæ¯ä¸€è¡Œï¼Œä¸è¦ç·¨è™Ÿ

è«‹ç›´æ¥è¼¸å‡º {count} æ¢æ¶ˆæ¯ï¼š"""
        
        provider = model.get('provider', '').lower()
        api_key = model.get('apiKey', '')
        api_endpoint = model.get('apiEndpoint', '')
        model_name = model.get('modelName', '')
        is_local = model.get('isLocal', False)
        
        messages = []
        
        timeout = aiohttp.ClientTimeout(total=30)
        async with aiohttp.ClientSession(timeout=timeout) as session:
            if is_local or provider == 'ollama' or provider == 'custom':
                # æœ¬åœ° AI (Ollama)
                endpoint = api_endpoint or 'http://localhost:11434'
                chat_url = f"{endpoint.rstrip('/')}/api/chat"
                
                request_body = {
                    "model": model_name or "qwen2:7b",
                    "messages": [{"role": "user", "content": prompt}],
                    "stream": False
                }
                
                async with session.post(chat_url, json=request_body) as resp:
                    if resp.status == 200:
                        data = await resp.json()
                        content = data.get('message', {}).get('content', '')
                        messages = self._parse_ai_messages(content, count)
                    else:
                        raise Exception(f"Ollama è¿”å› {resp.status}")
                        
            elif provider == 'openai':
                # OpenAI API
                async with session.post(
                    'https://api.openai.com/v1/chat/completions',
                    headers={
                        'Authorization': f'Bearer {api_key}',
                        'Content-Type': 'application/json'
                    },
                    json={
                        'model': model_name or 'gpt-3.5-turbo',
                        'messages': [{'role': 'user', 'content': prompt}],
                        'max_tokens': 1000,
                        'temperature': 0.8
                    }
                ) as resp:
                    if resp.status == 200:
                        data = await resp.json()
                        content = data.get('choices', [{}])[0].get('message', {}).get('content', '')
                        messages = self._parse_ai_messages(content, count)
                    else:
                        error_data = await resp.text()
                        raise Exception(f"OpenAI è¿”å› {resp.status}: {error_data[:100]}")
            
            else:
                # é€šç”¨ OpenAI å…¼å®¹æ ¼å¼
                endpoint = api_endpoint or 'http://localhost:11434/v1'
                chat_url = f"{endpoint.rstrip('/')}/chat/completions"
                
                headers = {'Content-Type': 'application/json'}
                if api_key:
                    headers['Authorization'] = f'Bearer {api_key}'
                
                async with session.post(
                    chat_url,
                    headers=headers,
                    json={
                        'model': model_name,
                        'messages': [{'role': 'user', 'content': prompt}],
                        'max_tokens': 1000,
                        'temperature': 0.8
                    }
                ) as resp:
                    if resp.status == 200:
                        data = await resp.json()
                        content = data.get('choices', [{}])[0].get('message', {}).get('content', '')
                        messages = self._parse_ai_messages(content, count)
                    else:
                        raise Exception(f"API è¿”å› {resp.status}")
        
        return messages

    def _parse_ai_messages(self, content: str, count: int) -> List[str]:
        """è§£æ AI è¿”å›çš„æ¶ˆæ¯"""
        lines = content.strip().split('\n')
        messages = []
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            # ç§»é™¤ç·¨è™Ÿï¼ˆå¦‚ "1." æˆ– "1ã€" æˆ– "1)"ï¼‰
            line = re.sub(r'^[\d]+[\.\ã€\)\]\:]\s*', '', line)
            line = line.strip()
            if line and len(line) > 5:  # éæ¿¾å¤ªçŸ­çš„è¡Œ
                messages.append(line)
        
        return messages[:count] if messages else []

    def _get_local_message_templates(self, topic: str, style: str, count: int) -> List[str]:
        """ç²å–æœ¬åœ°æ¶ˆæ¯æ¨¡æ¿ï¼ˆå›é€€æ–¹æ¡ˆï¼‰"""
        style_templates = {
            'friendly': [
                "{greeting}ï¼æˆ‘æ˜¯åœ¨ç¾¤è£¡çœ‹åˆ°ä½ çš„ï¼Œæƒ³èªè­˜ä¸€ä¸‹~",
                "Hi {firstName}ï¼å¾ˆé«˜èˆˆèƒ½èªè­˜ä½ ï¼Œå¸Œæœ›ä»¥å¾Œå¤šå¤šäº¤æµ ğŸ˜Š",
                "{greeting}{firstName}ï¼Œæˆ‘è¦ºå¾—æˆ‘å€‘å¯èƒ½æœ‰å…±åŒè©±é¡Œï¼Œæ–¹ä¾¿èŠèŠå—ï¼Ÿ",
                "å—¨ï¼çœ‹åˆ°ä½ çš„è³‡æ–™è¦ºå¾—å¾ˆæœ‰è¶£ï¼Œæƒ³è·Ÿä½ äº¤å€‹æœ‹å‹~",
                f"{{greeting}}ï¼æˆ‘å°{topic}å¾ˆæ„Ÿèˆˆè¶£ï¼Œçœ‹åˆ°ä½ ä¹Ÿåœ¨é—œæ³¨é€™å€‹ï¼Ÿ"
            ],
            'formal': [
                "{greeting}ï¼Œå¾ˆé«˜èˆˆèªè­˜æ‚¨ã€‚æˆ‘æ³¨æ„åˆ°æˆ‘å€‘å¯èƒ½æœ‰å…±åŒçš„èˆˆè¶£é»ï¼Œä¸çŸ¥æ˜¯å¦æ–¹ä¾¿äº¤æµï¼Ÿ",
                f"æ‚¨å¥½ {{firstName}}ï¼Œå†’æ˜§æ‰“æ“¾ã€‚æˆ‘å°ˆæ³¨æ–¼{topic}é ˜åŸŸï¼Œå¸Œæœ›èƒ½èˆ‡æ‚¨å»ºç«‹è¯ç¹«ã€‚",
                "{greeting}ï¼Œæˆ‘æ˜¯é€šéç¾¤çµ„èªè­˜åˆ°æ‚¨çš„ã€‚å¦‚æœ‰åˆä½œæ©Ÿæœƒï¼ŒæœŸå¾…é€²ä¸€æ­¥æºé€šã€‚",
                "å°Šæ•¬çš„ {firstName}ï¼Œå¾ˆæ¦®å¹¸èƒ½å¤ èˆ‡æ‚¨å–å¾—è¯ç¹«ã€‚æœŸå¾…æœªä¾†æœ‰æ©Ÿæœƒåˆä½œã€‚",
                f"{{greeting}}ï¼Œæˆ‘å°{topic}å¾ˆæ„Ÿèˆˆè¶£ï¼Œçœ‹åˆ°æ‚¨ä¹Ÿåœ¨é€™å€‹é ˜åŸŸï¼Œæƒ³å‘æ‚¨è«‹æ•™ã€‚"
            ],
            'humorous': [
                "{greeting}ï¼æˆ‘ä¸æ˜¯æ¨éŠ·å“¡ï¼Œåªæ˜¯è¦ºå¾—ä½ çœ‹èµ·ä¾†å¾ˆé…·æƒ³èªè­˜ä¸€ä¸‹ ğŸ˜",
                "Hi {firstName}ï¼å‘½é‹çš„å®‰æ’è®“æˆ‘å€‘åœ¨èŒ«èŒ«ç¶²æµ·ä¸­ç›¸é‡ ğŸŒŠ",
                "{greeting}~æˆ‘ç™¼èª“æˆ‘ä¸æ˜¯æ©Ÿå™¨äººï¼Œåªæ˜¯ä¸€å€‹æƒ³äº¤æœ‹å‹çš„æ™®é€šäºº ğŸ¤–âŒ",
                "å˜¿ï¼å¦‚æœé€™æ¢æ¶ˆæ¯æ‰“æ“¾åˆ°ä½ äº†ï¼Œè«‹å‡è£æ²’çœ‹åˆ°ï¼ˆä½†å…¶å¯¦å¾ˆæœŸå¾…ä½ çš„å›å¾©ï¼‰",
                "{greeting}{firstName}ï¼äººç”Ÿä½•è™•ä¸ç›¸é€¢ï¼Œæ—¢ç„¶ç›¸é‡ä¸å¦‚åŠ å€‹å¥½å‹ï¼Ÿ"
            ],
            'concise': [
                "{greeting}ï¼Œèªè­˜ä¸€ä¸‹ï¼Ÿ",
                f"Hi {{firstName}}ï¼Œå°{topic}æœ‰èˆˆè¶£å—ï¼Ÿ",
                "{greeting}ï¼æ–¹ä¾¿èŠèŠå—ï¼Ÿ",
                "ä½ å¥½ï¼Œæƒ³è·Ÿä½ äº¤æµä¸€ä¸‹ã€‚",
                "{greeting}ï¼Œå¯ä»¥èªè­˜ä¸€ä¸‹å—ï¼Ÿ"
            ],
            'enthusiastic': [
                "{greeting}ï¼ï¼å¤ªé–‹å¿ƒèƒ½èªè­˜ä½ äº†ï¼ï¼ğŸ‰ğŸ‰ğŸ‰",
                "å“‡ï¼{firstName}ï¼çµ‚æ–¼æ‰¾åˆ°å¿—åŒé“åˆçš„æœ‹å‹äº†ï¼ï¼",
                f"{{greeting}}ï¼æˆ‘å°{topic}è¶…ç´šæœ‰ç†±æƒ…çš„ï¼Œå¸Œæœ›èƒ½è·Ÿä½ ä¸€èµ·è¨è«–ï¼ğŸ’ª",
                "å—¨å—¨å—¨ï¼{firstName}ï¼æ„Ÿè¦ºæˆ‘å€‘æœƒæˆç‚ºå¾ˆå¥½çš„æœ‹å‹ï¼âœ¨",
                f"å¤ªæ£’äº†ï¼{{greeting}}ï¼ä¸€ç›´åœ¨æ‰¾å°{topic}æ„Ÿèˆˆè¶£çš„äººï¼"
            ]
        }
        
        templates = style_templates.get(style, style_templates['friendly'])
        messages = templates[:count]
        random.shuffle(messages)
        return messages

    async def _handle_collab_group_message(self, client, message, target_group_id: str):
        """
        ğŸ†• è™•ç†ç¾¤èŠå”ä½œä¸­çš„æ¶ˆæ¯
        """
        from pyrogram.enums import ChatType
        
        try:
            # åªè™•ç†ç¾¤çµ„æ¶ˆæ¯
            if message.chat.type not in [ChatType.GROUP, ChatType.SUPERGROUP]:
                return
            
            # åªè™•ç†ç›®æ¨™ç¾¤çµ„
            if str(message.chat.id) != str(target_group_id):
                return
            
            # ç²å–å”ä½œé…ç½®
            collab = self._active_group_collabs.get(str(target_group_id))
            if not collab:
                return
            
            # ç²å–ç™¼é€è€…ä¿¡æ¯
            sender_id = message.from_user.id if message.from_user else None
            sender_name = message.from_user.first_name if message.from_user else "Unknown"
            message_text = message.text or message.caption or ""
            
            if not message_text:
                return
            
            # æª¢æŸ¥æ˜¯å¦æ˜¯è§’è‰²å¸³è™Ÿç™¼çš„æ¶ˆæ¯ï¼ˆä¸å›è¦†è‡ªå·±ï¼‰
            role_phones = [r.get('phone') for r in collab.get('roles', [])]
            for phone in role_phones:
                role_client = self.telegram_manager.clients.get(phone)
                if role_client:
                    try:
                        me = await role_client.get_me()
                        if me.id == sender_id:
                            return  # ä¸å›è¦†è‡ªå·±
                    except:
                        pass
            
            print(f"[GroupCollab] æ”¶åˆ°ç¾¤æ¶ˆæ¯: from={sender_name}, text={message_text[:50]}...", file=sys.stderr)
            
            # ğŸ”§ P2-1: é¸æ“‡åˆé©çš„è§’è‰²å›è¦†ï¼ˆé¿å…æ‰€æœ‰è§’è‰²åŒæ™‚å›è¦†ï¼‰
            responding_role = await self._select_responding_role(collab, message_text, sender_id)
            
            if not responding_role:
                print(f"[GroupCollab] ç„¡åˆé©è§’è‰²å›è¦†æ­¤æ¶ˆæ¯", file=sys.stderr)
                return
            
            # ç”Ÿæˆ AI å›è¦†
            role_phone = responding_role.get('phone')
            role_name = responding_role.get('roleName', 'åŠ©æ‰‹')
            role_prompt = responding_role.get('prompt', '')
            
            try:
                # ä½¿ç”¨ AI ç”Ÿæˆå›è¦†ï¼ˆğŸ”§ P3-1: ä½¿ç”¨æ¨¡å—çº§å»¶è¿Ÿå¯¼å…¥ï¼‰
                ai_auto_chat = _get_ai_auto_chat()
                
                # ğŸ†• P0-2: æœç´¢çŸ¥è­˜åº«ï¼Œç²å–ç›¸é—œå°ˆæ¥­å…§å®¹
                knowledge_context = ""
                matched_knowledge = []  # ğŸ†• P1-2: è¨˜éŒ„åŒ¹é…çš„çŸ¥è­˜ç”¨æ–¼å¯è¦–åŒ–
                
                try:
                    # æ–¹æ³•1: å¾ RAG ç³»çµ±æœç´¢
                    telegram_rag = _get_telegram_rag()
                    if telegram_rag:
                        rag_context = await telegram_rag.build_rag_context(
                            user_message=message_text,
                            user_id=str(sender_id),
                            max_items=3,
                            max_tokens=500
                        )
                        if rag_context:
                            knowledge_context = rag_context
                            matched_knowledge.append({
                                'source': 'RAG',
                                'content': rag_context[:100] + '...' if len(rag_context) > 100 else rag_context
                            })
                            print(f"[GroupCollab] ğŸ“š å¾ RAG æ‰¾åˆ°ç›¸é—œçŸ¥è­˜", file=sys.stderr)
                    
                    # æ–¹æ³•2: å¾çŸ¥è­˜åº«è¡¨æœç´¢ï¼ˆå‚™ç”¨ï¼‰
                    if not knowledge_context:
                        from database import db
                        knowledge_items = await db.search_knowledge(message_text, limit=3)
                        if knowledge_items:
                            kb_parts = ["ã€æ¥­å‹™çŸ¥è­˜åƒè€ƒã€‘"]
                            for item in knowledge_items:
                                kb_parts.append(f"- {item.get('title')}: {item.get('content')}")
                                # ğŸ†• P1-2: è¨˜éŒ„æ¯æ¢åŒ¹é…çš„çŸ¥è­˜
                                matched_knowledge.append({
                                    'source': 'KnowledgeBase',
                                    'id': item.get('id'),
                                    'title': item.get('title'),
                                    'content': item.get('content', '')[:80]
                                })
                            knowledge_context = "\n".join(kb_parts)
                            print(f"[GroupCollab] ğŸ“š å¾çŸ¥è­˜åº«è¡¨æ‰¾åˆ° {len(knowledge_items)} æ¢çŸ¥è­˜", file=sys.stderr)
                except Exception as kb_err:
                    print(f"[GroupCollab] çŸ¥è­˜åº«æœç´¢å¤±æ•—: {kb_err}", file=sys.stderr)
                
                # æ§‹å»ºç¾¤èŠå°ˆç”¨ promptï¼ˆåŒ…å«çŸ¥è­˜åº«å…§å®¹ï¼‰
                group_prompt = f"""ä½ æ˜¯ç¾¤çµ„ä¸­çš„ã€Œ{role_name}ã€ï¼Œæ­£åœ¨åƒèˆ‡å¤šè§’è‰²å”ä½œæœå‹™å®¢æˆ¶ã€‚

{role_prompt}

{knowledge_context}

ã€ç¾¤èŠè¦å‰‡ã€‘
1. å›è¦†ç°¡çŸ­è‡ªç„¶ï¼ˆ10-50å­—ï¼‰ï¼Œåƒç¾¤èŠä¸€æ¨£
2. ä¸è¦é‡è¤‡å…¶ä»–è§’è‰²èªªéçš„è©±
3. å¾ä½ çš„è§’è‰²è§’åº¦æä¾›åƒ¹å€¼
4. å¦‚æœçŸ¥è­˜åº«æœ‰ç›¸é—œå…§å®¹ï¼Œå„ªå…ˆåƒè€ƒçŸ¥è­˜åº«å›ç­”
5. èªæ°£è¼•é¬†ï¼Œåƒæœ‹å‹èŠå¤©
"""
                
                # ç”Ÿæˆå›è¦†
                response = await ai_auto_chat._generate_response_with_prompt(
                    user_id=str(sender_id),
                    user_message=message_text,
                    custom_prompt=group_prompt,
                    usage_type='groupChat'
                )
                
                if response:
                    # æ·»åŠ éš¨æ©Ÿå»¶é²ï¼Œæ›´è‡ªç„¶
                    delay = random.uniform(2, 8)
                    await asyncio.sleep(delay)
                    
                    # ç™¼é€å›è¦†
                    role_client = self.telegram_manager.clients.get(role_phone)
                    if role_client and role_client.is_connected:
                        await role_client.send_message(int(target_group_id), response)
                        
                        print(f"[GroupCollab] {role_name} å›è¦†: {response[:50]}...", file=sys.stderr)
                        
                        # æ›´æ–°çµ±è¨ˆ
                        collab['message_count'] = collab.get('message_count', 0) + 1
                        collab['last_responder'] = role_name
                        
                        # ç™¼é€äº‹ä»¶ï¼ˆğŸ†• P1-2: åŒ…å«çŸ¥è­˜å¼•ç”¨ä¿¡æ¯ï¼‰
                        self.send_event("group:ai-reply-sent", {
                            "groupId": target_group_id,
                            "roleName": role_name,
                            "content": response,
                            "replyTo": message_text[:50],
                            "knowledgeUsed": matched_knowledge if matched_knowledge else None,
                            "hasKnowledgeRef": len(matched_knowledge) > 0
                        })
                        
            except Exception as ai_err:
                print(f"[GroupCollab] AI å›è¦†ç”Ÿæˆå¤±æ•—: {ai_err}", file=sys.stderr)
                
        except Exception as e:
            print(f"[GroupCollab] è™•ç†ç¾¤æ¶ˆæ¯å¤±æ•—: {traceback.format_exc()}", file=sys.stderr)

    async def _select_responding_role(
        self, 
        collab: Dict[str, Any], 
        message: str, 
        sender_id: int
    ) -> Optional[Dict[str, Any]]:
        """
        ğŸ†• P2-1: é¸æ“‡åˆé©çš„è§’è‰²å›è¦†ï¼ˆé¿å…åˆ·å±ï¼‰
        """
        roles = collab.get('roles', [])
        if not roles:
            return None
        
        last_responder = collab.get('last_responder')
        
        # è¦å‰‡ï¼š
        # 1. å¦‚æœåªæœ‰ä¸€å€‹è§’è‰²ï¼Œå°±ç”¨å®ƒ
        # 2. å¦‚æœä¸Šæ¬¡æ˜¯æŸè§’è‰²å›è¦†ï¼Œé€™æ¬¡å„ªå…ˆè®“å…¶ä»–è§’è‰²å›è¦†
        # 3. æ ¹æ“šæ¶ˆæ¯å…§å®¹åŒ¹é…è§’è‰²ï¼ˆé—œéµè©ï¼‰
        # ğŸ”§ Phase 8: ç§»é™¤è·³éæ¦‚ç‡ï¼Œç¢ºä¿ç¾¤èŠå”ä½œæ™‚ä¸€å®šæœ‰å›è¦†
        # 4. ä¸å†ä½¿ç”¨éš¨æ©Ÿè·³éï¼Œæ”¹ç‚ºå»¶é²å›è¦†æ§åˆ¶é »ç‡
        
        # ğŸ”§ Phase 8: æ·»åŠ èª¿è©¦æ—¥èªŒ
        print(f"[GroupCollab] ğŸ” é¸æ“‡å›è¦†è§’è‰²: roles={len(roles)}, last_responder={last_responder}", file=sys.stderr)
        
        available_roles = roles.copy()
        
        # å„ªå…ˆè®“ä¸åŒè§’è‰²å›è¦†
        if last_responder and len(available_roles) > 1:
            available_roles = [r for r in available_roles if r.get('roleName') != last_responder]
            if not available_roles:
                available_roles = roles  # å¦‚æœéæ¿¾å¾Œæ²’æœ‰äº†ï¼Œæ¢å¾©å…¨éƒ¨
        
        # æ ¹æ“šæ¶ˆæ¯å…§å®¹åŒ¹é…è§’è‰²
        message_lower = message.lower()
        
        # ç°¡å–®çš„é—œéµè©åŒ¹é…
        keyword_role_map = {
            'åƒ¹æ ¼': ['è²»ç‡åˆ†æå¸«', 'é¡§å•'],
            'å¤šå°‘éŒ¢': ['è²»ç‡åˆ†æå¸«', 'é¡§å•'],
            'è²»ç”¨': ['è²»ç‡åˆ†æå¸«', 'é¡§å•'],
            'æ€éº¼ç”¨': ['æŠ€è¡“æ”¯æŒ', 'å®¢æœ'],
            'å¦‚ä½•': ['æŠ€è¡“æ”¯æŒ', 'å®¢æœ'],
            'å•é¡Œ': ['æŠ€è¡“æ”¯æŒ', 'å®¢æœ'],
            'å®‰å…¨': ['å®‰å…¨é¡§å•', 'é¡§å•'],
            'å¯é ': ['å®‰å…¨é¡§å•', 'é¡§å•'],
            'æ¨è–¦': ['ç†±å¿ƒç¾¤å‹', 'è€ç”¨æˆ¶'],
            'å¥½ç”¨': ['ç†±å¿ƒç¾¤å‹', 'è€ç”¨æˆ¶'],
        }
        
        matched_roles = []
        for keyword, role_names in keyword_role_map.items():
            if keyword in message_lower:
                for role in available_roles:
                    if any(name in role.get('roleName', '') for name in role_names):
                        matched_roles.append(role)
        
        if matched_roles:
            return random.choice(matched_roles)
        
        # æ²’æœ‰åŒ¹é…çš„ï¼Œéš¨æ©Ÿé¸ä¸€å€‹
        return random.choice(available_roles) if available_roles else None

    async def _call_local_ai(self, endpoint: str, model: str, system_prompt: str, user_message: str) -> str:
        """ç›´æ¥èª¿ç”¨æœ¬åœ°/é ç¨‹ AI API"""
        import aiohttp
        import socket
        from urllib.parse import urlparse
        
        print(f"[AI] _call_local_ai called with endpoint: {endpoint}, model: {model}", file=sys.stderr)
        
        # é¦–å…ˆé€²è¡Œé€£æ¥è¨ºæ–·
        try:
            parsed = urlparse(endpoint)
            host = parsed.hostname
            port = parsed.port or (443 if parsed.scheme == 'https' else 80)
            
            print(f"[AI] Diagnosing connection to {host}:{port}...", file=sys.stderr)
            
            # æ¸¬è©¦ TCP é€£æ¥
            try:
                sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
                sock.settimeout(5)
                result = sock.connect_ex((host, port))
                sock.close()
                
                if result == 0:
                    print(f"[AI] âœ“ TCP connection to {host}:{port} successful", file=sys.stderr)
                else:
                    print(f"[AI] âœ— TCP connection to {host}:{port} failed (error code: {result})", file=sys.stderr)
                    raise Exception(f"ç„¡æ³•é€£æ¥åˆ° AI æœå‹™ {host}:{port}ã€‚è«‹æª¢æŸ¥ï¼š\n1. AI æœå‹™æ˜¯å¦æ­£åœ¨é‹è¡Œ\n2. é˜²ç«ç‰†æ˜¯å¦å…è¨±é€£æ¥\n3. ç¶²çµ¡æ˜¯å¦æ­£å¸¸")
            except socket.gaierror as e:
                print(f"[AI] âœ— DNS resolution failed for {host}: {e}", file=sys.stderr)
                raise Exception(f"ç„¡æ³•è§£æä¸»æ©Ÿå {host}ã€‚è«‹æª¢æŸ¥ç¶²çµ¡è¨­ç½®æˆ– DNS é…ç½®")
            except socket.timeout:
                print(f"[AI] âœ— Connection timeout to {host}:{port}", file=sys.stderr)
                raise Exception(f"é€£æ¥ {host}:{port} è¶…æ™‚ã€‚è«‹æª¢æŸ¥ï¼š\n1. AI æœå‹™æ˜¯å¦æ­£åœ¨é‹è¡Œ\n2. é˜²ç«ç‰†æ˜¯å¦é˜»å¡äº†é€£æ¥\n3. ç¶²çµ¡è·¯ç”±æ˜¯å¦æ­£ç¢º")
            except Exception as e:
                print(f"[AI] âœ— Connection test failed: {e}", file=sys.stderr)
                raise Exception(f"é€£æ¥æ¸¬è©¦å¤±æ•—: {str(e)}")
        except Exception as diag_error:
            # è¨ºæ–·å¤±æ•—ï¼Œä½†ç¹¼çºŒå˜—è©¦å¯¦éš›è«‹æ±‚ï¼ˆå¯èƒ½è¨ºæ–·æœ‰èª¤ï¼‰
            print(f"[AI] Connection diagnosis failed, but continuing: {diag_error}", file=sys.stderr)
        
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": user_message})
        
        # å˜—è©¦ OpenAI å…¼å®¹æ ¼å¼
        request_body = {
            "messages": messages,
            "max_tokens": 500,
            "temperature": 0.7
        }
        if model:
            request_body["model"] = model
        
        # å¢åŠ è¶…æ™‚æ™‚é–“åˆ° 90 ç§’ï¼ˆAI ç”Ÿæˆå¯èƒ½éœ€è¦æ›´é•·æ™‚é–“ï¼‰
        timeout = aiohttp.ClientTimeout(total=90, connect=10)
        
        try:
            start_time = time.time()
            async with aiohttp.ClientSession() as session:
                # å˜—è©¦ /v1/chat/completions ç«¯é»
                chat_url = endpoint.rstrip('/')
                if not chat_url.endswith('/v1/chat/completions'):
                    chat_url = chat_url.rstrip('/') + '/v1/chat/completions'
                
                print(f"[AI] Attempting to call AI endpoint: {chat_url}", file=sys.stderr)
                print(f"[AI] Request body: model={model}, messages={len(messages)}, max_tokens=500", file=sys.stderr)
                
                try:
                    request_start = time.time()
                    async with session.post(chat_url, json=request_body, timeout=timeout) as resp:
                        connect_time = time.time() - request_start
                        print(f"[AI] Connection established in {connect_time:.2f}s, status: {resp.status}", file=sys.stderr)
                        
                        if resp.status == 200:
                            data_start = time.time()
                            data = await resp.json()
                            data_time = time.time() - data_start
                            total_time = time.time() - start_time
                            
                            print(f"[AI] Response received in {data_time:.2f}s, total time: {total_time:.2f}s", file=sys.stderr)
                            
                            if 'choices' in data and len(data['choices']) > 0:
                                content = data['choices'][0].get('message', {}).get('content', '')
                                print(f"[AI] âœ“ Successfully generated response (length: {len(content)})", file=sys.stderr)
                                return content
                            else:
                                print(f"[AI] âœ— Response missing 'choices' field. Full response: {data}", file=sys.stderr)
                                raise Exception(f"AI æœå‹™è¿”å›äº†ç„¡æ•ˆçš„éŸ¿æ‡‰æ ¼å¼: {list(data.keys())}")
                        else:
                            error_text = await resp.text()
                            print(f"[AI] âœ— Error response (status {resp.status}): {error_text[:500]}", file=sys.stderr)
                            raise Exception(f"AI æœå‹™è¿”å›éŒ¯èª¤ (HTTP {resp.status}): {error_text[:200]}")
                            
                except asyncio.TimeoutError:
                    elapsed = time.time() - start_time
                    print(f"[AI] âœ— Request timeout after {elapsed:.2f}s for endpoint: {chat_url}", file=sys.stderr)
                    raise Exception(f"AI æœå‹™éŸ¿æ‡‰è¶…æ™‚ï¼ˆ{elapsed:.1f}ç§’ï¼‰ã€‚å¯èƒ½åŸå› ï¼š\n1. AI æœå‹™éŸ¿æ‡‰éæ…¢\n2. ç¶²çµ¡å»¶é²éé«˜\n3. æ¨¡å‹åŠ è¼‰ä¸­\nè«‹æª¢æŸ¥ AI æœå‹™ç‹€æ…‹")
                except aiohttp.ClientConnectorError as e:
                    elapsed = time.time() - start_time
                    print(f"[AI] âœ— Connection error after {elapsed:.2f}s: {e}", file=sys.stderr)
                    raise Exception(f"ç„¡æ³•é€£æ¥åˆ° AI æœå‹™ ({host}:{port})ã€‚è«‹æª¢æŸ¥ï¼š\n1. AI æœå‹™æ˜¯å¦æ­£åœ¨é‹è¡Œ\n2. é˜²ç«ç‰†æ˜¯å¦å…è¨±é€£æ¥\n3. ç«¯é»åœ°å€æ˜¯å¦æ­£ç¢º")
                except aiohttp.ClientError as e:
                    elapsed = time.time() - start_time
                    print(f"[AI] âœ— Client error after {elapsed:.2f}s: {e}", file=sys.stderr)
                    # å¦‚æœ /v1/chat/completions å¤±æ•—ï¼Œå˜—è©¦ç›´æ¥ç«¯é»
                    if chat_url != endpoint:
                        print(f"[AI] Trying direct endpoint: {endpoint}", file=sys.stderr)
                        try:
                            async with session.post(endpoint, json=request_body, timeout=timeout) as resp2:
                                if resp2.status == 200:
                                    data = await resp2.json()
                                    # è™•ç†å„ç¨®éŸ¿æ‡‰æ ¼å¼
                                    if 'choices' in data:
                                        return data['choices'][0].get('message', {}).get('content', '')
                                    elif 'response' in data:
                                        return data['response']
                                    elif 'content' in data:
                                        return data['content']
                                    elif 'text' in data:
                                        return data['text']
                                else:
                                    error_text = await resp2.text()
                                    print(f"[AI] Direct endpoint error (status {resp2.status}): {error_text[:200]}", file=sys.stderr)
                        except Exception as e2:
                            print(f"[AI] Direct endpoint also failed: {e2}", file=sys.stderr)
                    raise Exception(f"ç¶²çµ¡éŒ¯èª¤: {str(e)}")
                    
        except asyncio.TimeoutError:
            raise Exception("AI æœå‹™éŸ¿æ‡‰è¶…æ™‚ï¼Œè«‹æª¢æŸ¥æœå‹™é€£æ¥æˆ–å¢åŠ è¶…æ™‚æ™‚é–“")
        except aiohttp.ClientError as e:
            error_msg = str(e)
            print(f"[AI] Network error: {error_msg}", file=sys.stderr)
            raise Exception(f"ç„¡æ³•é€£æ¥åˆ° AI æœå‹™ ({endpoint}): {error_msg}")
        except Exception as e:
            error_details = traceback.format_exc()
            print(f"[AI] Unexpected error: {error_details}", file=sys.stderr)
            raise

    async def _execute_ai_group_search(self, strategy: Dict[str, Any]):
        """ç•°æ­¥åŸ·è¡Œç¾¤çµ„æœç´¢"""
        try:
            keywords = strategy.get('keywords', {})
            search_keywords = keywords.get('highIntent', [])[:5]  # ä½¿ç”¨å‰5å€‹é«˜æ„å‘é—œéµè©æœç´¢
            
            total_found = 0
            for keyword in search_keywords:
                self.send_event("ai-execution-status", {
                    "isExecuting": True,
                    "phase": "searching",
                    "message": f"æ­£åœ¨æœç´¢é—œéµè©: {keyword}..."
                })
                
                # èª¿ç”¨ç¾¤çµ„æœç´¢æœå‹™ï¼ˆğŸ”§ P3-1: å»¶è¿Ÿå¯¼å…¥ï¼‰
                try:
                    group_search_service = _get_group_search_service()
                    results = await group_search_service.search_groups(keyword, limit=10) if group_search_service else []
                    total_found += len(results) if results else 0
                    
                    self.send_event("ai-execution-stats", {
                        "groupsSearched": total_found,
                        "groupsJoined": 0,
                        "membersScanned": 0,
                        "leadsFound": 0,
                        "messagesSent": 0,
                        "responses": 0
                    })
                    
                    await asyncio.sleep(2)  # é¿å…é »ç¹è«‹æ±‚
                except Exception as search_error:
                    print(f"[AI Strategy] Search error for {keyword}: {search_error}", file=sys.stderr)
            
            self.send_event("ai-execution-status", {
                "isExecuting": True,
                "phase": "search_complete",
                "message": f"æœç´¢å®Œæˆï¼Œå…±ç™¼ç¾ {total_found} å€‹ç›¸é—œç¾¤çµ„"
            })
            
        except Exception as e:
            print(f"[AI Strategy] Group search failed: {e}", file=sys.stderr)
            self.send_event("ai-execution-status", {
                "isExecuting": False,
                "phase": "error",
                "message": f"æœç´¢å¤±æ•—: {str(e)}"
            })

    def _parse_ai_knowledge_response(self, response: str) -> list:
        """è§£æ AI ç”Ÿæˆçš„çŸ¥è­˜éŸ¿æ‡‰"""
        try:
            # å˜—è©¦ç›´æ¥è§£æ JSON
            if '{' in response and '}' in response:
                # æå– JSON éƒ¨åˆ†
                json_match = re.search(r'\{[\s\S]*\}', response)
                if json_match:
                    data = json.loads(json_match.group())
                    return data.get('items', [])
        except json.JSONDecodeError:
            pass
        
        # å¦‚æœè§£æå¤±æ•—ï¼Œå˜—è©¦æŒ‰è¡Œè§£æ
        items = []
        lines = response.split('\n')
        current_category = 'custom'
        
        for line in lines:
            line = line.strip()
            if 'ã€ç”¢å“çŸ¥è­˜ã€‘' in line or 'ã€äº§å“çŸ¥è¯†ã€‘' in line:
                current_category = 'product'
            elif 'ã€å¸¸è¦‹å•ç­”ã€‘' in line or 'ã€å¸¸è§é—®ç­”ã€‘' in line:
                current_category = 'faq'
            elif 'ã€éŠ·å”®è©±è¡“ã€‘' in line or 'ã€é”€å”®è¯æœ¯ã€‘' in line:
                current_category = 'sales'
            elif 'ã€ç•°è­°è™•ç†ã€‘' in line or 'ã€å¼‚è®®å¤„ç†ã€‘' in line:
                current_category = 'objection'
            elif line and not line.startswith('#') and len(line) > 10:
                items.append({
                    'category': current_category,
                    'title': line[:50],
                    'content': line
                })
        
        return items[:20]  # é™åˆ¶æœ€å¤š 20 æ¢

    def _generate_default_knowledge(self, business_desc: str) -> str:
        """ç”Ÿæˆé»˜èªçŸ¥è­˜æ¨¡æ¿"""
        return f'''{{
  "items": [
    {{"category": "product", "title": "æœå‹™ä»‹ç´¹", "content": "æˆ‘å€‘æä¾› {business_desc} ç›¸é—œæœå‹™ï¼Œè‡´åŠ›æ–¼ç‚ºå®¢æˆ¶æä¾›å°ˆæ¥­ã€é«˜æ•ˆçš„è§£æ±ºæ–¹æ¡ˆã€‚"}},
    {{"category": "product", "title": "æœå‹™å„ªå‹¢", "content": "æˆ‘å€‘æ“æœ‰å°ˆæ¥­åœ˜éšŠã€è±å¯Œç¶“é©—ï¼Œç¢ºä¿æœå‹™è³ªé‡å’Œå®¢æˆ¶æ»¿æ„åº¦ã€‚"}},
    {{"category": "faq", "title": "Q: å¦‚ä½•é–‹å§‹ä½¿ç”¨ï¼Ÿ", "content": "A: æ‚¨å¯ä»¥ç›´æ¥è¯ç¹«æˆ‘å€‘çš„å®¢æœï¼Œæˆ‘å€‘æœƒç‚ºæ‚¨è©³ç´°ä»‹ç´¹æµç¨‹ã€‚"}},
    {{"category": "faq", "title": "Q: æœå‹™è²»ç”¨å¦‚ä½•ï¼Ÿ", "content": "A: æˆ‘å€‘æä¾›å…·æœ‰ç«¶çˆ­åŠ›çš„åƒ¹æ ¼ï¼Œå…·é«”è²»ç”¨æ ¹æ“šæ‚¨çš„éœ€æ±‚è€Œå®šã€‚"}},
    {{"category": "sales", "title": "é–‹å ´è©±è¡“", "content": "æ‚¨å¥½ï¼å¾ˆé«˜èˆˆç‚ºæ‚¨æœå‹™ã€‚è«‹å•æœ‰ä»€éº¼å¯ä»¥å¹«åŠ©æ‚¨çš„ï¼Ÿ"}},
    {{"category": "sales", "title": "å„ªå‹¢ä»‹ç´¹", "content": "æˆ‘å€‘çš„æœå‹™å·²ç¶“å¹«åŠ©çœ¾å¤šå®¢æˆ¶è§£æ±ºå•é¡Œï¼Œæ‚¨å¯ä»¥æ”¾å¿ƒé¸æ“‡ã€‚"}},
    {{"category": "objection", "title": "åƒ¹æ ¼ç•°è­°", "content": "æˆ‘ç†è§£æ‚¨å°åƒ¹æ ¼çš„é—œæ³¨ã€‚æˆ‘å€‘çš„åƒ¹æ ¼æ˜¯åŸºæ–¼å„ªè³ªæœå‹™åˆ¶å®šçš„ï¼Œæ‚¨å¯ä»¥å…ˆé«”é©—ä¸€ä¸‹ã€‚"}},
    {{"category": "objection", "title": "ä¿¡ä»»ç•°è­°", "content": "æˆ‘å€‘å·²ç¶“æœå‹™å¤šå¹´ï¼Œæœ‰å¤§é‡æˆåŠŸæ¡ˆä¾‹ï¼Œæ‚¨å¯ä»¥æŸ¥çœ‹æˆ‘å€‘çš„å®¢æˆ¶è©•åƒ¹ã€‚"}}
  ]
}}'''

    def _parse_rag_knowledge_response(self, response: str) -> list:
        """è§£æ AI ç”Ÿæˆçš„çŸ¥è­˜ JSON"""
        # ğŸ”§ P0 ä¿®å¾©ï¼šç©ºå€¼æª¢æŸ¥ï¼Œé¿å… NoneType éŒ¯èª¤
        if not response:
            print("[RAG] âš ï¸ AI å›æ‡‰ç‚ºç©ºï¼Œè·³éè§£æ", file=sys.stderr)
            return []
        
        try:
            # å˜—è©¦æå– JSON
            json_match = re.search(r'\{[\s\S]*\}', response)
            if json_match:
                data = json.loads(json_match.group())
                items = data.get('items', [])
                if items:
                    print(f"[RAG] âœ“ JSON è§£ææˆåŠŸï¼Œç²å– {len(items)} æ¢çŸ¥è­˜", file=sys.stderr)
                    return items
        except Exception as json_err:
            print(f"[RAG] JSON è§£æå¤±æ•—: {json_err}", file=sys.stderr)
        
        # é™ç´šï¼šæŒ‰è¡Œè§£æ
        items = []
        try:
            lines = response.strip().split('\n')
            current_q = None
            
            for line in lines:
                line = line.strip()
                if line.startswith('Q:') or line.startswith('å•:'):
                    current_q = line[2:].strip()
                elif line.startswith('A:') or line.startswith('ç­”:'):
                    if current_q:
                        items.append({
                            'question': current_q,
                            'answer': line[2:].strip()
                        })
                        current_q = None
            
            if items:
                print(f"[RAG] âœ“ è¡Œè§£ææˆåŠŸï¼Œç²å– {len(items)} æ¢çŸ¥è­˜", file=sys.stderr)
        except Exception as line_err:
            print(f"[RAG] è¡Œè§£æå¤±æ•—: {line_err}", file=sys.stderr)
        
        # ğŸ”§ P0 ä¿®å¾©ï¼šæœ€çµ‚å®¹éŒ¯ - å°‡æ•´å€‹å›æ‡‰ä½œç‚ºä¸€æ¢çŸ¥è­˜
        if not items and response.strip():
            print(f"[RAG] ä½¿ç”¨å®¹éŒ¯æ¨¡å¼ï¼Œå°‡å›æ‡‰ä½œç‚ºå–®æ¢çŸ¥è­˜", file=sys.stderr)
            # å˜—è©¦æå–ç¬¬ä¸€è¡Œä½œç‚ºå•é¡Œï¼Œå…¶é¤˜ä½œç‚ºç­”æ¡ˆ
            lines = response.strip().split('\n')
            if len(lines) >= 2:
                items.append({
                    'question': lines[0][:100],  # å–å‰100å­—ä½œç‚ºå•é¡Œ
                    'answer': '\n'.join(lines[1:])[:500]  # å–å¾ŒçºŒå…§å®¹ä½œç‚ºç­”æ¡ˆ
                })
            else:
                items.append({
                    'question': 'æ¥­å‹™çŸ¥è­˜',
                    'answer': response.strip()[:500]
                })
        
        return items

    def _parse_document_to_knowledge(self, document: str) -> list:
        """
        ğŸ†• P1-1: ç›´æ¥è§£ææ–‡æª”å…§å®¹ç‚ºçµæ§‹åŒ–çŸ¥è­˜ï¼ˆğŸ†• P0-3: æ™ºèƒ½åˆ†é¡ï¼‰
        
        æ”¯æŒè§£ææ ¼å¼ï¼š
        - ã€æ¨™é¡Œã€‘ï¼šå…§å®¹
        - æ¨™é¡Œï¼šå…§å®¹
        - æ•¸å­—. å…§å®¹
        - å•ç­”æ ¼å¼
        
        è‡ªå‹•åˆ†é¡ï¼š
        - product: ç”¢å“ç›¸é—œ
        - price: åƒ¹æ ¼/è²»ç‡ç›¸é—œ
        - process: æµç¨‹/æ“ä½œç›¸é—œ
        - faq: å¸¸è¦‹å•ç­”
        - resource: è³‡æºé€£çµ
        """
        if not document or len(document.strip()) < 10:
            return []
        
        items = []
        lines = document.strip().split('\n')
        
        # ğŸ†• P0-3: åˆ†é¡é—œéµè©æ˜ å°„
        category_keywords = {
            'price': ['åƒ¹æ ¼', 'è²»ç‡', 'è²»ç”¨', 'é‡‘é¡', 'æˆæœ¬', 'æ”¶è²»', 'çµç®—', 'æ‰‹çºŒè²»', 'ä½£é‡‘', 'è¿”é»', 'D0', 'D1', 'T+'],
            'product': ['ç”¢å“', 'é€šé“', 'åŠŸèƒ½', 'æœå‹™', 'æ”¯ä»˜', 'æ”¶æ¬¾', 'ä»£ä»˜', 'H5', 'å¾®ä¿¡', 'æ”¯ä»˜å¯¶', 'USDT'],
            'process': ['æµç¨‹', 'æ­¥é©Ÿ', 'å¦‚ä½•', 'æ€éº¼', 'å°æ¥', 'æ¥å…¥', 'ä½¿ç”¨', 'æ“ä½œ', 'é–‹æˆ¶', 'ç”³è«‹'],
            'faq': ['å•', 'ç­”', 'Q:', 'A:', 'æ˜¯å¦', 'å¯ä»¥', 'æ”¯æŒ', 'èƒ½ä¸èƒ½'],
            'resource': ['ç¾¤çµ„', 'é »é“', 'å®˜ç¶²', 'ç¶²å€', 'http', 't.me', 'è¦–é »', 'æ•™ç¨‹', 'é€£çµ', 'éˆæ¥']
        }
        
        def classify_content(title: str, content: str) -> str:
            """æ ¹æ“šå…§å®¹è‡ªå‹•åˆ†é¡"""
            combined = (title + ' ' + content).lower()
            
            # æŒ‰å„ªå…ˆç´šåŒ¹é…
            for category, keywords in category_keywords.items():
                for kw in keywords:
                    if kw.lower() in combined:
                        return category
            
            return 'product'  # é»˜èªç‚ºç”¢å“çŸ¥è­˜
        
        # æ¨¡å¼1: è§£æã€ã€‘æ ¼å¼çš„çµæ§‹åŒ–å…§å®¹
        bracket_pattern = re.compile(r'ã€(.+?)ã€‘[ï¼š:]\s*(.+)')
        
        # æ¨¡å¼2: è§£æã€Œæ¨™é¡Œï¼šå…§å®¹ã€æ ¼å¼
        colon_pattern = re.compile(r'^([^ï¼š:]{2,15})[ï¼š:]\s*(.+)$')
        
        # æ¨¡å¼3: è§£æã€Œæ•¸å­—. å…§å®¹ã€æ ¼å¼
        number_pattern = re.compile(r'^\d+[\.ã€]\s*(.+)$')
        
        current_section = None
        section_content = []
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # å˜—è©¦åŒ¹é…ã€ã€‘æ ¼å¼
            bracket_match = bracket_pattern.match(line)
            if bracket_match:
                # ä¿å­˜ä¹‹å‰çš„ section
                if current_section and section_content:
                    answer = '\n'.join(section_content)
                    items.append({
                        'question': f"{current_section}æ˜¯ä»€éº¼ï¼Ÿ",
                        'answer': answer,
                        'context': document[:200],
                        'category': classify_content(current_section, answer)  # ğŸ†• è‡ªå‹•åˆ†é¡
                    })
                
                title = bracket_match.group(1).strip()
                content = bracket_match.group(2).strip()
                
                # ç›´æ¥ä½œç‚ºçŸ¥è­˜é»
                if len(content) > 5:
                    items.append({
                        'question': f"{title}æ˜¯å¤šå°‘ï¼Ÿ" if any(c.isdigit() for c in content) else f"{title}æ˜¯ä»€éº¼ï¼Ÿ",
                        'answer': content,
                        'context': document[:200],
                        'category': classify_content(title, content)  # ğŸ†• è‡ªå‹•åˆ†é¡
                    })
                
                current_section = title
                section_content = [content] if content else []
                continue
            
            # å˜—è©¦åŒ¹é…ã€Œæ¨™é¡Œï¼šå…§å®¹ã€æ ¼å¼
            colon_match = colon_pattern.match(line)
            if colon_match:
                title = colon_match.group(1).strip()
                content = colon_match.group(2).strip()
                
                # éæ¿¾å¸¸è¦‹çš„éçŸ¥è­˜æ¨™é¡Œ
                skip_titles = ['ç¾¤çµ„', 'é »é“', 'å®˜ç¶²', 'è¦–é »', 'ç¶²å€', 'http']
                if not any(skip in title for skip in skip_titles) and len(content) > 3:
                    # åˆ¤æ–·å•é¡Œé¡å‹
                    if any(c.isdigit() for c in content):
                        question = f"{title}æ˜¯å¤šå°‘ï¼Ÿ"
                    elif '~' in content or '-' in content or 'åˆ°' in content:
                        question = f"{title}ç¯„åœæ˜¯å¤šå°‘ï¼Ÿ"
                    else:
                        question = f"{title}æ˜¯ä»€éº¼ï¼Ÿ"
                    
                    items.append({
                        'question': question,
                        'answer': content,
                        'context': document[:200],
                        'category': classify_content(title, content)  # ğŸ†• è‡ªå‹•åˆ†é¡
                    })
                continue
            
            # æ”¶é›†ç•¶å‰ section çš„å…§å®¹
            if current_section:
                section_content.append(line)
        
        # è™•ç†æœ€å¾Œä¸€å€‹ section
        if current_section and section_content:
            answer = '\n'.join(section_content)
            items.append({
                'question': f"{current_section}æ˜¯ä»€éº¼ï¼Ÿ",
                'answer': answer,
                'context': document[:200],
                'category': classify_content(current_section, answer)  # ğŸ†• è‡ªå‹•åˆ†é¡
            })
        
        # ğŸ”§ é¡å¤–ï¼šæå– URL ä½œç‚ºè³‡æºçŸ¥è­˜
        url_pattern = re.compile(r'(https?://[^\s]+)')
        urls = url_pattern.findall(document)
        if urls:
            items.append({
                'question': 'æœ‰å“ªäº›ç›¸é—œé€£çµå’Œè³‡æºï¼Ÿ',
                'answer': '\n'.join(urls),
                'context': 'ç›¸é—œè³‡æºé€£çµ',
                'category': 'resource'  # ğŸ†• è³‡æºåˆ†é¡
            })
        
        # ğŸ†• P0-3: æ‰“å°åˆ†é¡çµ±è¨ˆ
        category_stats = {}
        for item in items:
            cat = item.get('category', 'unknown')
            category_stats[cat] = category_stats.get(cat, 0) + 1
        
        print(f"[RAG] ğŸ“„ æ–‡æª”è§£æå®Œæˆ: {len(items)} æ¢çŸ¥è­˜", file=sys.stderr)
        print(f"[RAG] ğŸ“Š åˆ†é¡çµ±è¨ˆ: {category_stats}", file=sys.stderr)
        return items
    
    # ==================== ğŸ†• P1-2: å°å…¥é è¦½ç¢ºèªæµç¨‹ ====================
    
    # è‡¨æ™‚å­˜å„²é è¦½çš„çŸ¥è­˜ï¼ˆç”¨æ–¼ç¢ºèªå°å…¥ï¼‰
    _pending_import_items: Dict[str, list] = {}

    def _get_advantages_by_industry(self, industry: str) -> list:
        """æ ¹æ“šè¡Œæ¥­è¿”å›å„ªå‹¢é¸é …"""
        common = [
            {'id': 'fast', 'label': 'âš¡ é€Ÿåº¦å¿«'},
            {'id': 'cheap', 'label': 'ğŸ’° åƒ¹æ ¼ä½'},
            {'id': 'safe', 'label': 'ğŸ”’ å®‰å…¨å¯é '},
            {'id': '24h', 'label': 'ğŸ• 24å°æ™‚æœå‹™'}
        ]
        
        industry_specific = {
            'payment': [
                {'id': 'high_rate', 'label': 'ğŸ“ˆ åŒ¯ç‡é«˜'},
                {'id': 'multi_channel', 'label': 'ğŸ’³ å¤šç¨®æ”¶ä»˜æ–¹å¼'}
            ],
            'ecommerce': [
                {'id': 'quality', 'label': 'âœ¨ å“è³ªä¿è­‰'},
                {'id': 'return', 'label': 'ğŸ”„ ä¸ƒå¤©é€€æ›'}
            ],
            'education': [
                {'id': 'expert', 'label': 'ğŸ‘¨â€ğŸ« å°ˆå®¶æˆèª²'},
                {'id': 'lifetime', 'label': 'â™¾ï¸ æ°¸ä¹…æœ‰æ•ˆ'}
            ]
        }
        
        return common + industry_specific.get(industry, [])

    def _get_faq_suggestions(self, industry: str) -> list:
        """æ ¹æ“šè¡Œæ¥­è¿”å›å¸¸è¦‹å•é¡Œå»ºè­°"""
        suggestions = {
            'payment': ['å¤šä¹…åˆ°è³¬ï¼Ÿ', 'åŒ¯ç‡æ€éº¼ç®—ï¼Ÿ', 'æ‰‹çºŒè²»å¤šå°‘ï¼Ÿ', 'æœ€ä½é‡‘é¡æ˜¯å¤šå°‘ï¼Ÿ', 'å®‰å…¨å—ï¼Ÿ'],
            'ecommerce': ['æ€éº¼ä¸‹å–®ï¼Ÿ', 'å¤šä¹…ç™¼è²¨ï¼Ÿ', 'å¯ä»¥é€€æ›å—ï¼Ÿ', 'æœ‰ç™¼ç¥¨å—ï¼Ÿ'],
            'education': ['èª²ç¨‹å¤šä¹…ï¼Ÿ', 'å¯ä»¥è©¦è½å—ï¼Ÿ', 'æœ‰è­‰æ›¸å—ï¼Ÿ', 'å¯ä»¥é€€æ¬¾å—ï¼Ÿ'],
            'finance': ['æ”¶ç›Šç‡å¤šå°‘ï¼Ÿ', 'é¢¨éšªå¤§å—ï¼Ÿ', 'éš¨æ™‚å¯å–å—ï¼Ÿ'],
            'service': ['æ€éº¼æ”¶è²»ï¼Ÿ', 'æœå‹™ç¯„åœæ˜¯ï¼Ÿ', 'æœ‰ä¿éšœå—ï¼Ÿ']
        }
        return suggestions.get(industry, ['æ€éº¼è³¼è²·ï¼Ÿ', 'åƒ¹æ ¼æ˜¯å¤šå°‘ï¼Ÿ', 'æœ‰å”®å¾Œå—ï¼Ÿ'])

    async def _generate_knowledge_from_guided_answers(self, answers: dict):
        """æ ¹æ“šå¼•å°å¼å•ç­”çš„ç­”æ¡ˆç”ŸæˆçŸ¥è­˜ï¼ˆğŸ”§ P3-1: æ¨¡å—çº§å»¶è¿Ÿå¯¼å…¥ï¼‰"""
        telegram_rag = _get_telegram_rag()
        KnowledgeType = _get_KnowledgeType()
        ai_auto_chat = _get_ai_auto_chat()
        
        try:
            industry = answers.get('step1', 'other')
            advantages = answers.get('step2', [])
            products = answers.get('step3', '')
            faqs = answers.get('step4', '')
            style = answers.get('step5', 'friendly')
            
            total_items = 0
            
            # ç™¼é€é€²åº¦
            self.send_event("rag-build-progress", {
                "progress": {"step": 1, "totalSteps": 4, "currentAction": "åˆ†ææ¥­å‹™ä¿¡æ¯...", "itemsGenerated": 0}
            })
            
            # 1. ä½¿ç”¨ AI ç”Ÿæˆç”¢å“çŸ¥è­˜
            if products and ai_auto_chat:
                prompt = f"""æ ¹æ“šä»¥ä¸‹æ¥­å‹™æè¿°ï¼Œç”Ÿæˆ 5 æ¢ç”¢å“çŸ¥è­˜ï¼ˆJSON æ ¼å¼ï¼‰:

æ¥­å‹™é¡å‹: {industry}
ç”¢å“æè¿°: {products}
å„ªå‹¢: {', '.join(advantages) if isinstance(advantages, list) else advantages}

è«‹è¿”å› JSON: {{"items": [{{"type": "product", "question": "...", "answer": "..."}}]}}"""
                
                response = await ai_auto_chat._generate_response_with_prompt(
                    user_id="system",
                    user_message=prompt,
                    custom_prompt=f"ä½ æ˜¯å°ˆæ¥­çš„çŸ¥è­˜åº«ç”ŸæˆåŠ©æ‰‹ã€‚è«‹ç”¨ç¹é«”ä¸­æ–‡ï¼Œé¢¨æ ¼: {style}",
                    usage_type="knowledge"
                )
                
                items = self._parse_rag_knowledge_response(response)
                for item in items:
                    await telegram_rag.add_manual_knowledge(
                        knowledge_type=KnowledgeType.PRODUCT,
                        question=item.get('question', ''),
                        answer=item.get('answer', '')
                    )
                    total_items += 1
            
            self.send_event("rag-build-progress", {
                "progress": {"step": 2, "totalSteps": 4, "currentAction": "ç”Ÿæˆå¸¸è¦‹å•ç­”...", "itemsGenerated": total_items}
            })
            
            # 2. æ ¹æ“šç”¨æˆ¶æä¾›çš„ FAQ ç”Ÿæˆç­”æ¡ˆ
            if faqs:
                faq_list = [q.strip() for q in faqs.split('\n') if q.strip()]
                for faq in faq_list[:10]:
                    if ai_auto_chat:
                        answer = await ai_auto_chat._generate_response_with_prompt(
                            user_id="system",
                            user_message=f"æ¥­å‹™ï¼š{products[:200]}\n\nå•é¡Œï¼š{faq}\n\nè«‹çµ¦å‡ºå°ˆæ¥­å›ç­”ã€‚",
                            custom_prompt=f"ä½ æ˜¯å°ˆæ¥­å®¢æœï¼Œé¢¨æ ¼: {style}ã€‚è«‹ç”¨ç¹é«”ä¸­æ–‡ç°¡æ½”å›ç­”ã€‚",
                            usage_type="knowledge"
                        )
                    else:
                        answer = f"é—œæ–¼æ‚¨è©¢å•çš„ã€Œ{faq}ã€ï¼Œæˆ‘å€‘çš„å›ç­”æ˜¯..."
                    
                    await telegram_rag.add_manual_knowledge(
                        knowledge_type=KnowledgeType.FAQ,
                        question=faq,
                        answer=answer
                    )
                    total_items += 1
            
            self.send_event("rag-build-progress", {
                "progress": {"step": 3, "totalSteps": 4, "currentAction": "ç”ŸæˆéŠ·å”®è©±è¡“...", "itemsGenerated": total_items}
            })
            
            # 3. ç”ŸæˆéŠ·å”®è©±è¡“
            if ai_auto_chat:
                script_prompt = f"""æ ¹æ“šä»¥ä¸‹ä¿¡æ¯ï¼Œç”Ÿæˆ 5 æ¢éŠ·å”®è©±è¡“:

æ¥­å‹™: {products[:200]}
å„ªå‹¢: {', '.join(advantages) if isinstance(advantages, list) else advantages}

è«‹è¿”å› JSON: {{"items": [{{"type": "script", "question": "å ´æ™¯", "answer": "è©±è¡“"}}]}}"""
                
                script_response = await ai_auto_chat._generate_response_with_prompt(
                    user_id="system",
                    user_message=script_prompt,
                    custom_prompt=f"ä½ æ˜¯éŠ·å”®è©±è¡“å°ˆå®¶ã€‚é¢¨æ ¼: {style}",
                    usage_type="knowledge"
                )
                
                script_items = self._parse_rag_knowledge_response(script_response)
                for item in script_items:
                    await telegram_rag.add_manual_knowledge(
                        knowledge_type=KnowledgeType.SCRIPT,
                        question=item.get('question', ''),
                        answer=item.get('answer', '')
                    )
                    total_items += 1
            
            self.send_event("rag-build-progress", {
                "progress": {"step": 4, "totalSteps": 4, "currentAction": "å®Œæˆï¼", "itemsGenerated": total_items}
            })
            
            # å®Œæˆ
            self.send_event("rag-build-complete", {
                "success": True,
                "totalItems": total_items,
                "industry": industry
            })
            self.send_log(f"ğŸ§  å¼•å°å¼æ§‹å»ºå®Œæˆï¼Œå…± {total_items} æ¢çŸ¥è­˜", "success")
            
        except Exception as e:
            traceback.print_exc(file=sys.stderr)
            self.send_event("rag-build-complete", {
                "success": False,
                "error": str(e)
            })

