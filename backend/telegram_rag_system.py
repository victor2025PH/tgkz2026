"""
Telegram RAG System - å¾ Telegram èŠå¤©è¨˜éŒ„è‡ªå‹•å­¸ç¿’çš„ RAG ç³»çµ±

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. è‡ªå‹•å¾èŠå¤©è¨˜éŒ„ä¸­å­¸ç¿’ Q&Aã€è©±è¡“ã€ç”¢å“çŸ¥è­˜
2. å‘é‡åŒ–å­˜å„²å’Œèªç¾©æœç´¢
3. æ™ºèƒ½æª¢ç´¢å¢å¼·ç”Ÿæˆ (RAG)
4. çŸ¥è­˜è³ªé‡è©•åˆ†å’Œåé¥‹å¾ªç’°
5. è‡ªå‹•æ¸…ç†å’Œåˆä½µé‡è¤‡çŸ¥è­˜

æ¶æ§‹ï¼š
- ChromaDB ä½œç‚ºå‘é‡æ•¸æ“šåº«ï¼ˆå¯é¸ï¼Œé™ç´šç‚º SQLiteï¼‰
- Sentence Transformers ä½œç‚ºåµŒå…¥æ¨¡å‹ï¼ˆå¯é¸ï¼Œé™ç´šç‚ºç°¡å–®åµŒå…¥ï¼‰
- SQLite ä½œç‚ºæŒä¹…åŒ–å­˜å„²
"""
import sys
import json
import hashlib
import asyncio
import numpy as np
from typing import List, Dict, Any, Optional, Tuple, Callable
from datetime import datetime, timedelta
from dataclasses import dataclass, field
from enum import Enum
from database import db

# å˜—è©¦å°å…¥å¯é¸ä¾è³´
try:
    import chromadb
    from chromadb.config import Settings
    CHROMADB_AVAILABLE = True
except ImportError:
    CHROMADB_AVAILABLE = False
    print("[TelegramRAG] ChromaDB æœªå®‰è£ï¼Œä½¿ç”¨ SQLite å‘é‡å­˜å„²", file=sys.stderr)

try:
    from sentence_transformers import SentenceTransformer
    SENTENCE_TRANSFORMERS_AVAILABLE = True
except ImportError:
    SENTENCE_TRANSFORMERS_AVAILABLE = False
    print("[TelegramRAG] SentenceTransformers æœªå®‰è£ï¼Œä½¿ç”¨ç°¡å–®åµŒå…¥", file=sys.stderr)


class KnowledgeType(Enum):
    """çŸ¥è­˜é¡å‹"""
    QA = "qa"                    # å•ç­”å°
    SCRIPT = "script"            # æˆåŠŸè©±è¡“
    PRODUCT = "product"          # ç”¢å“ä¿¡æ¯
    OBJECTION = "objection"      # ç•°è­°è™•ç†
    GREETING = "greeting"        # é–‹å ´ç™½
    CLOSING = "closing"          # æˆäº¤è©±è¡“
    FAQ = "faq"                  # å¸¸è¦‹å•é¡Œ
    CUSTOM = "custom"            # è‡ªå®šç¾©


class ConversationOutcome(Enum):
    """å°è©±çµæœ"""
    CONVERTED = "converted"      # å·²æˆäº¤
    INTERESTED = "interested"    # æœ‰èˆˆè¶£
    NEGOTIATING = "negotiating"  # æ´½è«‡ä¸­
    REPLIED = "replied"          # å·²å›å¾©
    CONTACTED = "contacted"      # å·²è¯ç¹«
    CHURNED = "churned"          # å·²æµå¤±
    UNKNOWN = "unknown"          # æœªçŸ¥


@dataclass
class KnowledgeItem:
    """çŸ¥è­˜é …ç›®"""
    id: Optional[int] = None
    knowledge_type: KnowledgeType = KnowledgeType.QA
    question: str = ""
    answer: str = ""
    context: str = ""
    keywords: List[str] = field(default_factory=list)
    source_user_id: str = ""
    source_chat_id: str = ""
    source_account: str = ""
    success_score: float = 0.5
    use_count: int = 0
    feedback_positive: int = 0
    feedback_negative: int = 0
    embedding_id: str = ""
    is_active: bool = True
    created_at: Optional[datetime] = None
    updated_at: Optional[datetime] = None


@dataclass
class SearchResult:
    """æœç´¢çµæœ"""
    item: KnowledgeItem
    similarity: float = 0.0
    source: str = "keyword"  # vector / keyword / hybrid


class TelegramRAGSystem:
    """
    Telegram RAG ç³»çµ± - å®Œæ•´çš„æª¢ç´¢å¢å¼·ç”Ÿæˆè§£æ±ºæ–¹æ¡ˆ
    
    ç‰¹æ€§ï¼š
    1. æ”¯æŒå¤šç¨®å‘é‡å­˜å„²å¾Œç«¯ï¼ˆChromaDB, SQLiteï¼‰
    2. æ”¯æŒå¤šç¨®åµŒå…¥æ¨¡å‹ï¼ˆSentence Transformers, ç°¡å–®å“ˆå¸Œï¼‰
    3. æ™ºèƒ½æ··åˆæœç´¢ï¼ˆå‘é‡ + é—œéµè©ï¼‰
    4. è‡ªå‹•å­¸ç¿’å’ŒçŸ¥è­˜æå–
    5. è³ªé‡è©•åˆ†å’Œåé¥‹å¾ªç’°
    """
    
    # æˆåŠŸå°è©±çš„çµæœ
    SUCCESS_OUTCOMES = [
        ConversationOutcome.CONVERTED,
        ConversationOutcome.INTERESTED,
        ConversationOutcome.NEGOTIATING
    ]
    
    # åµŒå…¥ç¶­åº¦
    EMBEDDING_DIM = 384
    
    def __init__(self):
        self.is_initialized = False
        self.event_callback: Optional[Callable] = None
        
        # å‘é‡å­˜å„²
        self.chroma_client = None
        self.collection = None
        self.use_chromadb = False
        
        # åµŒå…¥æ¨¡å‹
        self.embedding_model = None
        self.use_neural_embedding = False
        
        # ç·©å­˜ - ğŸ”§ Phase 1 å„ªåŒ–ï¼šæ·»åŠ å¤§å°é™åˆ¶
        self._embedding_cache: Dict[str, np.ndarray] = {}
        self._knowledge_cache: Dict[str, List[KnowledgeItem]] = {}
        self._cache_ttl = 300  # 5åˆ†é˜
        self._cache_timestamps: Dict[str, datetime] = {}
        self._max_embedding_cache_size = 500  # æœ€å¤šç·©å­˜ 500 å€‹åµŒå…¥
        self._max_knowledge_cache_size = 200  # æœ€å¤šç·©å­˜ 200 å€‹çŸ¥è­˜æŸ¥è©¢
        
        # é…ç½®
        self.min_question_length = 5
        self.min_answer_length = 10
        self.min_quality_score = 0.3
        self.max_similar_results = 5
        self.similarity_threshold = 0.4
    
    def log(self, message: str, level: str = "info"):
        """è¨˜éŒ„æ—¥èªŒ"""
        formatted = f"[TelegramRAG] {message}"
        print(formatted, file=sys.stderr)
        if self.event_callback:
            self.event_callback("log-entry", {
                "message": formatted,
                "type": level
            })
    
    async def initialize(self, use_chromadb: bool = True, use_neural: bool = False):
        """
        åˆå§‹åŒ– RAG ç³»çµ±
        
        Args:
            use_chromadb: æ˜¯å¦ä½¿ç”¨ ChromaDBï¼ˆå¦å‰‡ç”¨ SQLiteï¼‰
            use_neural: æ˜¯å¦ä½¿ç”¨ç¥ç¶“ç¶²çµ¡åµŒå…¥ï¼ˆå¦å‰‡ç”¨ç°¡å–®å“ˆå¸Œï¼‰
                       ğŸ”§ Phase 1 å„ªåŒ–ï¼šé»˜èª False ç¯€çœ 300-500MB å…§å­˜
        """
        if self.is_initialized:
            return True
        
        try:
            # 1. åˆå§‹åŒ–å‘é‡å­˜å„²
            if use_chromadb and CHROMADB_AVAILABLE:
                await self._init_chromadb()
            else:
                self.log("ä½¿ç”¨ SQLite ä½œç‚ºå‘é‡å­˜å„²")
            
            # 2. åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
            if use_neural and SENTENCE_TRANSFORMERS_AVAILABLE:
                await self._init_embedding_model()
            else:
                self.log("ä½¿ç”¨ç°¡å–®å“ˆå¸ŒåµŒå…¥")
            
            # 3. ç¢ºä¿æ•¸æ“šåº«è¡¨å­˜åœ¨
            await self._ensure_tables()
            
            self.is_initialized = True
            self.log(f"âœ“ RAG ç³»çµ±åˆå§‹åŒ–å®Œæˆ (å‘é‡å­˜å„²: {'ChromaDB' if self.use_chromadb else 'SQLite'}, åµŒå…¥: {'Neural' if self.use_neural_embedding else 'Simple'})", "success")
            return True
            
        except Exception as e:
            self.log(f"åˆå§‹åŒ–å¤±æ•—: {e}", "error")
            import traceback
            traceback.print_exc(file=sys.stderr)
            return False
    
    async def _init_chromadb(self):
        """åˆå§‹åŒ– ChromaDB"""
        try:
            # æ–°ç‰ˆ ChromaDB APIï¼ˆ0.4.0+ï¼‰
            import os
            persist_dir = os.path.join(os.path.dirname(__file__), "chroma_rag_db")
            os.makedirs(persist_dir, exist_ok=True)
            
            # ğŸ”§ Phase 3 å„ªåŒ–ï¼šChromaDB è¨­ç½®å„ªåŒ–
            chroma_settings = Settings(
                anonymized_telemetry=False,
                allow_reset=False,
                is_persistent=True,
            )
            
            # å˜—è©¦ä½¿ç”¨ PersistentClientï¼ˆæ–°ç‰ˆï¼‰
            try:
                self.chroma_client = chromadb.PersistentClient(
                    path=persist_dir,
                    settings=chroma_settings
                )
            except (AttributeError, TypeError):
                # é™ç´šåˆ°èˆŠç‰ˆ API
                try:
                    self.chroma_client = chromadb.PersistentClient(path=persist_dir)
                except AttributeError:
                    self.chroma_client = chromadb.Client(Settings(
                        persist_directory=persist_dir,
                        anonymized_telemetry=False
                    ))
            
            self.collection = self.chroma_client.get_or_create_collection(
                name="telegram_rag",
                metadata={"description": "Telegram èŠå¤©è¨˜éŒ„ RAG çŸ¥è­˜åº«"}
            )
            
            self.use_chromadb = True
            count = self.collection.count()
            self.log(f"âœ“ ChromaDB å·²åˆå§‹åŒ–ï¼Œç¾æœ‰ {count} æ¢çŸ¥è­˜")
            
            # ğŸ”§ Phase 3 å„ªåŒ–ï¼šå¦‚æœæ•¸æ“šé‡éå¤§ï¼Œæç¤ºæ¸…ç†
            if count > 10000:
                self.log(f"âš ï¸ ChromaDB æ•¸æ“šé‡è¼ƒå¤§ ({count})ï¼Œå»ºè­°å®šæœŸæ¸…ç†", "warning")
            
        except Exception as e:
            self.log(f"ChromaDB åˆå§‹åŒ–å¤±æ•—: {e}ï¼Œé™ç´šç‚º SQLite", "warning")
            self.use_chromadb = False
    
    async def _init_embedding_model(self):
        """åˆå§‹åŒ–åµŒå…¥æ¨¡å‹"""
        try:
            # ä½¿ç”¨æ”¯æŒä¸­æ–‡çš„å¤šèªè¨€æ¨¡å‹
            self.embedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
            self.use_neural_embedding = True
            self.log("âœ“ ç¥ç¶“ç¶²çµ¡åµŒå…¥æ¨¡å‹å·²è¼‰å…¥")
        except Exception as e:
            self.log(f"åµŒå…¥æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}ï¼Œä½¿ç”¨ç°¡å–®åµŒå…¥", "warning")
            self.use_neural_embedding = False
    
    async def _ensure_tables(self):
        """ç¢ºä¿æ•¸æ“šåº«è¡¨å­˜åœ¨"""
        # RAG çŸ¥è­˜è¡¨
        await db._connection.execute("""
            CREATE TABLE IF NOT EXISTS rag_knowledge (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                knowledge_type TEXT NOT NULL DEFAULT 'qa',
                question TEXT NOT NULL,
                answer TEXT NOT NULL,
                context TEXT,
                keywords TEXT,
                source_user_id TEXT,
                source_chat_id TEXT,
                source_account TEXT,
                success_score REAL DEFAULT 0.5,
                use_count INTEGER DEFAULT 0,
                feedback_positive INTEGER DEFAULT 0,
                feedback_negative INTEGER DEFAULT 0,
                embedding BLOB,
                embedding_id TEXT UNIQUE,
                is_active INTEGER DEFAULT 1,
                created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                updated_at DATETIME DEFAULT CURRENT_TIMESTAMP
            )
        """)
        
        # RAG å­¸ç¿’è¨˜éŒ„è¡¨
        await db._connection.execute("""
            CREATE TABLE IF NOT EXISTS rag_learning_log (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                user_id TEXT NOT NULL,
                session_id TEXT,
                outcome TEXT,
                message_count INTEGER DEFAULT 0,
                qa_extracted INTEGER DEFAULT 0,
                scripts_extracted INTEGER DEFAULT 0,
                quality_score REAL DEFAULT 0,
                is_processed INTEGER DEFAULT 0,
                created_at DATETIME DEFAULT CURRENT_TIMESTAMP
            )
        """)
        
        # ğŸ†• çŸ¥è­˜ç¼ºå£è¿½è¹¤è¡¨
        await db._connection.execute("""
            CREATE TABLE IF NOT EXISTS rag_knowledge_gaps (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                query TEXT NOT NULL,
                query_hash TEXT UNIQUE,
                hit_count INTEGER DEFAULT 1,
                best_similarity REAL DEFAULT 0,
                suggested_answer TEXT,
                suggested_type TEXT DEFAULT 'faq',
                status TEXT DEFAULT 'pending',
                resolved_knowledge_id INTEGER,
                user_context TEXT,
                source_type TEXT DEFAULT 'user',
                category TEXT DEFAULT 'general',
                created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                updated_at DATETIME DEFAULT CURRENT_TIMESTAMP
            )
        """)
        
        # ğŸ”§ Phase 8: ç¢ºä¿èˆŠè¡¨æœ‰æ–°å¢çš„åˆ—ï¼ˆå…¼å®¹ç¾æœ‰æ•¸æ“šï¼‰
        for col_name, col_def in [
            ('source_type', "TEXT DEFAULT 'user'"),
            ('category', "TEXT DEFAULT 'general'")
        ]:
            try:
                await db._connection.execute(f"""
                    ALTER TABLE rag_knowledge_gaps ADD COLUMN {col_name} {col_def}
                """)
                self.log(f"âœ“ æ·»åŠ åˆ— rag_knowledge_gaps.{col_name}")
            except Exception as col_err:
                if 'duplicate column' not in str(col_err).lower():
                    pass  # åˆ—å·²å­˜åœ¨æˆ–å…¶ä»–éè‡´å‘½éŒ¯èª¤
        
        # ğŸ†• çŸ¥è­˜æ•ˆæœè¿½è¹¤è¡¨
        await db._connection.execute("""
            CREATE TABLE IF NOT EXISTS rag_knowledge_effectiveness (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                knowledge_id INTEGER NOT NULL,
                conversation_id TEXT,
                was_helpful INTEGER DEFAULT 0,
                led_to_conversion INTEGER DEFAULT 0,
                response_time_ms INTEGER,
                user_continued INTEGER DEFAULT 1,
                created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (knowledge_id) REFERENCES rag_knowledge(id)
            )
        """)
        
        # å‰µå»ºç´¢å¼•
        try:
            await db._connection.execute(
                "CREATE INDEX IF NOT EXISTS idx_rag_knowledge_type ON rag_knowledge(knowledge_type)"
            )
            await db._connection.execute(
                "CREATE INDEX IF NOT EXISTS idx_rag_knowledge_active ON rag_knowledge(is_active)"
            )
            await db._connection.execute(
                "CREATE INDEX IF NOT EXISTS idx_rag_knowledge_score ON rag_knowledge(success_score DESC)"
            )
            await db._connection.execute(
                "CREATE INDEX IF NOT EXISTS idx_rag_gaps_status ON rag_knowledge_gaps(status)"
            )
            await db._connection.execute(
                "CREATE INDEX IF NOT EXISTS idx_rag_gaps_count ON rag_knowledge_gaps(hit_count DESC)"
            )
        except:
            pass
        
        await db._connection.commit()
    
    # ==================== åµŒå…¥æ–¹æ³• ====================
    
    def _compute_embedding(self, text: str) -> np.ndarray:
        """è¨ˆç®—æ–‡æœ¬åµŒå…¥å‘é‡"""
        # æª¢æŸ¥ç·©å­˜
        cache_key = hashlib.md5(text.encode()).hexdigest()[:16]
        if cache_key in self._embedding_cache:
            return self._embedding_cache[cache_key]
        
        if self.use_neural_embedding and self.embedding_model:
            embedding = self.embedding_model.encode(text, convert_to_numpy=True)
        else:
            embedding = self._simple_embedding(text)
        
        # ğŸ”§ Phase 1 å„ªåŒ–ï¼šç·©å­˜å¤§å°é™åˆ¶ï¼ˆLRU æ·˜æ±°ï¼‰
        if len(self._embedding_cache) >= self._max_embedding_cache_size:
            # ç§»é™¤æœ€èˆŠçš„ 20% æ¢ç›®
            keys_to_remove = list(self._embedding_cache.keys())[:self._max_embedding_cache_size // 5]
            for key in keys_to_remove:
                del self._embedding_cache[key]
        
        # ç·©å­˜çµæœ
        self._embedding_cache[cache_key] = embedding
        return embedding
    
    def _simple_embedding(self, text: str) -> np.ndarray:
        """ç°¡å–®æ–‡æœ¬åµŒå…¥ - åŸºæ–¼å­—ç¬¦ n-gram å“ˆå¸Œ"""
        text = text.lower()
        
        features = []
        
        # å­—ç¬¦ç´š n-gram å“ˆå¸Œç‰¹å¾µ
        for n in [2, 3, 4]:
            ngrams = [text[i:i+n] for i in range(len(text)-n+1)]
            for ngram in ngrams:
                h = int(hashlib.md5(ngram.encode()).hexdigest()[:8], 16)
                features.append(h % self.EMBEDDING_DIM)
        
        # è©ç´šç‰¹å¾µ
        words = text.split()
        for word in words:
            h = int(hashlib.md5(word.encode()).hexdigest()[:8], 16)
            features.append(h % self.EMBEDDING_DIM)
        
        # ç”Ÿæˆå‘é‡
        embedding = np.zeros(self.EMBEDDING_DIM, dtype=np.float32)
        for f in features:
            embedding[f] += 1.0
        
        # æ­£è¦åŒ–
        norm = np.linalg.norm(embedding)
        if norm > 0:
            embedding = embedding / norm
        
        return embedding
    
    def _cosine_similarity(self, a: np.ndarray, b: np.ndarray) -> float:
        """è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦"""
        dot = np.dot(a, b)
        norm_a = np.linalg.norm(a)
        norm_b = np.linalg.norm(b)
        if norm_a == 0 or norm_b == 0:
            return 0.0
        return float(dot / (norm_a * norm_b))
    
    # ==================== çŸ¥è­˜æå–æ–¹æ³• ====================
    
    async def learn_from_conversation(
        self,
        user_id: str,
        messages: List[Dict[str, Any]],
        outcome: ConversationOutcome = ConversationOutcome.UNKNOWN,
        account_phone: str = "",
        chat_id: str = ""
    ) -> Dict[str, Any]:
        """
        å¾å°è©±ä¸­å­¸ç¿’çŸ¥è­˜
        
        Args:
            user_id: ç”¨æˆ¶ ID
            messages: å°è©±æ¶ˆæ¯åˆ—è¡¨ [{role, content, timestamp}, ...]
            outcome: å°è©±çµæœ
            account_phone: å¸³è™Ÿé›»è©±
            chat_id: èŠå¤© ID
        
        Returns:
            å­¸ç¿’çµæœçµ±è¨ˆ
        """
        if not self.is_initialized:
            await self.initialize()
        
        result = {
            'qa_extracted': 0,
            'scripts_extracted': 0,
            'objections_extracted': 0,
            'total_knowledge': 0,
            'quality_score': 0.0,
            'skipped_reason': None
        }
        
        if len(messages) < 2:
            result['skipped_reason'] = 'å°è©±å¤ªçŸ­'
            return result
        
        try:
            # è¨ˆç®—å°è©±è³ªé‡è©•åˆ†
            quality_score = self._calculate_quality_score(messages, outcome)
            result['quality_score'] = quality_score
            
            # åªå¾é«˜è³ªé‡å°è©±ä¸­å­¸ç¿’
            if quality_score < self.min_quality_score:
                result['skipped_reason'] = f'è³ªé‡ä¸è¶³ ({quality_score:.2f} < {self.min_quality_score})'
                self.log(f"å°è©±è³ªé‡è¼ƒä½ ({quality_score:.2f})ï¼Œè·³éå­¸ç¿’")
                return result
            
            # 1. æå– Q&A å°
            qa_pairs = self._extract_qa_pairs(messages)
            for qa in qa_pairs:
                saved = await self._save_knowledge(
                    knowledge_type=KnowledgeType.QA,
                    question=qa['question'],
                    answer=qa['answer'],
                    context=qa.get('context', ''),
                    source_user_id=user_id,
                    source_chat_id=chat_id,
                    source_account=account_phone,
                    success_score=quality_score
                )
                if saved:
                    result['qa_extracted'] += 1
            
            # 2. å¾æˆåŠŸå°è©±æå–è©±è¡“
            if outcome in self.SUCCESS_OUTCOMES:
                scripts = self._extract_successful_scripts(messages)
                for script in scripts:
                    saved = await self._save_knowledge(
                        knowledge_type=KnowledgeType.SCRIPT,
                        question=script.get('trigger', ''),
                        answer=script['content'],
                        context=script.get('context', ''),
                        source_user_id=user_id,
                        source_chat_id=chat_id,
                        source_account=account_phone,
                        success_score=quality_score * 1.2  # æˆåŠŸè©±è¡“åŠ åˆ†
                    )
                    if saved:
                        result['scripts_extracted'] += 1
            
            # 3. æå–ç•°è­°è™•ç†
            objections = self._extract_objection_handling(messages)
            for obj in objections:
                saved = await self._save_knowledge(
                    knowledge_type=KnowledgeType.OBJECTION,
                    question=obj['objection'],
                    answer=obj['response'],
                    context=obj.get('context', ''),
                    source_user_id=user_id,
                    source_chat_id=chat_id,
                    source_account=account_phone,
                    success_score=quality_score
                )
                if saved:
                    result['objections_extracted'] += 1
            
            result['total_knowledge'] = (
                result['qa_extracted'] + 
                result['scripts_extracted'] + 
                result['objections_extracted']
            )
            
            # è¨˜éŒ„å­¸ç¿’æ—¥èªŒ
            await self._log_learning(user_id, result, outcome)
            
            if result['total_knowledge'] > 0:
                self.log(f"âœ“ å¾å°è©±å­¸ç¿’äº† {result['total_knowledge']} æ¢çŸ¥è­˜ " +
                        f"(Q&A: {result['qa_extracted']}, è©±è¡“: {result['scripts_extracted']}, ç•°è­°: {result['objections_extracted']})")
            
            return result
            
        except Exception as e:
            self.log(f"å­¸ç¿’å¤±æ•—: {e}", "error")
            result['skipped_reason'] = str(e)
            return result
    
    def _calculate_quality_score(
        self, 
        messages: List[Dict], 
        outcome: ConversationOutcome
    ) -> float:
        """è¨ˆç®—å°è©±è³ªé‡è©•åˆ†"""
        score = 0.3  # åŸºç¤åˆ†
        
        # æ ¹æ“šçµæœåŠ åˆ†
        outcome_scores = {
            ConversationOutcome.CONVERTED: 0.5,
            ConversationOutcome.INTERESTED: 0.3,
            ConversationOutcome.NEGOTIATING: 0.25,
            ConversationOutcome.REPLIED: 0.1,
            ConversationOutcome.CONTACTED: 0.05,
            ConversationOutcome.CHURNED: -0.1,
        }
        score += outcome_scores.get(outcome, 0)
        
        # æ ¹æ“šå°è©±é•·åº¦åŠ åˆ†
        msg_count = len(messages)
        if msg_count >= 15:
            score += 0.2
        elif msg_count >= 10:
            score += 0.15
        elif msg_count >= 5:
            score += 0.1
        
        # æ ¹æ“šç”¨æˆ¶åƒèˆ‡åº¦åŠ åˆ†
        user_msgs = [m for m in messages if m.get('role') == 'user']
        ai_msgs = [m for m in messages if m.get('role') == 'assistant']
        
        if len(user_msgs) >= 5:
            score += 0.1
        
        # å›å¾©æ¯”ä¾‹ï¼ˆç”¨æˆ¶å›å¾©è¶Šå¤šè¶Šå¥½ï¼‰
        if len(ai_msgs) > 0:
            reply_ratio = len(user_msgs) / len(ai_msgs)
            if reply_ratio >= 0.8:
                score += 0.1
        
        # ç”¨æˆ¶æ¶ˆæ¯å¹³å‡é•·åº¦
        if user_msgs:
            avg_length = sum(len(m.get('content', '')) for m in user_msgs) / len(user_msgs)
            if avg_length >= 30:
                score += 0.1
        
        return min(1.0, max(0.0, score))
    
    def _extract_qa_pairs(self, messages: List[Dict]) -> List[Dict]:
        """å¾å°è©±ä¸­æå– Q&A å°"""
        qa_pairs = []
        
        for i in range(len(messages) - 1):
            msg = messages[i]
            next_msg = messages[i + 1]
            
            # ç”¨æˆ¶å•é¡Œ â†’ AI/äººå·¥å›è¦†
            if msg.get('role') == 'user' and next_msg.get('role') == 'assistant':
                question = msg.get('content', '').strip()
                answer = next_msg.get('content', '').strip()
                
                # éæ¿¾å¤ªçŸ­çš„å…§å®¹
                if len(question) < self.min_question_length or len(answer) < self.min_answer_length:
                    continue
                
                # æª¢æŸ¥æ˜¯å¦æ˜¯å•é¡Œ
                question_indicators = [
                    '?', 'ï¼Ÿ', 'å—', 'å‘¢', 'æ€éº¼', 'ä»€éº¼', 'å¦‚ä½•', 'å¤šå°‘', 
                    'å“ª', 'ç‚ºä»€éº¼', 'èƒ½ä¸èƒ½', 'å¯ä»¥', 'æœ‰æ²’æœ‰', 'æ˜¯ä¸æ˜¯',
                    'what', 'how', 'why', 'when', 'where', 'which', 'can', 'could'
                ]
                
                is_question = any(ind in question.lower() for ind in question_indicators)
                
                if is_question or len(question) >= 15:
                    qa_pairs.append({
                        'question': question,
                        'answer': answer,
                        'context': self._get_context(messages, i)
                    })
        
        return qa_pairs
    
    def _extract_successful_scripts(self, messages: List[Dict]) -> List[Dict]:
        """æå–æˆåŠŸçš„è©±è¡“"""
        scripts = []
        
        for i, msg in enumerate(messages):
            if msg.get('role') == 'assistant':
                content = msg.get('content', '').strip()
                
                # éæ¿¾å¤ªçŸ­çš„å›è¦†
                if len(content) < 20:
                    continue
                
                # ç²å–è§¸ç™¼é€™å€‹å›è¦†çš„ç”¨æˆ¶æ¶ˆæ¯
                trigger = ''
                if i > 0 and messages[i-1].get('role') == 'user':
                    trigger = messages[i-1].get('content', '')
                
                scripts.append({
                    'trigger': trigger,
                    'content': content,
                    'context': self._get_context(messages, i)
                })
        
        return scripts
    
    def _extract_objection_handling(self, messages: List[Dict]) -> List[Dict]:
        """æå–ç•°è­°è™•ç†æ¨¡å¼"""
        objections = []
        
        # ç•°è­°é—œéµè©
        objection_keywords = [
            'å¤ªè²´', 'ä¸éœ€è¦', 'å†è€ƒæ…®', 'ç®—äº†', 'ä¸è¡Œ', 'æ²’èˆˆè¶£',
            'ä¸‹æ¬¡', 'ä»¥å¾Œ', 'ä¸ç¢ºå®š', 'æ“”å¿ƒ', 'æ€•', 'é¢¨éšª',
            'too expensive', "don't need", 'not sure', 'maybe later'
        ]
        
        for i in range(len(messages) - 1):
            msg = messages[i]
            next_msg = messages[i + 1]
            
            if msg.get('role') == 'user' and next_msg.get('role') == 'assistant':
                user_content = msg.get('content', '').lower()
                
                # æª¢æŸ¥æ˜¯å¦æ˜¯ç•°è­°
                is_objection = any(kw in user_content for kw in objection_keywords)
                
                if is_objection:
                    # æª¢æŸ¥å¾ŒçºŒæ˜¯å¦æœ‰ç©æ¥µå›å¾©ï¼ˆç•°è­°è¢«è™•ç†ï¼‰
                    handled = False
                    for j in range(i + 2, min(i + 5, len(messages))):
                        if messages[j].get('role') == 'user':
                            follow_up = messages[j].get('content', '').lower()
                            positive_words = ['å¥½', 'å¯ä»¥', 'è¡Œ', 'ok', 'äº†è§£', 'æ˜ç™½', 'è¬è¬']
                            if any(w in follow_up for w in positive_words):
                                handled = True
                                break
                    
                    if handled:
                        objections.append({
                            'objection': msg.get('content', ''),
                            'response': next_msg.get('content', ''),
                            'context': self._get_context(messages, i)
                        })
        
        return objections
    
    def _get_context(self, messages: List[Dict], index: int, window: int = 2) -> str:
        """ç²å–æ¶ˆæ¯çš„ä¸Šä¸‹æ–‡"""
        start = max(0, index - window)
        end = min(len(messages), index + window + 1)
        
        context_msgs = []
        for msg in messages[start:end]:
            role = msg.get('role', 'user')
            content = msg.get('content', '')[:100]
            context_msgs.append(f"{role}: {content}")
        
        return '\n'.join(context_msgs)
    
    def _extract_keywords(self, text: str) -> List[str]:
        """æå–é—œéµè©"""
        import re
        
        # ç§»é™¤æ¨™é»ç¬¦è™Ÿ
        text = re.sub(r'[^\w\s]', ' ', text)
        
        # åˆ†è©
        words = []
        current_word = ''
        for char in text:
            if '\u4e00' <= char <= '\u9fff':  # ä¸­æ–‡å­—ç¬¦
                if current_word:
                    words.append(current_word)
                    current_word = ''
                words.append(char)
            elif char.isalnum():
                current_word += char
            else:
                if current_word:
                    words.append(current_word)
                    current_word = ''
        if current_word:
            words.append(current_word)
        
        # éæ¿¾åœç”¨è©å’ŒçŸ­è©
        stopwords = {
            'çš„', 'æ˜¯', 'åœ¨', 'æœ‰', 'å’Œ', 'äº†', 'ä¸', 'é€™', 'é‚£', 'æˆ‘', 'ä½ ', 'ä»–', 'å¥¹', 'å®ƒ',
            'å€‘', 'å—', 'å‘¢', 'å§', 'å•Š', 'å“¦', 'å‘€', 'å°±', 'ä¹Ÿ', 'éƒ½', 'è¦', 'æœƒ', 'èƒ½',
            'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been', 'being',
            'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should'
        }
        keywords = [w for w in words if len(w) >= 2 and w.lower() not in stopwords]
        
        return keywords[:15]  # æœ€å¤š 15 å€‹é—œéµè©
    
    # ==================== çŸ¥è­˜å­˜å„²æ–¹æ³• ====================
    
    async def _save_knowledge(
        self,
        knowledge_type: KnowledgeType,
        question: str,
        answer: str,
        context: str = '',
        source_user_id: str = '',
        source_chat_id: str = '',
        source_account: str = '',
        success_score: float = 0.5
    ) -> Optional[int]:
        """ä¿å­˜çŸ¥è­˜åˆ°æ•¸æ“šåº«ï¼ˆğŸ†• P0: å¢å¼·å»é‡é‚è¼¯ï¼‰"""
        try:
            # ğŸ†• P0: åŸºæœ¬é©—è­‰
            if not question or not answer:
                self.log("çŸ¥è­˜ä¿å­˜å¤±æ•—: å•é¡Œæˆ–ç­”æ¡ˆç‚ºç©º", "warning")
                return None
            
            if len(question.strip()) < 3 or len(answer.strip()) < 5:
                self.log("çŸ¥è­˜ä¿å­˜å¤±æ•—: å…§å®¹éçŸ­", "warning")
                return None
            
            # ç”ŸæˆåµŒå…¥
            combined_text = f"{question} {answer}"
            embedding = self._compute_embedding(combined_text)
            
            # ç”Ÿæˆå”¯ä¸€ ID
            embedding_id = hashlib.md5(combined_text.encode()).hexdigest()
            
            # ğŸ†• P0: å…ˆæª¢æŸ¥ embedding_id æ˜¯å¦å·²å­˜åœ¨ï¼ˆç²¾ç¢ºå»é‡ï¼‰
            existing_by_id = await db._connection.execute("""
                SELECT id, question, answer FROM rag_knowledge WHERE embedding_id = ?
            """, (embedding_id,))
            existing_row = await existing_by_id.fetchone()
            
            if existing_row:
                # å®Œå…¨ç›¸åŒçš„å…§å®¹å·²å­˜åœ¨ï¼Œæ›´æ–°è©•åˆ†å³å¯
                self.log(f"çŸ¥è­˜å·²å­˜åœ¨ (ID={existing_row['id']})ï¼Œæ›´æ–°è©•åˆ†", "info")
                await db._connection.execute("""
                    UPDATE rag_knowledge 
                    SET success_score = (success_score + ?) / 2,
                        use_count = use_count + 1,
                        updated_at = CURRENT_TIMESTAMP
                    WHERE id = ?
                """, (success_score, existing_row['id']))
                await db._connection.commit()
                return existing_row['id']
            
            # æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸ä¼¼çŸ¥è­˜
            existing = await self._find_similar_knowledge(question, threshold=0.85)
            if existing:
                # æ›´æ–°ç¾æœ‰çŸ¥è­˜çš„è©•åˆ†
                self.log(f"ç™¼ç¾ç›¸ä¼¼çŸ¥è­˜ (ID={existing.id})ï¼Œæ›´æ–°è©•åˆ†", "info")
                await db._connection.execute("""
                    UPDATE rag_knowledge 
                    SET success_score = (success_score + ?) / 2,
                        use_count = use_count + 1,
                        updated_at = CURRENT_TIMESTAMP
                    WHERE id = ?
                """, (success_score, existing.id))
                await db._connection.commit()
                return existing.id
            
            # æå–é—œéµè©
            keywords = self._extract_keywords(question + ' ' + answer)
            
            # ğŸ†• P0: ä½¿ç”¨ INSERT OR REPLACE é¿å… UNIQUE éŒ¯èª¤
            try:
                cursor = await db._connection.execute("""
                    INSERT OR REPLACE INTO rag_knowledge 
                    (knowledge_type, question, answer, context, keywords,
                     source_user_id, source_chat_id, source_account,
                     success_score, embedding, embedding_id)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    knowledge_type.value, question, answer, context,
                    ','.join(keywords), source_user_id, source_chat_id,
                    source_account, success_score, embedding.tobytes(), embedding_id
                ))
                
                knowledge_id = cursor.lastrowid
                await db._connection.commit()
            except Exception as insert_err:
                # ğŸ†• P0: æ•ç²ä»»ä½•æ’å…¥éŒ¯èª¤ï¼Œå˜—è©¦æ›´æ–°
                if 'UNIQUE constraint' in str(insert_err):
                    self.log(f"çŸ¥è­˜å·²å­˜åœ¨ï¼Œè·³é: {question[:30]}...", "info")
                    return None
                raise insert_err
            
            # åŒæ™‚ä¿å­˜åˆ° ChromaDBï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.use_chromadb and self.collection:
                try:
                    self.collection.add(
                        embeddings=[embedding.tolist()],
                        documents=[answer],
                        metadatas=[{
                            'type': knowledge_type.value,
                            'question': question[:500],
                            'keywords': ','.join(keywords[:10]),
                            'score': success_score
                        }],
                        ids=[embedding_id]
                    )
                except Exception as e:
                    self.log(f"ChromaDB ä¿å­˜å¤±æ•—: {e}", "warning")
            
            return knowledge_id
            
        except Exception as e:
            self.log(f"ä¿å­˜çŸ¥è­˜å¤±æ•—: {e}", "error")
            return None
    
    async def _find_similar_knowledge(
        self, 
        query: str, 
        threshold: float = 0.85
    ) -> Optional[KnowledgeItem]:
        """æŸ¥æ‰¾ç›¸ä¼¼çš„çŸ¥è­˜"""
        query_embedding = self._compute_embedding(query)
        
        # å¾æ•¸æ“šåº«ä¸­ç²å–æœ€ç›¸ä¼¼çš„çŸ¥è­˜
        cursor = await db._connection.execute("""
            SELECT * FROM rag_knowledge 
            WHERE is_active = 1
            ORDER BY success_score DESC
            LIMIT 50
        """)
        
        rows = await cursor.fetchall()
        
        best_match = None
        best_similarity = 0.0
        
        for row in rows:
            if row['embedding']:
                try:
                    stored_embedding = np.frombuffer(row['embedding'], dtype=np.float32)
                    similarity = self._cosine_similarity(query_embedding, stored_embedding)
                    
                    if similarity >= threshold and similarity > best_similarity:
                        best_similarity = similarity
                        best_match = KnowledgeItem(
                            id=row['id'],
                            knowledge_type=KnowledgeType(row['knowledge_type']),
                            question=row['question'],
                            answer=row['answer'],
                            success_score=row['success_score']
                        )
                except Exception as e:
                    continue
        
        return best_match
    
    # ==================== çŸ¥è­˜æœç´¢æ–¹æ³• ====================
    
    async def search(
        self,
        query: str,
        limit: int = 5,
        knowledge_type: Optional[KnowledgeType] = None,
        min_score: float = 0.3
    ) -> List[SearchResult]:
        """
        æ··åˆæœç´¢çŸ¥è­˜
        
        Args:
            query: æœç´¢æŸ¥è©¢
            limit: è¿”å›æ•¸é‡
            knowledge_type: éæ¿¾çŸ¥è­˜é¡å‹
            min_score: æœ€å°ç›¸ä¼¼åº¦é–¾å€¼
        
        Returns:
            æœç´¢çµæœåˆ—è¡¨
        """
        if not self.is_initialized:
            await self.initialize()
        
        results = []
        
        try:
            # 1. å‘é‡æœç´¢
            vector_results = await self._vector_search(query, limit * 2, knowledge_type)
            results.extend(vector_results)
            
            # 2. é—œéµè©æœç´¢ï¼ˆè£œå……ï¼‰
            if len(results) < limit:
                keyword_results = await self._keyword_search(
                    query, limit * 2, knowledge_type
                )
                
                # å»é‡
                existing_ids = {r.item.id for r in results}
                for kr in keyword_results:
                    if kr.item.id not in existing_ids:
                        results.append(kr)
            
            # 3. éæ¿¾å’Œæ’åº
            results = [r for r in results if r.similarity >= min_score]
            results.sort(key=lambda x: x.similarity * (1 + x.item.success_score), reverse=True)
            
            # 4. æ›´æ–°ä½¿ç”¨è¨ˆæ•¸
            if results:
                for r in results[:limit]:
                    await self._increment_use_count(r.item.id)
            
            # ğŸ†• 5. è¿½è¹¤çŸ¥è­˜ç¼ºå£
            best_sim = results[0].similarity if results else 0
            if best_sim < 0.6:  # ä½æ–¼ 60% åŒ¹é…åº¦è¦–ç‚ºæ½›åœ¨ç¼ºå£
                await self._track_knowledge_gap(query, best_sim)
            
            return results[:limit]
            
        except Exception as e:
            self.log(f"æœç´¢å¤±æ•—: {e}", "error")
            return []
    
    async def _vector_search(
        self,
        query: str,
        limit: int,
        knowledge_type: Optional[KnowledgeType] = None
    ) -> List[SearchResult]:
        """å‘é‡æœç´¢"""
        results = []
        query_embedding = self._compute_embedding(query)
        
        # æ–¹æ³•1ï¼šä½¿ç”¨ ChromaDB
        if self.use_chromadb and self.collection:
            try:
                chroma_results = self.collection.query(
                    query_embeddings=[query_embedding.tolist()],
                    n_results=limit,
                    where={"type": knowledge_type.value} if knowledge_type else None
                )
                
                if chroma_results['ids'] and chroma_results['ids'][0]:
                    for i, doc_id in enumerate(chroma_results['ids'][0]):
                        # å¾æ•¸æ“šåº«ç²å–å®Œæ•´ä¿¡æ¯
                        cursor = await db._connection.execute("""
                            SELECT * FROM rag_knowledge WHERE embedding_id = ?
                        """, (doc_id,))
                        row = await cursor.fetchone()
                        
                        if row:
                            # è¨ˆç®—ç›¸ä¼¼åº¦ï¼ˆChromaDB è¿”å›çš„æ˜¯è·é›¢ï¼‰
                            distance = chroma_results.get('distances', [[0]])[0][i] if chroma_results.get('distances') else 0
                            similarity = max(0, 1 - distance)  # å°‡è·é›¢è½‰ç‚ºç›¸ä¼¼åº¦
                            
                            item = KnowledgeItem(
                                id=row['id'],
                                knowledge_type=KnowledgeType(row['knowledge_type']),
                                question=row['question'],
                                answer=row['answer'],
                                context=row['context'] or '',
                                keywords=row['keywords'].split(',') if row['keywords'] else [],
                                success_score=row['success_score'],
                                use_count=row['use_count']
                            )
                            
                            results.append(SearchResult(
                                item=item,
                                similarity=similarity,
                                source='vector'
                            ))
                
                return results
            except Exception as e:
                self.log(f"ChromaDB æœç´¢å¤±æ•—: {e}", "warning")
        
        # æ–¹æ³•2ï¼šä½¿ç”¨ SQLite å‘é‡æœç´¢
        sql = """
            SELECT * FROM rag_knowledge 
            WHERE is_active = 1 AND embedding IS NOT NULL
        """
        params = []
        
        if knowledge_type:
            sql += " AND knowledge_type = ?"
            params.append(knowledge_type.value)
        
        sql += " ORDER BY success_score DESC LIMIT 100"
        
        cursor = await db._connection.execute(sql, params)
        rows = await cursor.fetchall()
        
        for row in rows:
            try:
                stored_embedding = np.frombuffer(row['embedding'], dtype=np.float32)
                similarity = self._cosine_similarity(query_embedding, stored_embedding)
                
                if similarity >= self.similarity_threshold:
                    item = KnowledgeItem(
                        id=row['id'],
                        knowledge_type=KnowledgeType(row['knowledge_type']),
                        question=row['question'],
                        answer=row['answer'],
                        context=row['context'] or '',
                        keywords=row['keywords'].split(',') if row['keywords'] else [],
                        success_score=row['success_score'],
                        use_count=row['use_count']
                    )
                    
                    results.append(SearchResult(
                        item=item,
                        similarity=similarity,
                        source='vector'
                    ))
            except Exception:
                continue
        
        results.sort(key=lambda x: x.similarity, reverse=True)
        return results[:limit]
    
    async def _keyword_search(
        self,
        query: str,
        limit: int,
        knowledge_type: Optional[KnowledgeType] = None
    ) -> List[SearchResult]:
        """é—œéµè©æœç´¢"""
        results = []
        
        keywords = self._extract_keywords(query)
        if not keywords:
            return results
        
        # æ§‹å»ºæŸ¥è©¢
        conditions = ' OR '.join(['keywords LIKE ?' for _ in keywords[:5]])
        params = [f'%{kw}%' for kw in keywords[:5]]
        
        sql = f"""
            SELECT * FROM rag_knowledge 
            WHERE is_active = 1 AND ({conditions})
        """
        
        if knowledge_type:
            sql += " AND knowledge_type = ?"
            params.append(knowledge_type.value)
        
        sql += " ORDER BY success_score DESC, use_count DESC LIMIT ?"
        params.append(limit)
        
        cursor = await db._connection.execute(sql, params)
        rows = await cursor.fetchall()
        
        for row in rows:
            # è¨ˆç®—é—œéµè©åŒ¹é…åº¦
            stored_keywords = set(row['keywords'].split(',') if row['keywords'] else [])
            query_keywords = set(keywords)
            
            if stored_keywords:
                match_ratio = len(stored_keywords & query_keywords) / len(stored_keywords | query_keywords)
            else:
                match_ratio = 0.3
            
            item = KnowledgeItem(
                id=row['id'],
                knowledge_type=KnowledgeType(row['knowledge_type']),
                question=row['question'],
                answer=row['answer'],
                context=row['context'] or '',
                keywords=list(stored_keywords),
                success_score=row['success_score'],
                use_count=row['use_count']
            )
            
            results.append(SearchResult(
                item=item,
                similarity=match_ratio,
                source='keyword'
            ))
        
        return results
    
    async def _increment_use_count(self, knowledge_id: int):
        """å¢åŠ ä½¿ç”¨è¨ˆæ•¸"""
        try:
            await db._connection.execute("""
                UPDATE rag_knowledge 
                SET use_count = use_count + 1, updated_at = CURRENT_TIMESTAMP
                WHERE id = ?
            """, (knowledge_id,))
            await db._connection.commit()
        except:
            pass
    
    # ==================== RAG ä¸Šä¸‹æ–‡æ§‹å»º ====================
    
    async def build_rag_context(
        self,
        user_message: str,
        user_id: str = None,
        max_items: int = 3,
        max_tokens: int = 1000
    ) -> str:
        """
        æ§‹å»º RAG ä¸Šä¸‹æ–‡
        
        Args:
            user_message: ç”¨æˆ¶æ¶ˆæ¯
            user_id: ç”¨æˆ¶ IDï¼ˆç”¨æ–¼å€‹æ€§åŒ–ï¼‰
            max_items: æœ€å¤§çŸ¥è­˜é …æ•¸
            max_tokens: æœ€å¤§ token æ•¸ï¼ˆä¼°ç®—ï¼‰
        
        Returns:
            æ ¼å¼åŒ–çš„ RAG ä¸Šä¸‹æ–‡
        """
        if not self.is_initialized:
            await self.initialize()
        
        # æœç´¢ç›¸é—œçŸ¥è­˜
        results = await self.search(user_message, limit=max_items * 2)
        
        if not results:
            return ""
        
        context_parts = ["ã€çŸ¥è­˜åº«åƒè€ƒã€‘"]
        current_tokens = 0
        items_added = 0
        
        for result in results:
            if items_added >= max_items:
                break
            
            item = result.item
            
            # ä¼°ç®— tokenï¼ˆä¸­æ–‡ç´„ 1 å­— = 1.5 tokenï¼‰
            item_tokens = int((len(item.question) + len(item.answer)) * 1.5)
            
            if current_tokens + item_tokens > max_tokens:
                break
            
            # æ ¼å¼åŒ–çŸ¥è­˜é …
            type_names = {
                KnowledgeType.QA: 'å•ç­”',
                KnowledgeType.SCRIPT: 'è©±è¡“',
                KnowledgeType.PRODUCT: 'ç”¢å“',
                KnowledgeType.OBJECTION: 'ç•°è­°è™•ç†',
                KnowledgeType.GREETING: 'å•å€™',
                KnowledgeType.CLOSING: 'æˆäº¤',
                KnowledgeType.FAQ: 'FAQ',
            }
            type_name = type_names.get(item.knowledge_type, 'åƒè€ƒ')
            
            if item.question:
                context_parts.append(f"{items_added + 1}. [{type_name}]")
                context_parts.append(f"   å•: {item.question[:200]}")
                context_parts.append(f"   ç­”: {item.answer[:300]}")
            else:
                context_parts.append(f"{items_added + 1}. [{type_name}] {item.answer[:400]}")
            
            current_tokens += item_tokens
            items_added += 1
        
        if items_added > 0:
            context_parts.append("ã€è«‹åƒè€ƒä»¥ä¸ŠçŸ¥è­˜å›è¦†ï¼Œä½†ä¸è¦ç›´æ¥è¤‡è£½ã€‘")
            return '\n'.join(context_parts)
        
        return ""
    
    # ==================== åé¥‹å’Œå„ªåŒ– ====================
    
    async def record_feedback(
        self,
        knowledge_id: int,
        is_positive: bool,
        feedback_text: str = ""
    ):
        """è¨˜éŒ„çŸ¥è­˜ä½¿ç”¨åé¥‹"""
        try:
            field = 'feedback_positive' if is_positive else 'feedback_negative'
            
            await db._connection.execute(f"""
                UPDATE rag_knowledge 
                SET {field} = {field} + 1,
                    use_count = use_count + 1,
                    updated_at = CURRENT_TIMESTAMP
                WHERE id = ?
            """, (knowledge_id,))
            
            # é‡æ–°è¨ˆç®— success_score
            cursor = await db._connection.execute("""
                SELECT feedback_positive, feedback_negative FROM rag_knowledge WHERE id = ?
            """, (knowledge_id,))
            row = await cursor.fetchone()
            
            if row:
                pos = row['feedback_positive']
                neg = row['feedback_negative']
                total = pos + neg
                
                if total > 0:
                    new_score = 0.5 + 0.5 * (pos - neg) / total
                    new_score = max(0.1, min(1.0, new_score))
                    
                    await db._connection.execute("""
                        UPDATE rag_knowledge SET success_score = ? WHERE id = ?
                    """, (new_score, knowledge_id))
            
            await db._connection.commit()
            
        except Exception as e:
            self.log(f"è¨˜éŒ„åé¥‹å¤±æ•—: {e}", "error")
    
    async def _log_learning(
        self,
        user_id: str,
        result: Dict[str, Any],
        outcome: ConversationOutcome
    ):
        """è¨˜éŒ„å­¸ç¿’æ—¥èªŒ"""
        try:
            await db._connection.execute("""
                INSERT INTO rag_learning_log 
                (user_id, outcome, qa_extracted, scripts_extracted, quality_score, is_processed)
                VALUES (?, ?, ?, ?, ?, 1)
            """, (
                user_id, outcome.value,
                result.get('qa_extracted', 0),
                result.get('scripts_extracted', 0),
                result.get('quality_score', 0)
            ))
            await db._connection.commit()
        except:
            pass
    
    # ==================== ç¶­è­·æ–¹æ³• ====================
    
    async def cleanup_low_quality_knowledge(
        self,
        min_score: float = 0.2,
        min_uses: int = 0,
        days_old: int = 30
    ) -> int:
        """æ¸…ç†ä½è³ªé‡çŸ¥è­˜"""
        try:
            cursor = await db._connection.execute("""
                DELETE FROM rag_knowledge
                WHERE is_active = 1
                AND success_score < ?
                AND use_count <= ?
                AND created_at < datetime('now', '-' || ? || ' days')
            """, (min_score, min_uses, days_old))
            
            deleted = cursor.rowcount
            await db._connection.commit()
            
            if deleted > 0:
                self.log(f"æ¸…ç†äº† {deleted} æ¢ä½è³ªé‡çŸ¥è­˜")
            
            return deleted
        except Exception as e:
            self.log(f"æ¸…ç†å¤±æ•—: {e}", "error")
            return 0
    
    async def merge_similar_knowledge(self, similarity_threshold: float = 0.9) -> int:
        """åˆä½µç›¸ä¼¼çš„çŸ¥è­˜"""
        try:
            cursor = await db._connection.execute("""
                SELECT id, question, embedding, success_score
                FROM rag_knowledge
                WHERE is_active = 1 AND embedding IS NOT NULL
                ORDER BY success_score DESC
            """)
            
            rows = await cursor.fetchall()
            
            if len(rows) < 2:
                return 0
            
            merged_count = 0
            to_deactivate = []
            
            for i, row1 in enumerate(rows):
                if row1['id'] in to_deactivate:
                    continue
                
                emb1 = np.frombuffer(row1['embedding'], dtype=np.float32)
                
                for row2 in rows[i+1:]:
                    if row2['id'] in to_deactivate:
                        continue
                    
                    emb2 = np.frombuffer(row2['embedding'], dtype=np.float32)
                    similarity = self._cosine_similarity(emb1, emb2)
                    
                    if similarity >= similarity_threshold:
                        to_deactivate.append(row2['id'])
                        merged_count += 1
            
            if to_deactivate:
                placeholders = ','.join(['?' for _ in to_deactivate])
                await db._connection.execute(f"""
                    UPDATE rag_knowledge SET is_active = 0
                    WHERE id IN ({placeholders})
                """, to_deactivate)
                await db._connection.commit()
                self.log(f"åˆä½µäº† {merged_count} æ¢ç›¸ä¼¼çŸ¥è­˜")
            
            return merged_count
            
        except Exception as e:
            self.log(f"åˆä½µå¤±æ•—: {e}", "error")
            return 0
    
    async def get_statistics(self) -> Dict[str, Any]:
        """ç²å– RAG ç³»çµ±çµ±è¨ˆ"""
        try:
            # æŒ‰é¡å‹çµ±è¨ˆ
            cursor = await db._connection.execute("""
                SELECT 
                    knowledge_type,
                    COUNT(*) as count,
                    AVG(success_score) as avg_score,
                    SUM(use_count) as total_uses,
                    SUM(feedback_positive) as total_positive,
                    SUM(feedback_negative) as total_negative
                FROM rag_knowledge
                WHERE is_active = 1
                GROUP BY knowledge_type
            """)
            
            rows = await cursor.fetchall()
            
            stats = {
                'by_type': {},
                'total_knowledge': 0,
                'total_uses': 0,
                'avg_score': 0.0,
                'chromadb_enabled': self.use_chromadb,
                'neural_embedding': self.use_neural_embedding
            }
            
            type_names = {
                'qa': 'Q&A å•ç­”',
                'script': 'æˆåŠŸè©±è¡“',
                'product': 'ç”¢å“ä¿¡æ¯',
                'objection': 'ç•°è­°è™•ç†',
                'greeting': 'é–‹å ´ç™½',
                'closing': 'æˆäº¤è©±è¡“',
                'faq': 'FAQ',
                'custom': 'è‡ªå®šç¾©'
            }
            
            total_score = 0.0
            for row in rows:
                row_dict = dict(row)
                type_key = row_dict['knowledge_type']
                type_name = type_names.get(type_key, type_key)
                
                stats['by_type'][type_name] = {
                    'count': row_dict['count'],
                    'avg_score': round(row_dict['avg_score'] or 0, 2),
                    'uses': row_dict['total_uses'] or 0,
                    'positive_feedback': row_dict['total_positive'] or 0,
                    'negative_feedback': row_dict['total_negative'] or 0
                }
                stats['total_knowledge'] += row_dict['count']
                stats['total_uses'] += row_dict['total_uses'] or 0
                total_score += (row_dict['avg_score'] or 0) * row_dict['count']
            
            if stats['total_knowledge'] > 0:
                stats['avg_score'] = round(total_score / stats['total_knowledge'], 2)
            
            # ChromaDB çµ±è¨ˆ
            if self.use_chromadb and self.collection:
                stats['vector_count'] = self.collection.count()
            
            # å­¸ç¿’çµ±è¨ˆ
            cursor = await db._connection.execute("""
                SELECT 
                    COUNT(*) as sessions,
                    SUM(qa_extracted) as total_qa,
                    SUM(scripts_extracted) as total_scripts,
                    AVG(quality_score) as avg_quality
                FROM rag_learning_log
                WHERE is_processed = 1
            """)
            learning = await cursor.fetchone()
            
            if learning:
                stats['learning'] = {
                    'sessions_processed': learning['sessions'] or 0,
                    'total_qa_extracted': learning['total_qa'] or 0,
                    'total_scripts_extracted': learning['total_scripts'] or 0,
                    'avg_quality_score': round(learning['avg_quality'] or 0, 2)
                }
            
            return stats
            
        except Exception as e:
            self.log(f"ç²å–çµ±è¨ˆå¤±æ•—: {e}", "error")
            return {'error': str(e)}
    
    async def add_manual_knowledge(
        self,
        knowledge_type: KnowledgeType,
        question: str,
        answer: str,
        context: str = ""
    ) -> Optional[int]:
        """æ‰‹å‹•æ·»åŠ çŸ¥è­˜"""
        return await self._save_knowledge(
            knowledge_type=knowledge_type,
            question=question,
            answer=answer,
            context=context,
            success_score=0.7  # æ‰‹å‹•æ·»åŠ çš„çŸ¥è­˜çµ¦è¼ƒé«˜çš„åˆå§‹åˆ†æ•¸
        )
    
    # ==================== ğŸ†• çŸ¥è­˜ç¼ºå£ç®¡ç† ====================
    
    async def _track_knowledge_gap(self, query: str, best_similarity: float, source_type: str = 'user'):
        """
        è¿½è¹¤çŸ¥è­˜ç¼ºå£
        ğŸ†• P0: å…¥å£éæ¿¾ - åªè¨˜éŒ„çœŸå¯¦ç”¨æˆ¶å•é¡Œ
        """
        try:
            # ğŸ†• P0-2: éæ¿¾ç³»çµ±ç”Ÿæˆçš„ promptï¼ˆä¸æ˜¯çœŸå¯¦ç”¨æˆ¶å•é¡Œï¼‰
            system_keywords = [
                # æœ€å¸¸è¦‹çš„ç³»çµ± promptï¼ˆç²¾ç¢ºåŒ¹é…é–‹é ­ï¼‰
                'æ ¹æ“šä»¥ä¸‹å®¢æˆ¶å•é¡Œ', 'æ ¹æ®ä»¥ä¸‹å®¢æˆ·é—®é¢˜',
                'ç‚ºä»¥ä¸‹å®¢æˆ¶å•é¡Œç”Ÿæˆ', 'ä¸ºä»¥ä¸‹å®¢æˆ·é—®é¢˜ç”Ÿæˆ',
                # é€šç”¨ AI æŒ‡ä»¤
                'æ ¹æ“šä»¥ä¸‹', 'æ ¹æ®ä»¥ä¸‹', 'ç”Ÿæˆä¸€å€‹', 'ç”Ÿæˆä¸€ä¸ª',
                'ç”Ÿæˆ 5 æ¢', 'ç”Ÿæˆ5æ¢', 'ç”Ÿæˆ 5 æ¡', 'ç”Ÿæˆ5æ¡',
                'æ¥­å‹™æè¿°:', 'æ¥­å‹™æè¿°ï¼š', 'ä¸šåŠ¡æè¿°:', 'ä¸šåŠ¡æè¿°ï¼š',
                'JSON æ ¼å¼', 'ï¼ˆJSON', '(JSON',
                'æ¢ç”¢å“çŸ¥è­˜', 'æ¡äº§å“çŸ¥è¯†', 'æ¢éŠ·å”®è©±è¡“', 'æ¡é”€å”®è¯æœ¯',
                'æ¢å¸¸è¦‹å•ç­”', 'æ¡å¸¸è§é—®ç­”', 'è¦æ±‚ï¼š', 'è¦æ±‚:',
                'è«‹ç”¨ç¹é«”', 'è¯·ç”¨ç®€ä½“', 'å°ˆæ¥­ã€å‹å¥½', 'ä¸“ä¸šã€å‹å¥½',
                'é©åˆå®¢æœä½¿ç”¨', 'é€‚åˆå®¢æœä½¿ç”¨', 'å›ç­”è¦ç°¡æ½”', 'å›ç­”è¦ç®€æ´',
                'ä½ æ˜¯å°ˆæ¥­çš„', 'ä½ æ˜¯ä¸“ä¸šçš„', 'èªæ°£å‹å¥½', 'è¯­æ°”å‹å¥½'
            ]
            
            query_lower = query.lower().strip()
            
            # æª¢æŸ¥æ˜¯å¦ç‚ºç³»çµ± prompt
            for kw in system_keywords:
                if kw.lower() in query_lower:
                    self.log(f"éæ¿¾ç³»çµ± prompt: {query[:50]}...", "debug")
                    return  # ä¸è¨˜éŒ„ç³»çµ± prompt
            
            # ğŸ†• P0-2: éæ¿¾éé•·çš„å…§å®¹ï¼ˆè¶…é 200 å­—å¯èƒ½æ˜¯æ–‡æª”è€Œéå•é¡Œï¼‰
            if len(query) > 200:
                self.log(f"éæ¿¾éé•·å…§å®¹: {len(query)} å­—", "debug")
                return
            
            # ğŸ†• P0-2: éæ¿¾éçŸ­çš„å…§å®¹ï¼ˆå°‘æ–¼ 3 å­—ç„¡æ„ç¾©ï¼‰
            if len(query.strip()) < 3:
                return
            
            query_hash = hashlib.md5(query_lower.encode()).hexdigest()
            
            # å˜—è©¦æ›´æ–°ç¾æœ‰ç¼ºå£
            cursor = await db._connection.execute("""
                UPDATE rag_knowledge_gaps 
                SET hit_count = hit_count + 1,
                    best_similarity = MAX(best_similarity, ?),
                    updated_at = CURRENT_TIMESTAMP
                WHERE query_hash = ?
            """, (best_similarity, query_hash))
            
            if cursor.rowcount == 0:
                # ğŸ†• P1: è‡ªå‹•åˆ†é¡å•é¡Œ
                category = self._classify_question(query)
                
                # æ–°å¢ç¼ºå£ï¼ˆå¸¶åˆ†é¡å’Œä¾†æºé¡å‹ï¼‰
                await db._connection.execute("""
                    INSERT OR IGNORE INTO rag_knowledge_gaps 
                    (query, query_hash, best_similarity, hit_count, source_type, category)
                    VALUES (?, ?, ?, 1, ?, ?)
                """, (query[:200], query_hash, best_similarity, source_type, category))
            
            await db._connection.commit()
            
        except Exception as e:
            self.log(f"è¿½è¹¤ç¼ºå£å¤±æ•—: {e}", "warning")
    
    def _classify_question(self, query: str) -> str:
        """ğŸ†• P1: è‡ªå‹•åˆ†é¡å•é¡Œ"""
        query_lower = query.lower()
        
        # åƒ¹æ ¼é¡
        price_keywords = ['è²»ç‡', 'åƒ¹æ ¼', 'å¤šå°‘éŒ¢', 'æ”¶è²»', 'æˆæœ¬', 'ä½£é‡‘', 'è¿”é»', 'æ‰‹çºŒè²»']
        if any(kw in query_lower for kw in price_keywords):
            return 'price'
        
        # æµç¨‹é¡
        process_keywords = ['æ€éº¼', 'å¦‚ä½•', 'æ­¥é©Ÿ', 'æµç¨‹', 'å°æ¥', 'æ¥å…¥', 'é–‹æˆ¶', 'ç”³è«‹']
        if any(kw in query_lower for kw in process_keywords):
            return 'process'
        
        # ç”¢å“é¡
        product_keywords = ['æ”¯æŒ', 'é€šé“', 'ç”¢å“', 'åŠŸèƒ½', 'æœå‹™', 'h5', 'å¾®ä¿¡', 'æ”¯ä»˜å¯¶']
        if any(kw in query_lower for kw in product_keywords):
            return 'product'
        
        # å”®å¾Œé¡
        support_keywords = ['æŠ•è¨´', 'é€€æ¬¾', 'å•é¡Œ', 'æ•…éšœ', 'éŒ¯èª¤', 'å¤±æ•—']
        if any(kw in query_lower for kw in support_keywords):
            return 'support'
        
        return 'other'
    
    async def get_knowledge_gaps(
        self, 
        status: str = 'pending',
        limit: int = 50,
        min_hits: int = 1  # ğŸ†• P0-2: é™ä½é–€æª»ï¼Œé¡¯ç¤ºæ‰€æœ‰ç¼ºå£
    ) -> List[Dict[str, Any]]:
        """ç²å–çŸ¥è­˜ç¼ºå£åˆ—è¡¨"""
        try:
            cursor = await db._connection.execute("""
                SELECT * FROM rag_knowledge_gaps
                WHERE status = ? AND hit_count >= ?
                ORDER BY hit_count DESC, created_at DESC
                LIMIT ?
            """, (status, min_hits, limit))
            
            rows = await cursor.fetchall()
            
            gaps = []
            for row in rows:
                gaps.append({
                    'id': row['id'],
                    'query': row['query'],
                    'hitCount': row['hit_count'],
                    'bestSimilarity': row['best_similarity'],
                    'suggestedAnswer': row['suggested_answer'],
                    'suggestedType': row['suggested_type'],
                    'status': row['status'],
                    'createdAt': row['created_at'],
                    'updatedAt': row['updated_at']
                })
            
            return gaps
            
        except Exception as e:
            self.log(f"ç²å–ç¼ºå£å¤±æ•—: {e}", "error")
            return []
    
    async def suggest_gap_answer(self, gap_id: int) -> Optional[str]:
        """ä½¿ç”¨ AI ç‚ºç¼ºå£ç”Ÿæˆå»ºè­°ç­”æ¡ˆ"""
        try:
            cursor = await db._connection.execute(
                "SELECT query FROM rag_knowledge_gaps WHERE id = ?",
                (gap_id,)
            )
            row = await cursor.fetchone()
            
            if not row:
                return None
            
            query = row['query']
            
            # å˜—è©¦ä½¿ç”¨ AI ç”Ÿæˆç­”æ¡ˆ
            # é€™è£¡è¿”å›ä¸€å€‹ä½”ä½ç¬¦ï¼Œå¯¦éš›æœƒåœ¨ main.py ä¸­èª¿ç”¨ AI
            return f"[AI å»ºè­°] é‡å°å•é¡Œã€Œ{query[:50]}...ã€çš„å›ç­”..."
            
        except Exception as e:
            self.log(f"ç”Ÿæˆå»ºè­°å¤±æ•—: {e}", "error")
            return None
    
    async def resolve_gap(
        self, 
        gap_id: int, 
        knowledge_type: str,
        question: str,
        answer: str
    ) -> Optional[int]:
        """è§£æ±ºçŸ¥è­˜ç¼ºå£ - æ·»åŠ çŸ¥è­˜ä¸¦æ¨™è¨˜å·²è§£æ±º"""
        try:
            # 1. æ·»åŠ çŸ¥è­˜
            kt = KnowledgeType(knowledge_type) if knowledge_type else KnowledgeType.FAQ
            knowledge_id = await self._save_knowledge(
                knowledge_type=kt,
                question=question,
                answer=answer,
                success_score=0.7
            )
            
            if knowledge_id:
                # 2. æ¨™è¨˜ç¼ºå£å·²è§£æ±º
                await db._connection.execute("""
                    UPDATE rag_knowledge_gaps 
                    SET status = 'resolved',
                        resolved_knowledge_id = ?,
                        updated_at = CURRENT_TIMESTAMP
                    WHERE id = ?
                """, (knowledge_id, gap_id))
                await db._connection.commit()
                
                self.log(f"âœ“ è§£æ±ºäº†çŸ¥è­˜ç¼ºå£ #{gap_id}")
            
            return knowledge_id
            
        except Exception as e:
            self.log(f"è§£æ±ºç¼ºå£å¤±æ•—: {e}", "error")
            return None
    
    async def ignore_gap(self, gap_id: int) -> bool:
        """å¿½ç•¥çŸ¥è­˜ç¼ºå£"""
        try:
            await db._connection.execute("""
                UPDATE rag_knowledge_gaps 
                SET status = 'ignored',
                    updated_at = CURRENT_TIMESTAMP
                WHERE id = ?
            """, (gap_id,))
            await db._connection.commit()
            return True
        except:
            return False
    
    async def get_health_report(self) -> Dict[str, Any]:
        """ğŸ†• ç²å–çŸ¥è­˜åº«å¥åº·åº¦å ±å‘Š"""
        try:
            report = {
                'overallScore': 0,
                'completeness': {'score': 0, 'details': {}},
                'effectiveness': {'score': 0, 'details': {}},
                'freshness': {'score': 0, 'details': {}},
                'gaps': {'count': 0, 'topGaps': []},
                'suggestions': []
            }
            
            # 1. å®Œæ•´æ€§è©•åˆ†
            cursor = await db._connection.execute("""
                SELECT knowledge_type, COUNT(*) as count
                FROM rag_knowledge WHERE is_active = 1
                GROUP BY knowledge_type
            """)
            type_counts = {row['knowledge_type']: row['count'] for row in await cursor.fetchall()}
            
            recommended = {
                'qa': 10, 'faq': 15, 'product': 10, 
                'script': 10, 'objection': 10, 'greeting': 5, 'closing': 5
            }
            
            completeness_score = 0
            completeness_details = {}
            for ktype, rec_count in recommended.items():
                actual = type_counts.get(ktype, 0)
                ratio = min(1.0, actual / rec_count)
                completeness_score += ratio
                completeness_details[ktype] = {
                    'actual': actual,
                    'recommended': rec_count,
                    'ratio': round(ratio * 100)
                }
            
            completeness_score = round((completeness_score / len(recommended)) * 100)
            report['completeness'] = {
                'score': completeness_score,
                'details': completeness_details
            }
            
            # 2. æ•ˆæœè©•åˆ†
            cursor = await db._connection.execute("""
                SELECT 
                    AVG(success_score) as avg_score,
                    SUM(use_count) as total_uses,
                    SUM(feedback_positive) as positive,
                    SUM(feedback_negative) as negative
                FROM rag_knowledge WHERE is_active = 1
            """)
            eff = await cursor.fetchone()
            
            avg_score = eff['avg_score'] or 0.5
            total_feedback = (eff['positive'] or 0) + (eff['negative'] or 0)
            satisfaction = (eff['positive'] or 0) / max(1, total_feedback)
            
            effectiveness_score = round((avg_score * 0.6 + satisfaction * 0.4) * 100)
            report['effectiveness'] = {
                'score': effectiveness_score,
                'details': {
                    'avgScore': round(avg_score, 2),
                    'totalUses': eff['total_uses'] or 0,
                    'satisfaction': round(satisfaction * 100),
                    'positiveFeedback': eff['positive'] or 0,
                    'negativeFeedback': eff['negative'] or 0
                }
            }
            
            # 3. æ™‚æ•ˆæ€§è©•åˆ†
            cursor = await db._connection.execute("""
                SELECT 
                    COUNT(*) as total,
                    SUM(CASE WHEN updated_at > datetime('now', '-7 days') THEN 1 ELSE 0 END) as recent,
                    SUM(CASE WHEN updated_at < datetime('now', '-30 days') THEN 1 ELSE 0 END) as stale
                FROM rag_knowledge WHERE is_active = 1
            """)
            fresh = await cursor.fetchone()
            
            total = fresh['total'] or 1
            recent_ratio = (fresh['recent'] or 0) / total
            stale_ratio = (fresh['stale'] or 0) / total
            
            freshness_score = round((1 - stale_ratio * 0.5) * 100)
            report['freshness'] = {
                'score': freshness_score,
                'details': {
                    'total': total,
                    'recentlyUpdated': fresh['recent'] or 0,
                    'stale': fresh['stale'] or 0,
                    'staleRatio': round(stale_ratio * 100)
                }
            }
            
            # 4. çŸ¥è­˜ç¼ºå£
            cursor = await db._connection.execute("""
                SELECT COUNT(*) as count FROM rag_knowledge_gaps WHERE status = 'pending'
            """)
            gap_count = (await cursor.fetchone())['count']
            
            top_gaps = await self.get_knowledge_gaps(limit=5, min_hits=1)
            report['gaps'] = {
                'count': gap_count,
                'topGaps': top_gaps
            }
            
            # 5. è¨ˆç®—ç¸½åˆ†
            report['overallScore'] = round(
                completeness_score * 0.3 +
                effectiveness_score * 0.4 +
                freshness_score * 0.3
            )
            
            # 6. ç”Ÿæˆå»ºè­°
            suggestions = []
            
            if completeness_score < 70:
                low_types = [k for k, v in completeness_details.items() if v['ratio'] < 50]
                if low_types:
                    suggestions.append({
                        'type': 'completeness',
                        'priority': 'high',
                        'message': f"å»ºè­°æ·»åŠ æ›´å¤šã€Œ{', '.join(low_types)}ã€é¡å‹çš„çŸ¥è­˜"
                    })
            
            if gap_count > 5:
                suggestions.append({
                    'type': 'gaps',
                    'priority': 'high',
                    'message': f"æœ‰ {gap_count} å€‹æœªè§£æ±ºçš„çŸ¥è­˜ç¼ºå£ï¼Œå»ºè­°å„ªå…ˆè™•ç†"
                })
            
            if fresh['stale'] > 5:
                suggestions.append({
                    'type': 'freshness',
                    'priority': 'medium',
                    'message': f"æœ‰ {fresh['stale']} æ¢çŸ¥è­˜è¶…é 30 å¤©æœªæ›´æ–°"
                })
            
            if effectiveness_score < 60:
                suggestions.append({
                    'type': 'effectiveness',
                    'priority': 'medium',
                    'message': "çŸ¥è­˜æ•ˆæœè©•åˆ†è¼ƒä½ï¼Œå»ºè­°å¯©æŸ¥ä½åˆ†çŸ¥è­˜"
                })
            
            report['suggestions'] = suggestions
            
            return report
            
        except Exception as e:
            self.log(f"ç”Ÿæˆå¥åº·å ±å‘Šå¤±æ•—: {e}", "error")
            return {'error': str(e), 'overallScore': 0}


# å‰µå»ºå…¨å±€å¯¦ä¾‹
telegram_rag = TelegramRAGSystem()
