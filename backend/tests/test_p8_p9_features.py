"""
ğŸ”§ P9-5: P8/P9 åŠŸèƒ½å–®å…ƒæ¸¬è©¦

è¦†è“‹ï¼š
1. å‰ç«¯å¯©è¨ˆæ—¥èªŒå­˜å„² â€” frontend_audit_log è¡¨ CRUD
2. å‰ç«¯å¯©è¨ˆæŸ¥è©¢ API â€” éæ¿¾/åˆ†é 
3. i18n ç¿»è­¯æ–‡ä»¶å®Œæ•´æ€§ â€” ä¸‰èªè¨€ key ä¸€è‡´æ€§
4. æ€§èƒ½æŒ‡æ¨™è¡¨çµæ§‹ â€” performance_metrics
5. æ­»ä»£ç¢¼é©—è­‰ â€” ç¢ºèªå·²åˆªé™¤æ–‡ä»¶ä¸å½±éŸ¿å°å…¥
"""

import os
import sys
import json
import sqlite3
import tempfile
import pytest
from pathlib import Path
from unittest.mock import MagicMock, patch, AsyncMock
from datetime import datetime

# ç¢ºä¿å¯ä»¥å°å…¥ backend æ¨¡å¡Š
sys.path.insert(0, str(Path(__file__).parent.parent))


# ==================== å‰ç«¯å¯©è¨ˆæ—¥èªŒæ¸¬è©¦ ====================

class TestFrontendAuditLog:
    """æ¸¬è©¦å‰ç«¯å¯©è¨ˆæ—¥èªŒå­˜å„²å’ŒæŸ¥è©¢"""
    
    @pytest.fixture
    def audit_db(self):
        """å‰µå»ºå¸¶æœ‰å¯©è¨ˆè¡¨çš„è‡¨æ™‚æ•¸æ“šåº«"""
        fd, db_path = tempfile.mkstemp(suffix='.db')
        os.close(fd)
        
        conn = sqlite3.connect(db_path)
        conn.row_factory = sqlite3.Row
        conn.execute('''
            CREATE TABLE IF NOT EXISTS frontend_audit_log (
                id TEXT PRIMARY KEY,
                action TEXT NOT NULL,
                severity TEXT DEFAULT 'info',
                user_id TEXT,
                details TEXT,
                timestamp INTEGER,
                received_at TEXT DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        conn.commit()
        
        yield conn, db_path
        
        conn.close()
        os.unlink(db_path)
    
    def test_create_audit_table(self, audit_db):
        """å¯©è¨ˆè¡¨æ‡‰è©²æ­£ç¢ºå‰µå»º"""
        conn, _ = audit_db
        cursor = conn.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='frontend_audit_log'")
        table = cursor.fetchone()
        assert table is not None
        assert table[0] == 'frontend_audit_log'
    
    def test_insert_audit_entry(self, audit_db):
        """æ‡‰èƒ½æ’å…¥å¯©è¨ˆæ¢ç›®"""
        conn, _ = audit_db
        conn.execute(
            'INSERT INTO frontend_audit_log (id, action, severity, user_id, details, timestamp) VALUES (?, ?, ?, ?, ?, ?)',
            ('audit_001', 'nav.view_change', 'info', 'user_1', '{"from":"dashboard","to":"accounts"}', 1707321600000)
        )
        conn.commit()
        
        row = conn.execute('SELECT * FROM frontend_audit_log WHERE id = ?', ('audit_001',)).fetchone()
        assert row is not None
        assert row['action'] == 'nav.view_change'
        assert row['severity'] == 'info'
        assert row['user_id'] == 'user_1'
    
    def test_insert_duplicate_id_ignored(self, audit_db):
        """é‡è¤‡ ID çš„æ’å…¥æ‡‰è¢«å¿½ç•¥ï¼ˆINSERT OR IGNOREï¼‰"""
        conn, _ = audit_db
        conn.execute(
            'INSERT OR IGNORE INTO frontend_audit_log (id, action, severity, user_id, details, timestamp) VALUES (?, ?, ?, ?, ?, ?)',
            ('audit_dup', 'auth.login', 'info', 'user_1', '{}', 1707321600000)
        )
        conn.execute(
            'INSERT OR IGNORE INTO frontend_audit_log (id, action, severity, user_id, details, timestamp) VALUES (?, ?, ?, ?, ?, ?)',
            ('audit_dup', 'auth.logout', 'info', 'user_1', '{}', 1707321700000)
        )
        conn.commit()
        
        count = conn.execute('SELECT COUNT(*) FROM frontend_audit_log WHERE id = ?', ('audit_dup',)).fetchone()[0]
        assert count == 1
        
        # ç¬¬ä¸€æ¢è¨˜éŒ„æ‡‰ä¿ç•™
        row = conn.execute('SELECT action FROM frontend_audit_log WHERE id = ?', ('audit_dup',)).fetchone()
        assert row['action'] == 'auth.login'
    
    def test_batch_insert(self, audit_db):
        """æ‰¹é‡æ’å…¥æ‡‰æ­£ç¢ºå·¥ä½œ"""
        conn, _ = audit_db
        entries = [
            {'id': f'batch_{i}', 'action': f'action_{i}', 'severity': 'info', 'userId': 'user_1', 'details': {}, 'timestamp': 1707321600000 + i}
            for i in range(50)
        ]
        
        for entry in entries:
            conn.execute(
                'INSERT OR IGNORE INTO frontend_audit_log (id, action, severity, user_id, details, timestamp) VALUES (?, ?, ?, ?, ?, ?)',
                (entry['id'], entry['action'], entry['severity'], str(entry['userId']), json.dumps(entry['details']), entry['timestamp'])
            )
        conn.commit()
        
        count = conn.execute('SELECT COUNT(*) FROM frontend_audit_log').fetchone()[0]
        assert count == 50
    
    def test_query_by_action(self, audit_db):
        """æŒ‰æ“ä½œé¡å‹éæ¿¾æŸ¥è©¢"""
        conn, _ = audit_db
        actions = ['auth.login', 'auth.logout', 'nav.view_change', 'nav.view_change', 'system.error']
        for i, action in enumerate(actions):
            conn.execute(
                'INSERT INTO frontend_audit_log (id, action, severity, user_id, details, timestamp) VALUES (?, ?, ?, ?, ?, ?)',
                (f'q_{i}', action, 'info', 'user_1', '{}', 1707321600000 + i)
            )
        conn.commit()
        
        nav_logs = conn.execute(
            'SELECT * FROM frontend_audit_log WHERE action = ?', ('nav.view_change',)
        ).fetchall()
        assert len(nav_logs) == 2
    
    def test_query_by_severity(self, audit_db):
        """æŒ‰åš´é‡ç´šåˆ¥éæ¿¾æŸ¥è©¢"""
        conn, _ = audit_db
        severities = ['info', 'info', 'warning', 'error', 'info']
        for i, sev in enumerate(severities):
            conn.execute(
                'INSERT INTO frontend_audit_log (id, action, severity, user_id, details, timestamp) VALUES (?, ?, ?, ?, ?, ?)',
                (f's_{i}', 'test.action', sev, 'user_1', '{}', 1707321600000 + i)
            )
        conn.commit()
        
        errors = conn.execute(
            'SELECT * FROM frontend_audit_log WHERE severity = ?', ('error',)
        ).fetchall()
        assert len(errors) == 1
    
    def test_query_with_pagination(self, audit_db):
        """åˆ†é æŸ¥è©¢æ‡‰æ­£ç¢ºå·¥ä½œ"""
        conn, _ = audit_db
        for i in range(20):
            conn.execute(
                'INSERT INTO frontend_audit_log (id, action, severity, user_id, details, timestamp) VALUES (?, ?, ?, ?, ?, ?)',
                (f'p_{i}', 'test.action', 'info', 'user_1', '{}', 1707321600000 + i)
            )
        conn.commit()
        
        # ç¬¬ä¸€é ï¼ˆ5æ¢ï¼‰
        page1 = conn.execute(
            'SELECT * FROM frontend_audit_log ORDER BY timestamp DESC LIMIT ? OFFSET ?', (5, 0)
        ).fetchall()
        assert len(page1) == 5
        
        # ç¬¬äºŒé 
        page2 = conn.execute(
            'SELECT * FROM frontend_audit_log ORDER BY timestamp DESC LIMIT ? OFFSET ?', (5, 5)
        ).fetchall()
        assert len(page2) == 5
        
        # ç¢ºèªä¸é‡ç–Š
        page1_ids = {row['id'] for row in page1}
        page2_ids = {row['id'] for row in page2}
        assert page1_ids.isdisjoint(page2_ids)
    
    def test_batch_size_limit(self, audit_db):
        """æ‰¹é‡æ’å…¥æ‡‰é™åˆ¶åœ¨ 100 æ¢ä»¥å…§"""
        conn, _ = audit_db
        entries = [
            {'id': f'limit_{i}', 'action': 'test', 'severity': 'info', 'userId': 'u', 'details': {}, 'timestamp': i}
            for i in range(150)
        ]
        
        # æ¨¡æ“¬å¾Œç«¯é™åˆ¶ï¼šåªè™•ç†å‰ 100 æ¢
        for entry in entries[:100]:
            conn.execute(
                'INSERT OR IGNORE INTO frontend_audit_log (id, action, severity, user_id, details, timestamp) VALUES (?, ?, ?, ?, ?, ?)',
                (entry['id'], entry['action'], entry['severity'], entry['userId'], '{}', entry['timestamp'])
            )
        conn.commit()
        
        count = conn.execute('SELECT COUNT(*) FROM frontend_audit_log').fetchone()[0]
        assert count == 100


# ==================== i18n ç¿»è­¯æ–‡ä»¶å®Œæ•´æ€§æ¸¬è©¦ ====================

class TestI18nCompleteness:
    """æ¸¬è©¦ä¸‰èªè¨€ç¿»è­¯æ–‡ä»¶çš„ key ä¸€è‡´æ€§"""
    
    @pytest.fixture
    def i18n_dir(self):
        """è¿”å› i18n ç¿»è­¯æ–‡ä»¶ç›®éŒ„"""
        base_dir = Path(__file__).parent.parent.parent / 'src' / 'assets' / 'i18n'
        return base_dir
    
    def _load_json(self, path: Path) -> dict:
        """åŠ è¼‰ JSON æ–‡ä»¶"""
        if not path.exists():
            pytest.skip(f"File not found: {path}")
        with open(path, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def _flatten_keys(self, obj: dict, prefix: str = '') -> set:
        """å°‡åµŒå¥—å­—å…¸çš„ key æ‰å¹³åŒ–"""
        keys = set()
        for k, v in obj.items():
            full_key = f'{prefix}.{k}' if prefix else k
            if isinstance(v, dict):
                keys.update(self._flatten_keys(v, full_key))
            else:
                keys.add(full_key)
        return keys
    
    def test_all_locale_files_exist(self, i18n_dir):
        """ä¸‰å€‹èªè¨€æ–‡ä»¶éƒ½æ‡‰å­˜åœ¨"""
        for locale in ['en', 'zh-CN', 'zh-TW']:
            path = i18n_dir / f'{locale}.json'
            assert path.exists(), f"Missing locale file: {path}"
    
    def test_all_files_valid_json(self, i18n_dir):
        """æ‰€æœ‰èªè¨€æ–‡ä»¶æ‡‰ç‚ºåˆæ³• JSON"""
        for locale in ['en', 'zh-CN', 'zh-TW']:
            path = i18n_dir / f'{locale}.json'
            if path.exists():
                data = self._load_json(path)
                assert isinstance(data, dict), f"{locale}.json root should be an object"
    
    def test_zh_tw_has_p8_keys(self, i18n_dir):
        """ç¹é«”ä¸­æ–‡æ‡‰åŒ…å« P8 æ–°å¢çš„ç¿»è­¯ key"""
        data = self._load_json(i18n_dir / 'zh-TW.json')
        
        # æª¢æŸ¥ P8 æ–°å¢ section
        assert 'notification' in data, "Missing 'notification' section"
        assert 'offline' in data, "Missing 'offline' section"
        assert 'audit' in data, "Missing 'audit' section"
        
        # æª¢æŸ¥é—œéµ key
        assert 'center' in data['notification']
        assert 'markAllRead' in data['notification']
        assert 'networkOffline' in data['offline']
        assert 'syncing' in data['offline']
        assert 'title' in data['audit']
    
    def test_top_level_sections_consistent(self, i18n_dir):
        """ä¸‰å€‹èªè¨€æ–‡ä»¶çš„é ‚ç´š section æ‡‰ä¸€è‡´"""
        zh_tw = self._load_json(i18n_dir / 'zh-TW.json')
        zh_cn = self._load_json(i18n_dir / 'zh-CN.json')
        en = self._load_json(i18n_dir / 'en.json')
        
        zh_tw_sections = set(zh_tw.keys())
        zh_cn_sections = set(zh_cn.keys())
        en_sections = set(en.keys())
        
        # P8 æ–°å¢çš„ section æ‡‰åœ¨æ‰€æœ‰èªè¨€ä¸­å­˜åœ¨
        for section in ['notification', 'offline', 'audit']:
            assert section in zh_tw_sections, f"zh-TW missing section: {section}"
            assert section in zh_cn_sections, f"zh-CN missing section: {section}"
            assert section in en_sections, f"en missing section: {section}"
    
    def test_p8_section_keys_match(self, i18n_dir):
        """P8 æ–°å¢ section çš„ key åœ¨ä¸‰èªè¨€ä¸­æ‡‰å®Œå…¨åŒ¹é…"""
        zh_tw = self._load_json(i18n_dir / 'zh-TW.json')
        zh_cn = self._load_json(i18n_dir / 'zh-CN.json')
        en = self._load_json(i18n_dir / 'en.json')
        
        for section in ['notification', 'offline', 'audit']:
            tw_keys = set(zh_tw.get(section, {}).keys())
            cn_keys = set(zh_cn.get(section, {}).keys())
            en_keys = set(en.get(section, {}).keys())
            
            missing_cn = tw_keys - cn_keys
            missing_en = tw_keys - en_keys
            
            assert not missing_cn, f"zh-CN missing keys in '{section}': {missing_cn}"
            assert not missing_en, f"en missing keys in '{section}': {missing_en}"


# ==================== db_utils å¢å¼·æ¸¬è©¦ ====================

class TestDbUtilsEnhanced:
    """db_utils çš„è£œå……æ¸¬è©¦"""
    
    def test_connection_stats_thread_safety(self):
        """ConnectionStats æ‡‰ç·šç¨‹å®‰å…¨"""
        from core.db_utils import ConnectionStats
        
        initial = ConnectionStats.stats()
        initial_created = initial['total_created']
        
        # æ¨¡æ“¬å¤šæ¬¡å‰µå»ºå’Œé—œé–‰
        ConnectionStats.on_create()
        ConnectionStats.on_create()
        ConnectionStats.on_close()
        
        stats = ConnectionStats.stats()
        assert stats['total_created'] == initial_created + 2
        assert stats['total_closed'] >= 1
        assert stats.get('currently_open', stats.get('active', 0)) >= 0
    
    def test_get_connection_context_manager(self):
        """get_connection ä¸Šä¸‹æ–‡ç®¡ç†å™¨æ‡‰è‡ªå‹•é—œé–‰é€£æ¥"""
        from core.db_utils import get_connection, ConnectionStats
        
        stats_before = ConnectionStats.stats()
        
        with get_connection() as conn:
            result = conn.execute('SELECT 1').fetchone()
            assert result[0] == 1
        
        # é€£æ¥æ‡‰å·²é—œé–‰
        stats_after = ConnectionStats.stats()
        # é€£æ¥æ‡‰å·²ä½¿ç”¨ï¼ˆget_connection å¯èƒ½ä½¿ç”¨é€£æ¥æ± æˆ–é‡ç”¨ï¼‰
        assert stats_after.get('total_closed', 0) >= stats_before.get('total_closed', 0)


# ==================== å®‰å…¨ä¸­é–“ä»¶å¢å¼·æ¸¬è©¦ ====================

class TestSecurityMiddlewareEnhanced:
    """å®‰å…¨ä¸­é–“ä»¶è£œå……æ¸¬è©¦"""
    
    def test_skip_paths_defined(self):
        """å®‰å…¨é ­è·³éè·¯å¾‘æ‡‰å·²å®šç¾©"""
        from api.middleware import SKIP_SECURITY_HEADERS_PATHS
        assert isinstance(SKIP_SECURITY_HEADERS_PATHS, (list, tuple, set, frozenset))
        assert len(SKIP_SECURITY_HEADERS_PATHS) > 0
    
    def test_rate_limiter_importable(self):
        """é€Ÿç‡é™åˆ¶å™¨æ‡‰å¯å°å…¥"""
        try:
            from core.rate_limiter import RateLimiter
            limiter = RateLimiter()
            assert limiter is not None
        except ImportError:
            pytest.skip("RateLimiter not available")


# ==================== å‰ç«¯æ–‡ä»¶çµæ§‹é©—è­‰ ====================

class TestFrontendFileStructure:
    """é©—è­‰å‰ç«¯æ–‡ä»¶çµæ§‹çš„å®Œæ•´æ€§ï¼ˆP9-2 æ¸…ç†å¾Œï¼‰"""
    
    @pytest.fixture
    def src_dir(self):
        # CI: GITHUB_WORKSPACE ç‚ºå€‰åº«æ ¹ç›®éŒ„ï¼Œé¿å… cd backend å¾Œè·¯å¾‘è§£æéŒ¯èª¤
        root = os.environ.get('GITHUB_WORKSPACE')
        if root:
            return Path(root) / 'src'
        return Path(__file__).parent.parent.parent / 'src'
    
    def test_dead_code_removed(self, src_dir):
        """å·²åˆªé™¤çš„æ­»ä»£ç¢¼æ–‡ä»¶ä¸æ‡‰å­˜åœ¨"""
        dead_files = [
            'translation.service.ts',          # P9-2 å·²åˆªé™¤
            'core/offline-cache.service.ts',    # P9-2 å·²åˆªé™¤
            'auth.service.ts',                  # P6 å·²åˆªé™¤
        ]
        for f in dead_files:
            path = src_dir / f
            assert not path.exists(), f"Dead code file should be removed: {f}"
    
    def test_p8_new_files_exist(self, src_dir):
        """P8 æ–°å»ºæ–‡ä»¶æ‡‰å…¨éƒ¨å­˜åœ¨"""
        p8_files = [
            'core/offline.interceptor.ts',
            'components/offline-indicator.component.ts',
            'components/notification-center.component.ts',
            'services/audit-tracker.service.ts',
        ]
        for f in p8_files:
            path = src_dir / f
            assert path.exists(), f"P8 file should exist: {f}"
    
    def test_i18n_pipe_exists(self, src_dir):
        """i18n pipe æ‡‰å­˜åœ¨"""
        assert (src_dir / 'core' / 'i18n.pipe.ts').exists()
    
    def test_services_offline_cache_exists(self, src_dir):
        """çµ±ä¸€ç‰ˆ offline-cache æœå‹™æ‡‰å­˜åœ¨"""
        assert (src_dir / 'services' / 'offline-cache.service.ts').exists()
    
    def test_app_config_exists(self, src_dir):
        """app.config.ts æ‡‰å­˜åœ¨"""
        assert (src_dir / 'app.config.ts').exists()
