"""
Vector Memory System - å‘é‡åŒ–è¨˜æ†¶ç³»çµ±
æ”¯æŒèªç¾©æœç´¢çš„æ°¸ä¹…èŠå¤©è¨˜æ†¶

åŠŸèƒ½:
1. èŠå¤©è¨˜æ†¶å‘é‡åŒ–
2. èªç¾©ç›¸ä¼¼åº¦æœç´¢
3. æ™ºèƒ½è¨˜æ†¶æ‘˜è¦
4. RAG ä¸Šä¸‹æ–‡å¢å¼·

ğŸ”§ Phase 3 å„ªåŒ–ï¼š
- å»¶é²åŠ è¼‰ sentence-transformersï¼ˆç¯€çœ ~200MBï¼‰
- æ”¯æŒå…§å­˜å„ªåŒ–é…ç½®
- é»˜èªä½¿ç”¨è¼•é‡ç´šç°¡å–®åµŒå…¥
"""
import sys
import json
import hashlib
import asyncio
from typing import List, Dict, Any, Optional, Tuple, TYPE_CHECKING
from datetime import datetime, timedelta
from database import db

# ğŸ”§ å»¶é²å°å…¥ numpyï¼ˆç´„ 30MBï¼‰
_numpy = None

def _get_numpy():
    """å»¶é²åŠ è¼‰ numpy"""
    global _numpy
    if _numpy is None:
        import numpy
        _numpy = numpy
        print("[VectorMemory] numpy loaded", file=sys.stderr)
    return _numpy


class VectorMemorySystem:
    """å‘é‡åŒ–è¨˜æ†¶ç³»çµ± - æ”¯æŒæœ¬åœ°åµŒå…¥å’Œèªç¾©æœç´¢"""
    
    def __init__(self):
        self._embedding_model = None
        self._embedding_dim = 384  # é»˜èªç¶­åº¦
        self._model_name = "local"
        self._initialized = False
        self._use_simple_embedding = True  # ä½¿ç”¨ç°¡å–®åµŒå…¥ï¼ˆç„¡éœ€å¤–éƒ¨ä¾è³´ï¼‰
        self._neural_loading = False  # é˜²æ­¢é‡è¤‡åŠ è¼‰
        
    async def initialize(self, use_neural: bool = None):
        """
        åˆå§‹åŒ–å‘é‡è¨˜æ†¶ç³»çµ±
        
        Args:
            use_neural: æ˜¯å¦ä½¿ç”¨ç¥ç¶“ç¶²çµ¡åµŒå…¥ã€‚
                        None = è‡ªå‹•æ ¹æ“š MemoryOptConfig æ±ºå®š
                        True = å¼·åˆ¶ä½¿ç”¨ç¥ç¶“ç¶²çµ¡
                        False = ä½¿ç”¨ç°¡å–®åµŒå…¥
        """
        if self._initialized:
            return
        
        # ğŸ”§ å¾é…ç½®ä¸­è®€å–æ˜¯å¦å•Ÿç”¨ç¥ç¶“ç¶²çµ¡åµŒå…¥
        if use_neural is None:
            try:
                from config import MemoryOptConfig
                use_neural = MemoryOptConfig.should_use_neural_embedding()
            except ImportError:
                use_neural = False
        
        if use_neural and not self._neural_loading:
            self._neural_loading = True
            await self._load_neural_model()
        else:
            print("[VectorMemory] ğŸš€ Using simple embeddings (memory-optimized mode)", file=sys.stderr)
        
        self._initialized = True
        print(f"[VectorMemory] Initialized with {self._model_name} embeddings", file=sys.stderr)
    
    async def _load_neural_model(self):
        """
        ç•°æ­¥åŠ è¼‰ç¥ç¶“ç¶²çµ¡æ¨¡å‹
        åœ¨å¾Œå°ç·šç¨‹ä¸­åŠ è¼‰ä»¥é¿å…é˜»å¡ä¸»ç·šç¨‹
        """
        try:
            import asyncio
            
            def _load_sync():
                try:
                    from sentence_transformers import SentenceTransformer
                    # ä½¿ç”¨æ›´å°çš„æ¨¡å‹ä»¥ç¯€çœå…§å­˜
                    model_name = 'paraphrase-multilingual-MiniLM-L12-v2'
                    print(f"[VectorMemory] Loading neural model: {model_name}...", file=sys.stderr)
                    return SentenceTransformer(model_name)
                except ImportError as e:
                    print(f"[VectorMemory] sentence-transformers not available: {e}", file=sys.stderr)
                    return None
                except Exception as e:
                    print(f"[VectorMemory] Failed to load neural model: {e}", file=sys.stderr)
                    return None
            
            # åœ¨ç·šç¨‹æ± ä¸­é‹è¡Œä»¥é¿å…é˜»å¡
            loop = asyncio.get_event_loop()
            model = await loop.run_in_executor(None, _load_sync)
            
            if model:
                self._embedding_model = model
                self._embedding_dim = 384
                self._model_name = "neural"
                self._use_simple_embedding = False
                print("[VectorMemory] âœ“ Neural model loaded successfully", file=sys.stderr)
            else:
                self._use_simple_embedding = True
                print("[VectorMemory] Falling back to simple embeddings", file=sys.stderr)
                
        except Exception as e:
            print(f"[VectorMemory] Error loading neural model: {e}", file=sys.stderr)
            self._use_simple_embedding = True
        finally:
            self._neural_loading = False
    
    def load_neural_on_demand(self):
        """
        æŒ‰éœ€åŠ è¼‰ç¥ç¶“ç¶²çµ¡æ¨¡å‹ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰
        åƒ…åœ¨çœŸæ­£éœ€è¦é«˜è³ªé‡åµŒå…¥æ™‚èª¿ç”¨
        """
        if self._embedding_model is not None:
            return True
        
        try:
            from config import MemoryOptConfig
            if MemoryOptConfig.DISABLE_NEURAL_EMBEDDING:
                print("[VectorMemory] Neural embedding disabled by config", file=sys.stderr)
                return False
        except ImportError:
            pass
        
        try:
            from sentence_transformers import SentenceTransformer
            model_name = 'paraphrase-multilingual-MiniLM-L12-v2'
            print(f"[VectorMemory] On-demand loading: {model_name}...", file=sys.stderr)
            self._embedding_model = SentenceTransformer(model_name)
            self._embedding_dim = 384
            self._model_name = "neural"
            self._use_simple_embedding = False
            print("[VectorMemory] âœ“ Neural model loaded on demand", file=sys.stderr)
            return True
        except Exception as e:
            print(f"[VectorMemory] On-demand load failed: {e}", file=sys.stderr)
            return False
    
    def _simple_embedding(self, text: str):
        """
        ç°¡å–®æ–‡æœ¬åµŒå…¥ - åŸºæ–¼ TF-IDF å’Œå­—ç¬¦ç‰¹å¾µ
        é©ç”¨æ–¼æ²’æœ‰ç¥ç¶“ç¶²çµ¡çš„ç’°å¢ƒ
        
        ğŸ”§ å„ªåŒ–ï¼šä½¿ç”¨å»¶é²åŠ è¼‰çš„ numpy
        """
        np = _get_numpy()
        
        # å°‡æ–‡æœ¬è½‰ç‚ºå°å¯«ä¸¦åˆ†è©
        text = text.lower()
        
        # ç‰¹å¾µæå–
        features = []
        
        # 1. å­—ç¬¦ç´š n-gram å“ˆå¸Œç‰¹å¾µ
        for n in [2, 3, 4]:
            ngrams = [text[i:i+n] for i in range(len(text)-n+1)]
            for ngram in ngrams:
                h = int(hashlib.md5(ngram.encode()).hexdigest()[:8], 16)
                features.append(h % self._embedding_dim)
        
        # 2. è©ç´šç‰¹å¾µ
        words = text.split()
        for word in words:
            h = int(hashlib.md5(word.encode()).hexdigest()[:8], 16)
            features.append(h % self._embedding_dim)
        
        # 3. ç”Ÿæˆç¨€ç–å‘é‡
        embedding = np.zeros(self._embedding_dim, dtype=np.float32)
        for f in features:
            embedding[f] += 1.0
        
        # æ­£è¦åŒ–
        norm = np.linalg.norm(embedding)
        if norm > 0:
            embedding = embedding / norm
        
        return embedding
    
    def _get_embedding(self, text: str):
        """ç²å–æ–‡æœ¬çš„åµŒå…¥å‘é‡"""
        if self._use_simple_embedding or self._embedding_model is None:
            return self._simple_embedding(text)
        else:
            return self._embedding_model.encode(text, convert_to_numpy=True)
    
    def _cosine_similarity(self, a, b) -> float:
        """è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦"""
        np = _get_numpy()
        dot = np.dot(a, b)
        norm_a = np.linalg.norm(a)
        norm_b = np.linalg.norm(b)
        if norm_a == 0 or norm_b == 0:
            return 0.0
        return float(dot / (norm_a * norm_b))
    
    async def add_memory(self, user_id: str, content: str, 
                         memory_type: str = 'conversation',
                         source: str = None,
                         importance: float = 0.5) -> int:
        """
        æ·»åŠ è¨˜æ†¶
        
        Args:
            user_id: ç”¨æˆ¶ID
            content: è¨˜æ†¶å…§å®¹
            memory_type: è¨˜æ†¶é¡å‹ (conversation, fact, preference, summary)
            source: ä¾†æº
            importance: é‡è¦æ€§ (0-1)
        
        Returns:
            è¨˜æ†¶ID
        """
        # ç”ŸæˆåµŒå…¥å‘é‡
        embedding = self._get_embedding(content)
        embedding_bytes = embedding.tobytes()
        
        cursor = await db._connection.execute("""
            INSERT INTO vector_memories 
            (user_id, content, embedding, memory_type, source, importance)
            VALUES (?, ?, ?, ?, ?, ?)
        """, (user_id, content, embedding_bytes, memory_type, source, importance))
        
        await db._connection.commit()
        return cursor.lastrowid
    
    async def search_memories(self, user_id: str, query: str, 
                              limit: int = 5,
                              memory_type: str = None,
                              min_similarity: float = 0.1) -> List[Dict[str, Any]]:
        """
        èªç¾©æœç´¢è¨˜æ†¶
        
        Args:
            user_id: ç”¨æˆ¶ID
            query: æœç´¢æŸ¥è©¢
            limit: è¿”å›æ•¸é‡
            memory_type: éæ¿¾è¨˜æ†¶é¡å‹
            min_similarity: æœ€å°ç›¸ä¼¼åº¦é–¾å€¼
        
        Returns:
            ç›¸é—œè¨˜æ†¶åˆ—è¡¨
        """
        # ç²å–æŸ¥è©¢å‘é‡
        query_embedding = self._get_embedding(query)
        
        # ç²å–ç”¨æˆ¶çš„æ‰€æœ‰è¨˜æ†¶
        query_sql = """
            SELECT id, content, embedding, memory_type, importance, created_at
            FROM vector_memories
            WHERE user_id = ? AND is_active = 1
        """
        params = [user_id]
        
        if memory_type:
            query_sql += " AND memory_type = ?"
            params.append(memory_type)
        
        cursor = await db._connection.execute(query_sql, params)
        rows = await cursor.fetchall()
        
        # è¨ˆç®—ç›¸ä¼¼åº¦
        results = []
        for row in rows:
            memory_embedding = np.frombuffer(row['embedding'], dtype=np.float32)
            similarity = self._cosine_similarity(query_embedding, memory_embedding)
            
            if similarity >= min_similarity:
                results.append({
                    'id': row['id'],
                    'content': row['content'],
                    'memory_type': row['memory_type'],
                    'importance': row['importance'],
                    'similarity': round(similarity, 4),
                    'created_at': row['created_at'],
                })
        
        # æŒ‰ç›¸ä¼¼åº¦å’Œé‡è¦æ€§æ’åº
        results.sort(key=lambda x: x['similarity'] * (1 + x['importance']), reverse=True)
        
        # æ›´æ–°è¨ªå•è¨ˆæ•¸
        if results:
            memory_ids = [r['id'] for r in results[:limit]]
            placeholders = ','.join(['?' for _ in memory_ids])
            await db._connection.execute(f"""
                UPDATE vector_memories 
                SET access_count = access_count + 1, last_accessed = CURRENT_TIMESTAMP
                WHERE id IN ({placeholders})
            """, memory_ids)
            await db._connection.commit()
        
        return results[:limit]
    
    async def build_context_from_memory(self, user_id: str, current_message: str,
                                         max_tokens: int = 1500) -> str:
        """
        å¾è¨˜æ†¶æ§‹å»º RAG ä¸Šä¸‹æ–‡
        
        Args:
            user_id: ç”¨æˆ¶ID
            current_message: ç•¶å‰æ¶ˆæ¯
            max_tokens: æœ€å¤§ token æ•¸
        
        Returns:
            æ§‹å»ºçš„ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
        """
        context_parts = []
        current_tokens = 0
        
        # 1. æœç´¢ç›¸é—œè¨˜æ†¶
        memories = await self.search_memories(user_id, current_message, limit=10)
        
        for memory in memories:
            content = memory['content']
            # ä¼°ç®— tokenï¼ˆç°¡å–®æŒ‰å­—ç¬¦è¨ˆç®—ï¼‰
            tokens = len(content)
            
            if current_tokens + tokens > max_tokens:
                break
            
            memory_type_names = {
                'conversation': 'å°è©±',
                'fact': 'äº‹å¯¦',
                'preference': 'åå¥½',
                'summary': 'æ‘˜è¦',
            }
            type_name = memory_type_names.get(memory['memory_type'], memory['memory_type'])
            
            context_parts.append(f"[{type_name}] {content}")
            current_tokens += tokens
        
        # 2. ç²å–æœ€è¿‘çš„å°è©±æ‘˜è¦
        cursor = await db._connection.execute("""
            SELECT summary, key_points FROM conversation_summaries
            WHERE user_id = ?
            ORDER BY created_at DESC
            LIMIT 1
        """, (user_id,))
        summary = await cursor.fetchone()
        
        if summary and current_tokens + len(summary['summary']) <= max_tokens:
            context_parts.insert(0, f"[å°è©±æ‘˜è¦] {summary['summary']}")
        
        if not context_parts:
            return ""
        
        return "[ç”¨æˆ¶è¨˜æ†¶]\n" + "\n".join(context_parts)
    
    async def summarize_conversation(self, user_id: str, 
                                      max_messages: int = 50) -> Dict[str, Any]:
        """
        ç”Ÿæˆå°è©±æ‘˜è¦ä¸¦ä¿å­˜
        
        Args:
            user_id: ç”¨æˆ¶ID
            max_messages: è¦æ‘˜è¦çš„æœ€å¤§æ¶ˆæ¯æ•¸
        
        Returns:
            æ‘˜è¦çµæœ
        """
        # ç²å–æœ€è¿‘çš„å°è©±
        history = await db.get_chat_history(user_id, limit=max_messages)
        
        if not history:
            return {'success': False, 'error': 'No conversation history'}
        
        # çµ±è¨ˆä¿¡æ¯
        user_messages = [m for m in history if m['role'] == 'user']
        ai_messages = [m for m in history if m['role'] == 'assistant']
        
        # æå–é—œéµè©ï¼ˆç°¡å–®å¯¦ç¾ï¼‰
        all_text = " ".join([m['content'] for m in history])
        words = all_text.split()
        word_freq = {}
        for word in words:
            if len(word) > 2:
                word_freq[word] = word_freq.get(word, 0) + 1
        
        key_points = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:10]
        key_points = [w for w, _ in key_points]
        
        # ç”Ÿæˆæ‘˜è¦ï¼ˆç°¡å–®ç‰ˆæœ¬ï¼‰
        first_time = history[0].get('timestamp', '')
        last_time = history[-1].get('timestamp', '')
        
        summary = f"èˆ‡ç”¨æˆ¶é€²è¡Œäº† {len(history)} æ¢æ¶ˆæ¯çš„å°è©±ã€‚"
        summary += f"ç”¨æˆ¶ç™¼é€äº† {len(user_messages)} æ¢æ¶ˆæ¯ã€‚"
        
        # åˆ†ææƒ…æ„Ÿè¶¨å‹¢ï¼ˆç°¡å–®ç‰ˆæœ¬ï¼‰
        positive_words = ['å¥½', 'å–œæ­¡', 'æ„Ÿè¬', 'è¬è¬', 'good', 'great', 'thanks', 'like']
        negative_words = ['ä¸', 'æ²’', 'å·®', 'å£', 'bad', 'no', "don't", 'hate']
        
        positive_count = sum(1 for m in user_messages 
                           if any(w in m['content'].lower() for w in positive_words))
        negative_count = sum(1 for m in user_messages 
                           if any(w in m['content'].lower() for w in negative_words))
        
        total = positive_count + negative_count
        sentiment_trend = 0.5
        if total > 0:
            sentiment_trend = positive_count / total
        
        # ä¿å­˜æ‘˜è¦
        cursor = await db._connection.execute("""
            INSERT INTO conversation_summaries 
            (user_id, summary, key_points, sentiment_trend, 
             period_start, period_end, message_count)
            VALUES (?, ?, ?, ?, ?, ?, ?)
        """, (
            user_id, summary, json.dumps(key_points), sentiment_trend,
            first_time, last_time, len(history)
        ))
        
        await db._connection.commit()
        
        # åŒæ™‚æ·»åŠ ç‚ºè¨˜æ†¶
        await self.add_memory(
            user_id=user_id,
            content=summary,
            memory_type='summary',
            importance=0.8
        )
        
        return {
            'success': True,
            'summary_id': cursor.lastrowid,
            'summary': summary,
            'key_points': key_points,
            'sentiment_trend': sentiment_trend,
            'message_count': len(history),
        }
    
    async def auto_extract_facts(self, user_id: str, message: str) -> List[Dict[str, Any]]:
        """
        è‡ªå‹•å¾æ¶ˆæ¯ä¸­æå–äº‹å¯¦å’Œåå¥½
        
        Args:
            user_id: ç”¨æˆ¶ID
            message: ç”¨æˆ¶æ¶ˆæ¯
        
        Returns:
            æå–çš„äº‹å¯¦åˆ—è¡¨
        """
        extracted = []
        msg_lower = message.lower()
        
        # äº‹å¯¦é—œéµè©æ¨¡å¼
        fact_patterns = [
            ('fact', ['æˆ‘æ˜¯', 'æˆ‘åœ¨', 'æˆ‘åš', 'æˆ‘æœ‰', 'æˆ‘çš„', "i'm", 'i am', 'i work', 'i have', 'my']),
            ('preference', ['å–œæ­¡', 'æƒ³è¦', 'éœ€è¦', 'åå¥½', 'æ„›', 'like', 'want', 'prefer', 'love', 'need']),
            ('goal', ['æƒ³', 'æ‰“ç®—', 'è¨ˆåŠƒ', 'å¸Œæœ›', 'plan to', 'want to', 'hope to', 'going to']),
        ]
        
        for fact_type, keywords in fact_patterns:
            if any(kw in msg_lower for kw in keywords):
                # æå–ä¸¦ä¿å­˜
                memory_id = await self.add_memory(
                    user_id=user_id,
                    content=message[:500],
                    memory_type=fact_type,
                    source='auto_extract',
                    importance=0.6
                )
                
                extracted.append({
                    'id': memory_id,
                    'type': fact_type,
                    'content': message[:200],
                })
        
        return extracted
    
    async def get_user_memory_stats(self, user_id: str) -> Dict[str, Any]:
        """ç²å–ç”¨æˆ¶è¨˜æ†¶çµ±è¨ˆ"""
        cursor = await db._connection.execute("""
            SELECT 
                memory_type,
                COUNT(*) as count,
                AVG(importance) as avg_importance,
                SUM(access_count) as total_accesses
            FROM vector_memories
            WHERE user_id = ? AND is_active = 1
            GROUP BY memory_type
        """, (user_id,))
        
        rows = await cursor.fetchall()
        
        stats = {
            'by_type': {row['memory_type']: {
                'count': row['count'],
                'avg_importance': round(row['avg_importance'], 2),
                'total_accesses': row['total_accesses'],
            } for row in rows},
            'total_memories': sum(row['count'] for row in rows),
        }
        
        # ç²å–æœ€è¿‘æ‘˜è¦
        cursor = await db._connection.execute("""
            SELECT * FROM conversation_summaries
            WHERE user_id = ?
            ORDER BY created_at DESC
            LIMIT 1
        """, (user_id,))
        summary = await cursor.fetchone()
        
        if summary:
            stats['latest_summary'] = {
                'summary': summary['summary'],
                'sentiment_trend': summary['sentiment_trend'],
                'created_at': summary['created_at'],
            }
        
        return stats
    
    async def cleanup_old_memories(self, user_id: str = None, 
                                    days: int = 90,
                                    keep_important: bool = True) -> int:
        """
        æ¸…ç†èˆŠè¨˜æ†¶
        
        Args:
            user_id: ç”¨æˆ¶IDï¼ˆå¯é¸ï¼Œä¸å‚³å‰‡æ¸…ç†æ‰€æœ‰ç”¨æˆ¶ï¼‰
            days: ä¿ç•™å¤©æ•¸
            keep_important: æ˜¯å¦ä¿ç•™é‡è¦è¨˜æ†¶
        
        Returns:
            åˆªé™¤çš„è¨˜æ†¶æ•¸é‡
        """
        query = """
            DELETE FROM vector_memories
            WHERE created_at < datetime('now', '-' || ? || ' days')
            AND is_active = 1
        """
        params = [days]
        
        if user_id:
            query += " AND user_id = ?"
            params.append(user_id)
        
        if keep_important:
            query += " AND importance < 0.8"
        
        cursor = await db._connection.execute(query, params)
        deleted = cursor.rowcount
        await db._connection.commit()
        
        return deleted
    
    async def merge_similar_memories(self, user_id: str, 
                                      similarity_threshold: float = 0.85) -> int:
        """
        åˆä½µç›¸ä¼¼çš„è¨˜æ†¶
        
        Args:
            user_id: ç”¨æˆ¶ID
            similarity_threshold: ç›¸ä¼¼åº¦é–¾å€¼
        
        Returns:
            åˆä½µçš„è¨˜æ†¶æ•¸é‡
        """
        cursor = await db._connection.execute("""
            SELECT id, content, embedding, importance
            FROM vector_memories
            WHERE user_id = ? AND is_active = 1
            ORDER BY importance DESC
        """, (user_id,))
        
        rows = await cursor.fetchall()
        
        if len(rows) < 2:
            return 0
        
        merged_count = 0
        to_deactivate = []
        
        for i, row1 in enumerate(rows):
            if row1['id'] in to_deactivate:
                continue
            
            emb1 = np.frombuffer(row1['embedding'], dtype=np.float32)
            
            for row2 in rows[i+1:]:
                if row2['id'] in to_deactivate:
                    continue
                
                emb2 = np.frombuffer(row2['embedding'], dtype=np.float32)
                similarity = self._cosine_similarity(emb1, emb2)
                
                if similarity >= similarity_threshold:
                    # æ¨™è¨˜è¼ƒä¸é‡è¦çš„ç‚ºéæ´»èº
                    to_deactivate.append(row2['id'])
                    merged_count += 1
        
        if to_deactivate:
            placeholders = ','.join(['?' for _ in to_deactivate])
            await db._connection.execute(f"""
                UPDATE vector_memories SET is_active = 0
                WHERE id IN ({placeholders})
            """, to_deactivate)
            await db._connection.commit()
        
        return merged_count


# å…¨å±€å¯¦ä¾‹
vector_memory = VectorMemorySystem()
