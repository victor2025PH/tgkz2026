/**
 * AI Provider Service
 * AI å¤šæ¨¡å‹æä¾›å•†æœå‹™
 * 
 * æ”¯æŒï¼š
 * - Google Gemini
 * - OpenAI GPT
 * - Anthropic Claude
 * - æœ¬åœ°æ¨¡å‹ (Ollama)
 * - DeepSeek
 */
import { Injectable, signal, computed } from '@angular/core';

// ============ é¡å‹å®šç¾© ============

export type AIProviderType = 'gemini' | 'openai' | 'claude' | 'ollama' | 'deepseek';

export interface AIProvider {
  id: AIProviderType;
  name: string;
  icon: string;
  description: string;
  models: AIModel[];
  requiresApiKey: boolean;
  baseUrl?: string;
}

export interface AIModel {
  id: string;
  name: string;
  description: string;
  contextLength: number;
  pricePerMToken?: number;  // æ¯ç™¾è¬ token åƒ¹æ ¼ï¼ˆç¾å…ƒï¼‰
  capabilities: string[];
}

export interface AIConfig {
  provider: AIProviderType;
  model: string;
  apiKey: string;
  baseUrl?: string;
  temperature: number;
  maxTokens: number;
  topP: number;
  systemPrompt: string;
}

export interface AIMessage {
  role: 'system' | 'user' | 'assistant';
  content: string;
}

export interface AIResponse {
  content: string;
  usage: {
    promptTokens: number;
    completionTokens: number;
    totalTokens: number;
  };
  model: string;
  finishReason: string;
}

// ============ æä¾›å•†å®šç¾© ============

export const AI_PROVIDERS: AIProvider[] = [
  {
    id: 'gemini',
    name: 'Google Gemini',
    icon: 'âœ¨',
    description: 'è°·æ­Œæœ€æ–° AI æ¨¡å‹ï¼Œå¿«é€Ÿä¸”å¤šèªè¨€æ”¯æŒ',
    requiresApiKey: true,
    baseUrl: 'https://generativelanguage.googleapis.com/v1beta',
    models: [
      {
        id: 'gemini-1.5-pro',
        name: 'Gemini 1.5 Pro',
        description: 'å¼·å¤§çš„å¤šæ¨¡æ…‹æ¨¡å‹',
        contextLength: 2000000,
        pricePerMToken: 3.5,
        capabilities: ['text', 'vision', 'code', 'reasoning']
      },
      {
        id: 'gemini-1.5-flash',
        name: 'Gemini 1.5 Flash',
        description: 'å¿«é€Ÿé«˜æ•ˆçš„è¼•é‡æ¨¡å‹',
        contextLength: 1000000,
        pricePerMToken: 0.075,
        capabilities: ['text', 'vision', 'code']
      },
      {
        id: 'gemini-2.0-flash-exp',
        name: 'Gemini 2.0 Flash (å¯¦é©—)',
        description: 'æœ€æ–°å¯¦é©—ç‰ˆæœ¬',
        contextLength: 1000000,
        pricePerMToken: 0,
        capabilities: ['text', 'vision', 'code', 'reasoning']
      }
    ]
  },
  {
    id: 'openai',
    name: 'OpenAI',
    icon: 'ğŸ¤–',
    description: 'ChatGPT èƒŒå¾Œçš„ AI æ¨¡å‹',
    requiresApiKey: true,
    baseUrl: 'https://api.openai.com/v1',
    models: [
      {
        id: 'gpt-4o',
        name: 'GPT-4o',
        description: 'æœ€æ–°æ——è‰¦æ¨¡å‹',
        contextLength: 128000,
        pricePerMToken: 5,
        capabilities: ['text', 'vision', 'code', 'reasoning']
      },
      {
        id: 'gpt-4o-mini',
        name: 'GPT-4o Mini',
        description: 'ç¶“æ¿Ÿå¯¦æƒ çš„å°å‹æ¨¡å‹',
        contextLength: 128000,
        pricePerMToken: 0.15,
        capabilities: ['text', 'vision', 'code']
      },
      {
        id: 'gpt-4-turbo',
        name: 'GPT-4 Turbo',
        description: 'é«˜æ€§èƒ½æ¨¡å‹',
        contextLength: 128000,
        pricePerMToken: 10,
        capabilities: ['text', 'vision', 'code', 'reasoning']
      },
      {
        id: 'o1-preview',
        name: 'o1 Preview',
        description: 'æ·±åº¦æ¨ç†æ¨¡å‹',
        contextLength: 128000,
        pricePerMToken: 15,
        capabilities: ['text', 'code', 'reasoning', 'math']
      }
    ]
  },
  {
    id: 'claude',
    name: 'Anthropic Claude',
    icon: 'ğŸ§ ',
    description: 'å®‰å…¨å¯é çš„ AI åŠ©æ‰‹',
    requiresApiKey: true,
    baseUrl: 'https://api.anthropic.com/v1',
    models: [
      {
        id: 'claude-3-5-sonnet-20241022',
        name: 'Claude 3.5 Sonnet',
        description: 'æœ€ä½³ç¶œåˆæ€§èƒ½',
        contextLength: 200000,
        pricePerMToken: 3,
        capabilities: ['text', 'vision', 'code', 'reasoning']
      },
      {
        id: 'claude-3-opus-20240229',
        name: 'Claude 3 Opus',
        description: 'æœ€å¼·æ¨ç†èƒ½åŠ›',
        contextLength: 200000,
        pricePerMToken: 15,
        capabilities: ['text', 'vision', 'code', 'reasoning']
      },
      {
        id: 'claude-3-haiku-20240307',
        name: 'Claude 3 Haiku',
        description: 'å¿«é€Ÿè¼•é‡',
        contextLength: 200000,
        pricePerMToken: 0.25,
        capabilities: ['text', 'vision', 'code']
      }
    ]
  },
  {
    id: 'deepseek',
    name: 'DeepSeek',
    icon: 'ğŸ”',
    description: 'é«˜æ€§åƒ¹æ¯”çš„ä¸­åœ‹ AI æ¨¡å‹',
    requiresApiKey: true,
    baseUrl: 'https://api.deepseek.com',
    models: [
      {
        id: 'deepseek-chat',
        name: 'DeepSeek Chat',
        description: 'é€šç”¨å°è©±æ¨¡å‹',
        contextLength: 64000,
        pricePerMToken: 0.14,
        capabilities: ['text', 'code']
      },
      {
        id: 'deepseek-coder',
        name: 'DeepSeek Coder',
        description: 'å°ˆæ¥­ç·¨ç¨‹æ¨¡å‹',
        contextLength: 64000,
        pricePerMToken: 0.14,
        capabilities: ['code', 'text']
      }
    ]
  },
  {
    id: 'ollama',
    name: 'Ollama (æœ¬åœ°)',
    icon: 'ğŸ¦™',
    description: 'æœ¬åœ°é‹è¡Œï¼Œå®Œå…¨éš±ç§',
    requiresApiKey: false,
    baseUrl: 'http://localhost:11434',
    models: [
      {
        id: 'llama3.2',
        name: 'Llama 3.2',
        description: 'Meta æœ€æ–°é–‹æºæ¨¡å‹',
        contextLength: 128000,
        capabilities: ['text', 'code']
      },
      {
        id: 'qwen2.5',
        name: 'Qwen 2.5',
        description: 'é˜¿é‡Œé€šç¾©åƒå•',
        contextLength: 128000,
        capabilities: ['text', 'code', 'reasoning']
      },
      {
        id: 'mistral',
        name: 'Mistral',
        description: 'æ­æ´²é–‹æºæ¨¡å‹',
        contextLength: 32000,
        capabilities: ['text', 'code']
      },
      {
        id: 'codellama',
        name: 'Code Llama',
        description: 'å°ˆæ¥­ç·¨ç¨‹æ¨¡å‹',
        contextLength: 16000,
        capabilities: ['code']
      }
    ]
  }
];

// ============ æœå‹™å¯¦ç¾ ============

@Injectable({
  providedIn: 'root'
})
export class AIProviderService {
  
  // ç•¶å‰é…ç½®
  private _config = signal<AIConfig>({
    provider: 'gemini',
    model: 'gemini-1.5-flash',
    apiKey: '',
    temperature: 0.7,
    maxTokens: 2048,
    topP: 0.9,
    systemPrompt: ''
  });
  
  config = this._config.asReadonly();
  
  // é€£æ¥ç‹€æ…‹
  private _isConnected = signal(false);
  isConnected = this._isConnected.asReadonly();
  
  // ä½¿ç”¨çµ±è¨ˆ
  private _usage = signal({
    totalTokens: 0,
    totalCalls: 0,
    totalCost: 0
  });
  usage = this._usage.asReadonly();
  
  // è¨ˆç®—å±¬æ€§
  providers = AI_PROVIDERS;
  
  currentProvider = computed(() => 
    AI_PROVIDERS.find(p => p.id === this._config().provider)
  );
  
  currentModel = computed(() => 
    this.currentProvider()?.models.find(m => m.id === this._config().model)
  );
  
  availableModels = computed(() => 
    this.currentProvider()?.models || []
  );
  
  constructor() {
    this.loadConfig();
  }
  
  /**
   * è¨­ç½®é…ç½®
   */
  setConfig(config: Partial<AIConfig>): void {
    this._config.update(c => ({ ...c, ...config }));
    this.saveConfig();
  }
  
  /**
   * åˆ‡æ›æä¾›å•†
   */
  setProvider(providerId: AIProviderType): void {
    const provider = AI_PROVIDERS.find(p => p.id === providerId);
    if (provider) {
      this.setConfig({
        provider: providerId,
        model: provider.models[0]?.id || '',
        baseUrl: provider.baseUrl
      });
    }
  }
  
  /**
   * åˆ‡æ›æ¨¡å‹
   */
  setModel(modelId: string): void {
    this.setConfig({ model: modelId });
  }
  
  /**
   * æ¸¬è©¦é€£æ¥
   */
  async testConnection(): Promise<{ success: boolean; message: string }> {
    try {
      const response = await this.chat([
        { role: 'user', content: 'Hello, please respond with "OK".' }
      ]);
      
      if (response.content) {
        this._isConnected.set(true);
        return { success: true, message: 'é€£æ¥æˆåŠŸï¼' };
      }
      
      return { success: false, message: 'ç„¡éŸ¿æ‡‰' };
    } catch (error: any) {
      this._isConnected.set(false);
      return { success: false, message: error.message || 'é€£æ¥å¤±æ•—' };
    }
  }
  
  /**
   * ç™¼é€èŠå¤©è«‹æ±‚
   */
  async chat(messages: AIMessage[], options?: Partial<AIConfig>): Promise<AIResponse> {
    const config = { ...this._config(), ...options };
    
    // ğŸ”§ ç¢ºä¿ baseUrl æ­£ç¢ºè¨­ç½®
    if (!config.baseUrl || config.baseUrl.startsWith('/')) {
      const provider = AI_PROVIDERS.find(p => p.id === config.provider);
      if (provider?.baseUrl) {
        config.baseUrl = provider.baseUrl;
        console.log(`[AIProvider] ä½¿ç”¨æä¾›å•†é»˜èª baseUrl: ${config.baseUrl}`);
      }
    }
    
    // ğŸ”§ é©—è­‰é…ç½®
    if (!config.baseUrl) {
      throw new Error(`æœªé…ç½® ${config.provider} çš„ API ç«¯é»`);
    }
    
    if (config.provider !== 'ollama' && !config.apiKey) {
      throw new Error(`æœªé…ç½® ${config.provider} çš„ API Key`);
    }
    
    console.log(`[AIProvider] èª¿ç”¨ ${config.provider}/${config.model}, baseUrl: ${config.baseUrl}`);
    
    switch (config.provider) {
      case 'gemini':
        return this.chatGemini(messages, config);
      case 'openai':
        return this.chatOpenAI(messages, config);
      case 'claude':
        return this.chatClaude(messages, config);
      case 'deepseek':
        return this.chatDeepSeek(messages, config);
      case 'ollama':
        return this.chatOllama(messages, config);
      default:
        throw new Error(`Unsupported provider: ${config.provider}`);
    }
  }
  
  /**
   * æµå¼èŠå¤©
   */
  async *chatStream(messages: AIMessage[], options?: Partial<AIConfig>): AsyncGenerator<string> {
    const config = { ...this._config(), ...options };
    
    // ç°¡åŒ–å¯¦ç¾ï¼šéæµå¼å›èª¿
    const response = await this.chat(messages, options);
    
    // æ¨¡æ“¬æµå¼è¼¸å‡º
    const words = response.content.split('');
    for (const word of words) {
      yield word;
      await new Promise(r => setTimeout(r, 20));
    }
  }
  
  // ============ æä¾›å•†ç‰¹å®šå¯¦ç¾ ============
  
  private async chatGemini(messages: AIMessage[], config: AIConfig): Promise<AIResponse> {
    const url = `${config.baseUrl}/models/${config.model}:generateContent?key=${config.apiKey}`;
    
    console.log(`[AIProvider] Gemini URL: ${url.replace(config.apiKey, '***')}`);
    
    // è½‰æ›æ¶ˆæ¯æ ¼å¼
    const contents = messages
      .filter(m => m.role !== 'system')
      .map(m => ({
        role: m.role === 'user' ? 'user' : 'model',
        parts: [{ text: m.content }]
      }));
    
    const systemInstruction = messages.find(m => m.role === 'system');
    
    const response = await fetch(url, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        contents,
        systemInstruction: systemInstruction ? { parts: [{ text: systemInstruction.content }] } : undefined,
        generationConfig: {
          temperature: config.temperature,
          maxOutputTokens: config.maxTokens,
          topP: config.topP
        }
      })
    });
    
    // ğŸ”§ æª¢æŸ¥éŸ¿æ‡‰é¡å‹
    const contentType = response.headers.get('content-type') || '';
    if (!contentType.includes('application/json')) {
      const text = await response.text();
      console.error(`[AIProvider] Gemini è¿”å›é JSON éŸ¿æ‡‰:`, text.substring(0, 200));
      throw new Error(`Gemini API è¿”å›éŒ¯èª¤æ ¼å¼ (${response.status}): å¯èƒ½æ˜¯ URL é…ç½®éŒ¯èª¤`);
    }
    
    if (!response.ok) {
      const error = await response.json();
      throw new Error(error.error?.message || `Gemini API error: ${response.status}`);
    }
    
    const data = await response.json();
    const content = data.candidates?.[0]?.content?.parts?.[0]?.text || '';
    
    this.updateUsage(data.usageMetadata?.promptTokenCount || 0, data.usageMetadata?.candidatesTokenCount || 0, config);
    
    return {
      content,
      usage: {
        promptTokens: data.usageMetadata?.promptTokenCount || 0,
        completionTokens: data.usageMetadata?.candidatesTokenCount || 0,
        totalTokens: data.usageMetadata?.totalTokenCount || 0
      },
      model: config.model,
      finishReason: data.candidates?.[0]?.finishReason || 'stop'
    };
  }
  
  private async chatOpenAI(messages: AIMessage[], config: AIConfig): Promise<AIResponse> {
    const url = `${config.baseUrl}/chat/completions`;
    
    console.log(`[AIProvider] OpenAI URL: ${url}`);
    
    const response = await fetch(url, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${config.apiKey}`
      },
      body: JSON.stringify({
        model: config.model,
        messages,
        temperature: config.temperature,
        max_tokens: config.maxTokens,
        top_p: config.topP
      })
    });
    
    // ğŸ”§ æª¢æŸ¥éŸ¿æ‡‰é¡å‹
    const contentType = response.headers.get('content-type') || '';
    if (!contentType.includes('application/json')) {
      const text = await response.text();
      console.error(`[AIProvider] OpenAI è¿”å›é JSON éŸ¿æ‡‰:`, text.substring(0, 200));
      throw new Error(`OpenAI API è¿”å›éŒ¯èª¤æ ¼å¼ (${response.status}): å¯èƒ½æ˜¯ URL é…ç½®éŒ¯èª¤`);
    }
    
    if (!response.ok) {
      const error = await response.json();
      throw new Error(error.error?.message || `OpenAI API error: ${response.status}`);
    }
    
    const data = await response.json();
    const content = data.choices?.[0]?.message?.content || '';
    
    this.updateUsage(data.usage?.prompt_tokens || 0, data.usage?.completion_tokens || 0, config);
    
    return {
      content,
      usage: {
        promptTokens: data.usage?.prompt_tokens || 0,
        completionTokens: data.usage?.completion_tokens || 0,
        totalTokens: data.usage?.total_tokens || 0
      },
      model: config.model,
      finishReason: data.choices?.[0]?.finish_reason || 'stop'
    };
  }
  
  private async chatClaude(messages: AIMessage[], config: AIConfig): Promise<AIResponse> {
    const url = `${config.baseUrl}/messages`;
    
    console.log(`[AIProvider] Claude URL: ${url}`);
    
    // åˆ†é›¢ system æ¶ˆæ¯
    const systemMessage = messages.find(m => m.role === 'system');
    const chatMessages = messages.filter(m => m.role !== 'system');
    
    const response = await fetch(url, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'x-api-key': config.apiKey,
        'anthropic-version': '2023-06-01'
      },
      body: JSON.stringify({
        model: config.model,
        max_tokens: config.maxTokens,
        system: systemMessage?.content,
        messages: chatMessages.map(m => ({
          role: m.role,
          content: m.content
        }))
      })
    });
    
    // ğŸ”§ æª¢æŸ¥éŸ¿æ‡‰é¡å‹
    const contentType = response.headers.get('content-type') || '';
    if (!contentType.includes('application/json')) {
      const text = await response.text();
      console.error(`[AIProvider] Claude è¿”å›é JSON éŸ¿æ‡‰:`, text.substring(0, 200));
      throw new Error(`Claude API è¿”å›éŒ¯èª¤æ ¼å¼ (${response.status}): å¯èƒ½æ˜¯ URL é…ç½®éŒ¯èª¤`);
    }
    
    if (!response.ok) {
      const error = await response.json();
      throw new Error(error.error?.message || `Claude API error: ${response.status}`);
    }
    
    const data = await response.json();
    const content = data.content?.[0]?.text || '';
    
    this.updateUsage(data.usage?.input_tokens || 0, data.usage?.output_tokens || 0, config);
    
    return {
      content,
      usage: {
        promptTokens: data.usage?.input_tokens || 0,
        completionTokens: data.usage?.output_tokens || 0,
        totalTokens: (data.usage?.input_tokens || 0) + (data.usage?.output_tokens || 0)
      },
      model: config.model,
      finishReason: data.stop_reason || 'stop'
    };
  }
  
  private async chatDeepSeek(messages: AIMessage[], config: AIConfig): Promise<AIResponse> {
    // DeepSeek ä½¿ç”¨ OpenAI å…¼å®¹ API
    const url = `${config.baseUrl}/chat/completions`;
    
    console.log(`[AIProvider] DeepSeek URL: ${url}`);
    
    const response = await fetch(url, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${config.apiKey}`
      },
      body: JSON.stringify({
        model: config.model,
        messages,
        temperature: config.temperature,
        max_tokens: config.maxTokens,
        top_p: config.topP
      })
    });
    
    // ğŸ”§ æª¢æŸ¥éŸ¿æ‡‰é¡å‹
    const contentType = response.headers.get('content-type') || '';
    if (!contentType.includes('application/json')) {
      const text = await response.text();
      console.error(`[AIProvider] DeepSeek è¿”å›é JSON éŸ¿æ‡‰:`, text.substring(0, 200));
      throw new Error(`DeepSeek API è¿”å›éŒ¯èª¤æ ¼å¼ (${response.status}): å¯èƒ½æ˜¯ URL é…ç½®éŒ¯èª¤`);
    }
    
    if (!response.ok) {
      const error = await response.json();
      throw new Error(error.error?.message || `DeepSeek API error: ${response.status}`);
    }
    
    const data = await response.json();
    const content = data.choices?.[0]?.message?.content || '';
    
    this.updateUsage(data.usage?.prompt_tokens || 0, data.usage?.completion_tokens || 0, config);
    
    return {
      content,
      usage: {
        promptTokens: data.usage?.prompt_tokens || 0,
        completionTokens: data.usage?.completion_tokens || 0,
        totalTokens: data.usage?.total_tokens || 0
      },
      model: config.model,
      finishReason: data.choices?.[0]?.finish_reason || 'stop'
    };
  }
  
  private async chatOllama(messages: AIMessage[], config: AIConfig): Promise<AIResponse> {
    const url = `${config.baseUrl || 'http://localhost:11434'}/api/chat`;
    
    const response = await fetch(url, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        model: config.model,
        messages,
        options: {
          temperature: config.temperature,
          num_predict: config.maxTokens,
          top_p: config.topP
        },
        stream: false
      })
    });
    
    if (!response.ok) {
      throw new Error('Ollama connection failed. Make sure Ollama is running.');
    }
    
    const data = await response.json();
    const content = data.message?.content || '';
    
    return {
      content,
      usage: {
        promptTokens: data.prompt_eval_count || 0,
        completionTokens: data.eval_count || 0,
        totalTokens: (data.prompt_eval_count || 0) + (data.eval_count || 0)
      },
      model: config.model,
      finishReason: 'stop'
    };
  }
  
  // ============ è¼”åŠ©æ–¹æ³• ============
  
  private updateUsage(promptTokens: number, completionTokens: number, config: AIConfig): void {
    const model = this.currentModel();
    const cost = model?.pricePerMToken 
      ? ((promptTokens + completionTokens) / 1000000) * model.pricePerMToken
      : 0;
    
    this._usage.update(u => ({
      totalTokens: u.totalTokens + promptTokens + completionTokens,
      totalCalls: u.totalCalls + 1,
      totalCost: u.totalCost + cost
    }));
  }
  
  private loadConfig(): void {
    try {
      const stored = localStorage.getItem('tg-matrix-ai-config');
      if (stored) {
        this._config.set(JSON.parse(stored));
      }
      
      const usage = localStorage.getItem('tg-matrix-ai-usage');
      if (usage) {
        this._usage.set(JSON.parse(usage));
      }
    } catch (e) {
      console.error('Failed to load AI config:', e);
    }
  }
  
  private saveConfig(): void {
    try {
      localStorage.setItem('tg-matrix-ai-config', JSON.stringify(this._config()));
      localStorage.setItem('tg-matrix-ai-usage', JSON.stringify(this._usage()));
    } catch (e) {
      console.error('Failed to save AI config:', e);
    }
  }
  
  /**
   * é‡ç½®ä½¿ç”¨çµ±è¨ˆ
   */
  resetUsage(): void {
    this._usage.set({
      totalTokens: 0,
      totalCalls: 0,
      totalCost: 0
    });
    this.saveConfig();
  }
  
  /**
   * ä¼°ç®—æ–‡æœ¬ token æ•¸ï¼ˆç²—ç•¥ä¼°ç®—ï¼‰
   */
  estimateTokens(text: string): number {
    // ç²—ç•¥ä¼°ç®—ï¼šè‹±æ–‡ç´„ 4 å­—ç¬¦/tokenï¼Œä¸­æ–‡ç´„ 2 å­—ç¬¦/token
    const chineseChars = (text.match(/[\u4e00-\u9fff]/g) || []).length;
    const otherChars = text.length - chineseChars;
    return Math.ceil(chineseChars / 1.5 + otherChars / 4);
  }
}
